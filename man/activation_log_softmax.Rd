% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/activations.R
\name{activation_log_softmax}
\alias{activation_log_softmax}
\title{Log-Softmax activation function.}
\usage{
activation_log_softmax(x, axis = -1L)
}
\arguments{
\item{x}{Input tensor.}

\item{axis}{Integer, axis along which the softmax is applied.}
}
\description{
Each input vector is handled independently.
The \code{axis} argument sets which axis of the input the function
is applied along.
}
\seealso{
\itemize{
\item \url{https://keras.io/api/layers/activations#logsoftmax-function}
}

Other activations: \cr
\code{\link{activation_elu}()} \cr
\code{\link{activation_exponential}()} \cr
\code{\link{activation_gelu}()} \cr
\code{\link{activation_hard_sigmoid}()} \cr
\code{\link{activation_leaky_relu}()} \cr
\code{\link{activation_linear}()} \cr
\code{\link{activation_mish}()} \cr
\code{\link{activation_relu}()} \cr
\code{\link{activation_relu6}()} \cr
\code{\link{activation_selu}()} \cr
\code{\link{activation_sigmoid}()} \cr
\code{\link{activation_silu}()} \cr
\code{\link{activation_softmax}()} \cr
\code{\link{activation_softplus}()} \cr
\code{\link{activation_softsign}()} \cr
\code{\link{activation_tanh}()} \cr
}
\concept{activations}
