% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizers.R
\name{optimizer_muon}
\alias{optimizer_muon}
\title{Optimizer that implements the Muon algorithm.}
\usage{
optimizer_muon(
  learning_rate = 0.001,
  adam_beta_1 = 0.9,
  adam_beta_2 = 0.999,
  epsilon = 1e-07,
  weight_decay = 0.1,
  clipnorm = NULL,
  clipvalue = NULL,
  global_clipnorm = NULL,
  use_ema = FALSE,
  ema_momentum = 0.99,
  ema_overwrite_frequency = NULL,
  loss_scale_factor = NULL,
  gradient_accumulation_steps = NULL,
  name = "muon",
  exclude_layers = NULL,
  exclude_embeddings = TRUE,
  muon_a = 3.4445,
  muon_b = -4.775,
  muon_c = 2.0315,
  adam_lr_ratio = 0.1,
  momentum = 0.95,
  ns_steps = 6L,
  nesterov = TRUE,
  ...
)
}
\arguments{
\item{learning_rate}{A float,
\code{LearningRateSchedule()} instance, or
a callable that takes no arguments and returns the actual value to
use. The learning rate. Defaults to \code{0.001}.}

\item{adam_beta_1}{A float value or a constant float tensor, or a callable
that takes no arguments and returns the actual value to use.
The exponential decay rate for the 1st moment estimates. Defaults to
\code{0.9}.}

\item{adam_beta_2}{A float value or a constant float tensor, ora callable
that takes no arguments and returns the actual value to use.
The exponential decay rate for the 2nd moment estimates. Defaults to
\code{0.999}.}

\item{epsilon}{A small constant for numerical stability. This is
"epsilon hat" in the Kingma and Ba paper
(in the formula just before Section 2.1),
not the epsilon in Algorithm 1 of the paper.
It is used as in AdamW. Defaults to \code{1e-7}.}

\item{name}{String, name for the object}

\item{exclude_layers}{List of strings, keywords of layer names to exclude.
All layers with keywords in their path will use AdamW.}

\item{exclude_embeddings}{Boolean value.
If \code{TRUE}, embedding layers will use AdamW.}

\item{muon_a}{Float, parameter a of the muon algorithm.
It is recommended to use the default value.}

\item{muon_b}{Float, parameter b of the muon algorithm.
It is recommended to use the default value.}

\item{muon_c}{Float, parameter c of the muon algorithm.
It is recommended to use the default value.}

\item{adam_lr_ratio}{Float, the ratio of the learning rate when
using Adam to the main learning rate.
it is recommended to set it to \code{0.1}.}

\item{momentum}{Float, momentum used by internal SGD.}

\item{ns_steps}{Integer, number of Newton-Schulz iterations to run.}

\item{nesterov}{Boolean, whether to use Nesterov-style momentum.}

\item{...}{For forward/backward compatibility.}
}
\value{
an \code{Optimizer} instance
}
\description{
Note that this optimizer should not be used in the following layers:
\enumerate{
\item Embedding layer
\item Final output fully connected layer
\item Any {0,1}-D variables
}

These should all be optimized using AdamW.

The Muon optimizer can use both the Muon update step or the
AdamW update step based on the following:
\itemize{
\item For any variable that isn't 2D, 3D or 4D, the AdamW step
will be used. This is not configurable.
\item If the argument \code{exclude_embeddings} (defaults to \code{TRUE}) is set
to \code{TRUE}, the AdamW step will be used.
\item For any variable with a name that matches an expression
listed in the argument \code{exclude_layers} (a list), the
AdamW step will be used.
\item Any other variable uses the Muon step.
}

Typically, you only need to pass the name of your densely-connected
output layer to \code{exclude_layers}, e.g.
\code{exclude_layers = "output_dense"}.
}
\section{References}{
\itemize{
\item \href{https://github.com/KellerJordan/Muon}{Original implementation}
\item \href{https://arxiv.org/abs/2502.16982}{Liu et al, 2025}
}
}

\seealso{
Other optimizers: \cr
\code{\link{optimizer_adadelta}()} \cr
\code{\link{optimizer_adafactor}()} \cr
\code{\link{optimizer_adagrad}()} \cr
\code{\link{optimizer_adam}()} \cr
\code{\link{optimizer_adam_w}()} \cr
\code{\link{optimizer_adamax}()} \cr
\code{\link{optimizer_ftrl}()} \cr
\code{\link{optimizer_lamb}()} \cr
\code{\link{optimizer_lion}()} \cr
\code{\link{optimizer_loss_scale}()} \cr
\code{\link{optimizer_nadam}()} \cr
\code{\link{optimizer_rmsprop}()} \cr
\code{\link{optimizer_sgd}()} \cr
}
\concept{optimizers}
