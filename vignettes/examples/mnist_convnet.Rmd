---
title: Simple MNIST convnet
date-created: 2015/06/19
last-modified: 2020/04/21
description: A simple convnet that achieves ~99% test accuracy on MNIST.
domain: vision
category: basic
output: rmarkdown::html_vignette
date: 'Last Modified: 2023-12-07; Last Rendered: 2024-01-04'
vignette: >
  %\VignetteIndexEntry{Simple MNIST convnet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Setup


```r
library(keras3)
```

## Prepare the data


```r
# Model / data parameters
num_classes <- 10
input_shape <- c(28, 28, 1)

# Load the data and split it between train and test sets
c(c(x_train, y_train), c(x_test, y_test)) %<-% dataset_mnist()

# Scale images to the [0, 1] range
x_train <- x_train / 255
x_test <- x_test / 255
# Make sure images have shape (28, 28, 1)
x_train <- op_expand_dims(x_train, -1)
x_test <- op_expand_dims(x_test, -1)


dim(x_train)
```

```
## [1] 60000    28    28     1
```

```r
dim(x_test)
```

```
## [1] 10000    28    28     1
```

```r
# convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
```

## Build the model


```r
model <- keras_model_sequential(input_shape = input_shape)
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = num_classes, activation = "softmax")

summary(model)
```

```
## [1mModel: "sequential"[0m
## â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
## â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape             [0m[1m [0mâ”ƒ[1m [0m[1m   Param #[0m[1m [0mâ”ƒ
## â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
## â”‚ conv2d_1 ([38;5;33mConv2D[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m26[0m, [38;5;34m26[0m, [38;5;34m32[0m)        â”‚        [38;5;34m320[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ max_pooling2d_1 ([38;5;33mMaxPooling2D[0m)  â”‚ ([38;5;45mNone[0m, [38;5;34m13[0m, [38;5;34m13[0m, [38;5;34m32[0m)        â”‚          [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ conv2d ([38;5;33mConv2D[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m11[0m, [38;5;34m11[0m, [38;5;34m64[0m)        â”‚     [38;5;34m18,496[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ max_pooling2d ([38;5;33mMaxPooling2D[0m)    â”‚ ([38;5;45mNone[0m, [38;5;34m5[0m, [38;5;34m5[0m, [38;5;34m64[0m)          â”‚          [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ flatten ([38;5;33mFlatten[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m1600[0m)              â”‚          [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dropout ([38;5;33mDropout[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m1600[0m)              â”‚          [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense ([38;5;33mDense[0m)                   â”‚ ([38;5;45mNone[0m, [38;5;34m10[0m)                â”‚     [38;5;34m16,010[0m â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
## [1m Total params: [0m[38;5;34m34,826[0m (136.04 KB)
## [1m Trainable params: [0m[38;5;34m34,826[0m (136.04 KB)
## [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
```

## Train the model


```r
batch_size <- 128
epochs <- 15

model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)

model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.1
)
```

```
## Epoch 1/15
## 422/422 - 5s - 12ms/step - accuracy: 0.8846 - loss: 0.3815 - val_accuracy: 0.9782 - val_loss: 0.0809
## Epoch 2/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9644 - loss: 0.1150 - val_accuracy: 0.9863 - val_loss: 0.0549
## Epoch 3/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9735 - loss: 0.0840 - val_accuracy: 0.9877 - val_loss: 0.0454
## Epoch 4/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9782 - loss: 0.0690 - val_accuracy: 0.9898 - val_loss: 0.0415
## Epoch 5/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9809 - loss: 0.0609 - val_accuracy: 0.9905 - val_loss: 0.0374
## Epoch 6/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9823 - loss: 0.0563 - val_accuracy: 0.9913 - val_loss: 0.0359
## Epoch 7/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9849 - loss: 0.0489 - val_accuracy: 0.9917 - val_loss: 0.0328
## Epoch 8/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9842 - loss: 0.0484 - val_accuracy: 0.9915 - val_loss: 0.0330
## Epoch 9/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9859 - loss: 0.0441 - val_accuracy: 0.9920 - val_loss: 0.0319
## Epoch 10/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9866 - loss: 0.0406 - val_accuracy: 0.9925 - val_loss: 0.0320
## Epoch 11/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9876 - loss: 0.0389 - val_accuracy: 0.9922 - val_loss: 0.0315
## Epoch 12/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9883 - loss: 0.0371 - val_accuracy: 0.9925 - val_loss: 0.0300
## Epoch 13/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9880 - loss: 0.0360 - val_accuracy: 0.9930 - val_loss: 0.0278
## Epoch 14/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9890 - loss: 0.0334 - val_accuracy: 0.9935 - val_loss: 0.0286
## Epoch 15/15
## 422/422 - 1s - 2ms/step - accuracy: 0.9898 - loss: 0.0307 - val_accuracy: 0.9923 - val_loss: 0.0292
```

## Evaluate the trained model


```r
score <- model %>% evaluate(x_test, y_test, verbose=0)
score
```

```
## $accuracy
## [1] 0.9908
##
## $loss
## [1] 0.02631393
```
