---
title: 'Imbalanced classification: credit card fraud detection'
date-created: 2019/05/28
last-modified: 2020/04/17
description: Demonstration of how to handle highly imbalanced classification problems.
output: rmarkdown::html_vignette
domain: structured data
category: basic
backend: jax
vignette: >
  %\VignetteIndexEntry{Imbalanced classification: credit card fraud detection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


``` r
library(keras3)
use_backend("jax")
```
## Introduction

This example looks at the
[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)
dataset to demonstrate how
to train a classification model on data with highly imbalanced classes.
You can download the data by clicking "Download" at
the link, or if you're setup with a kaggle API key at
`"~/.kaggle/kagle.json"`, you can run the following:


``` r
reticulate::py_install("kaggle", pip = TRUE)
reticulate::py_available(TRUE) # ensure 'kaggle' is on the PATH
system("kaggle datasets download -d mlg-ulb/creditcardfraud")
zip::unzip("creditcardfraud.zip", files = "creditcard.csv")
```

## First, load the data


``` r
library(readr)
df <- read_csv("creditcard.csv", col_types = cols(
  Class = col_integer(),
  .default = col_double()
))
tibble::glimpse(df)
```

```
## Rows: 284,807
## Columns: 31
## $ Time   [3m[38;5;246m<dbl>[39m[23m 0[38;5;246m, [39m0[38;5;246m, [39m1[38;5;246m, [39m1[38;5;246m, [39m2[38;5;246m, [39m2[38;5;246m, [39m4[38;5;246m, [39m7[38;5;246m, [39m7[38;5;246m, [39m9[38;5;246m, [39m10[38;5;246m, [39m10[38;5;246m, [39m10[38;5;246m, [39m11[38;5;246m, [39m12[38;5;246m, [39m12[38;5;246m, [39m12[38;5;246m, [39m1â€¦
## $ V1     [3m[38;5;246m<dbl>[39m[23m -1.3598071[38;5;246m, [39m1.1918571[38;5;246m, [39m-1.3583541[38;5;246m, [39m-0.9662717[38;5;246m, [39m-1.1582331[38;5;246m, [39mâ€¦
## $ V2     [3m[38;5;246m<dbl>[39m[23m -0.07278117[38;5;246m, [39m0.26615071[38;5;246m, [39m-1.34016307[38;5;246m, [39m-0.18522601[38;5;246m, [39m0.877736â€¦
## $ V3     [3m[38;5;246m<dbl>[39m[23m 2.53634674[38;5;246m, [39m0.16648011[38;5;246m, [39m1.77320934[38;5;246m, [39m1.79299334[38;5;246m, [39m1.54871785[38;5;246m,[39mâ€¦
## $ V4     [3m[38;5;246m<dbl>[39m[23m 1.37815522[38;5;246m, [39m0.44815408[38;5;246m, [39m0.37977959[38;5;246m, [39m-0.86329128[38;5;246m, [39m0.40303393â€¦
## $ V5     [3m[38;5;246m<dbl>[39m[23m -0.33832077[38;5;246m, [39m0.06001765[38;5;246m, [39m-0.50319813[38;5;246m, [39m-0.01030888[38;5;246m, [39m-0.40719â€¦
## $ V6     [3m[38;5;246m<dbl>[39m[23m 0.46238778[38;5;246m, [39m-0.08236081[38;5;246m, [39m1.80049938[38;5;246m, [39m1.24720317[38;5;246m, [39m0.09592146â€¦
## $ V7     [3m[38;5;246m<dbl>[39m[23m 0.239598554[38;5;246m, [39m-0.078802983[38;5;246m, [39m0.791460956[38;5;246m, [39m0.237608940[38;5;246m, [39m0.5929â€¦
## $ V8     [3m[38;5;246m<dbl>[39m[23m 0.098697901[38;5;246m, [39m0.085101655[38;5;246m, [39m0.247675787[38;5;246m, [39m0.377435875[38;5;246m, [39m-0.2705â€¦
## $ V9     [3m[38;5;246m<dbl>[39m[23m 0.3637870[38;5;246m, [39m-0.2554251[38;5;246m, [39m-1.5146543[38;5;246m, [39m-1.3870241[38;5;246m, [39m0.8177393[38;5;246m, [39m-â€¦
## $ V10    [3m[38;5;246m<dbl>[39m[23m 0.09079417[38;5;246m, [39m-0.16697441[38;5;246m, [39m0.20764287[38;5;246m, [39m-0.05495192[38;5;246m, [39m0.7530744â€¦
## $ V11    [3m[38;5;246m<dbl>[39m[23m -0.55159953[38;5;246m, [39m1.61272666[38;5;246m, [39m0.62450146[38;5;246m, [39m-0.22648726[38;5;246m, [39m-0.822842â€¦
## $ V12    [3m[38;5;246m<dbl>[39m[23m -0.61780086[38;5;246m, [39m1.06523531[38;5;246m, [39m0.06608369[38;5;246m, [39m0.17822823[38;5;246m, [39m0.53819555â€¦
## $ V13    [3m[38;5;246m<dbl>[39m[23m -0.99138985[38;5;246m, [39m0.48909502[38;5;246m, [39m0.71729273[38;5;246m, [39m0.50775687[38;5;246m, [39m1.34585159â€¦
## $ V14    [3m[38;5;246m<dbl>[39m[23m -0.31116935[38;5;246m, [39m-0.14377230[38;5;246m, [39m-0.16594592[38;5;246m, [39m-0.28792375[38;5;246m, [39m-1.1196â€¦
## $ V15    [3m[38;5;246m<dbl>[39m[23m 1.468176972[38;5;246m, [39m0.635558093[38;5;246m, [39m2.345864949[38;5;246m, [39m-0.631418118[38;5;246m, [39m0.1751â€¦
## $ V16    [3m[38;5;246m<dbl>[39m[23m -0.47040053[38;5;246m, [39m0.46391704[38;5;246m, [39m-2.89008319[38;5;246m, [39m-1.05964725[38;5;246m, [39m-0.45144â€¦
## $ V17    [3m[38;5;246m<dbl>[39m[23m 0.207971242[38;5;246m, [39m-0.114804663[38;5;246m, [39m1.109969379[38;5;246m, [39m-0.684092786[38;5;246m, [39m-0.23â€¦
## $ V18    [3m[38;5;246m<dbl>[39m[23m 0.02579058[38;5;246m, [39m-0.18336127[38;5;246m, [39m-0.12135931[38;5;246m, [39m1.96577500[38;5;246m, [39m-0.038194â€¦
## $ V19    [3m[38;5;246m<dbl>[39m[23m 0.40399296[38;5;246m, [39m-0.14578304[38;5;246m, [39m-2.26185710[38;5;246m, [39m-1.23262197[38;5;246m, [39m0.803486â€¦
## $ V20    [3m[38;5;246m<dbl>[39m[23m 0.25141210[38;5;246m, [39m-0.06908314[38;5;246m, [39m0.52497973[38;5;246m, [39m-0.20803778[38;5;246m, [39m0.4085423â€¦
## $ V21    [3m[38;5;246m<dbl>[39m[23m -0.018306778[38;5;246m, [39m-0.225775248[38;5;246m, [39m0.247998153[38;5;246m, [39m-0.108300452[38;5;246m, [39m-0.0â€¦
## $ V22    [3m[38;5;246m<dbl>[39m[23m 0.277837576[38;5;246m, [39m-0.638671953[38;5;246m, [39m0.771679402[38;5;246m, [39m0.005273597[38;5;246m, [39m0.7982â€¦
## $ V23    [3m[38;5;246m<dbl>[39m[23m -0.110473910[38;5;246m, [39m0.101288021[38;5;246m, [39m0.909412262[38;5;246m, [39m-0.190320519[38;5;246m, [39m-0.13â€¦
## $ V24    [3m[38;5;246m<dbl>[39m[23m 0.06692807[38;5;246m, [39m-0.33984648[38;5;246m, [39m-0.68928096[38;5;246m, [39m-1.17557533[38;5;246m, [39m0.141266â€¦
## $ V25    [3m[38;5;246m<dbl>[39m[23m 0.12853936[38;5;246m, [39m0.16717040[38;5;246m, [39m-0.32764183[38;5;246m, [39m0.64737603[38;5;246m, [39m-0.2060095â€¦
## $ V26    [3m[38;5;246m<dbl>[39m[23m -0.18911484[38;5;246m, [39m0.12589453[38;5;246m, [39m-0.13909657[38;5;246m, [39m-0.22192884[38;5;246m, [39m0.502292â€¦
## $ V27    [3m[38;5;246m<dbl>[39m[23m 0.133558377[38;5;246m, [39m-0.008983099[38;5;246m, [39m-0.055352794[38;5;246m, [39m0.062722849[38;5;246m, [39m0.219â€¦
## $ V28    [3m[38;5;246m<dbl>[39m[23m -0.021053053[38;5;246m, [39m0.014724169[38;5;246m, [39m-0.059751841[38;5;246m, [39m0.061457629[38;5;246m, [39m0.215â€¦
## $ Amount [3m[38;5;246m<dbl>[39m[23m 149.62[38;5;246m, [39m2.69[38;5;246m, [39m378.66[38;5;246m, [39m123.50[38;5;246m, [39m69.99[38;5;246m, [39m3.67[38;5;246m, [39m4.99[38;5;246m, [39m40.80[38;5;246m, [39m93.â€¦
## $ Class  [3m[38;5;246m<int>[39m[23m 0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m, [39m0[38;5;246m,[39mâ€¦
```

## Prepare a validation set


``` r
val_idx <- nrow(df) %>% sample.int(., round( . * 0.2))
val_df <- df[val_idx, ]
train_df <- df[-val_idx, ]

cat("Number of training samples:", nrow(train_df), "\n")
```

```
## Number of training samples: 227846
```

``` r
cat("Number of validation samples:", nrow(val_df), "\n")
```

```
## Number of validation samples: 56961
```

## Analyze class imbalance in the targets


``` r
counts <- table(train_df$Class)
counts
```

```
##
##      0      1
## 227454    392
```

``` r
cat(sprintf("Number of positive samples in training data: %i (%.2f%% of total)",
            counts["1"], 100 * counts["1"] / sum(counts)))
```

```
## Number of positive samples in training data: 392 (0.17% of total)
```

``` r
weight_for_0 = 1 / counts["0"]
weight_for_1 = 1 / counts["1"]
```

## Normalize the data using training set statistics


``` r
feature_names <- colnames(train_df) %>% setdiff("Class")

train_features <- as.matrix(train_df[feature_names])
train_targets <- as.matrix(train_df$Class)

val_features <- as.matrix(val_df[feature_names])
val_targets <- as.matrix(val_df$Class)

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))
```

## Build a binary classification model


``` r
model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(256, activation = "relu") |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

model
```

```
## [1mModel: "sequential"[0m
## â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
## â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ
## â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
## â”‚ dense ([38;5;33mDense[0m)                   â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚         [38;5;34m7,936[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_1 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚        [38;5;34m65,792[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dropout ([38;5;33mDropout[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚             [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_2 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚        [38;5;34m65,792[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dropout_1 ([38;5;33mDropout[0m)             â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚             [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_3 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m)              â”‚           [38;5;34m257[0m â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
## [1m Total params: [0m[38;5;34m139,777[0m (546.00 KB)
## [1m Trainable params: [0m[38;5;34m139,777[0m (546.00 KB)
## [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
```

## Train the model with `class_weight` argument


``` r
metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-2),
  loss = "binary_crossentropy",
  metrics = metrics
)
callbacks <- list(
  callback_model_checkpoint("fraud_model_at_epoch_{epoch}.keras")
)

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)

model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  callbacks = callbacks,
  verbose = 2
)
```

```
## Epoch 1/30
## 112/112 - 5s - 44ms/step - fn: 44.0000 - fp: 27073.0000 - loss: 2.3729e-06 - precision: 0.0127 - recall: 0.8878 - tn: 200381.0000 - tp: 348.0000 - val_fn: 9.0000 - val_fp: 691.0000 - val_loss: 0.0903 - val_precision: 0.1164 - val_recall: 0.9100 - val_tn: 56170.0000 - val_tp: 91.0000
## Epoch 2/30
## 112/112 - 0s - 2ms/step - fn: 36.0000 - fp: 8040.0000 - loss: 1.5991e-06 - precision: 0.0424 - recall: 0.9082 - tn: 219414.0000 - tp: 356.0000 - val_fn: 7.0000 - val_fp: 2363.0000 - val_loss: 0.1152 - val_precision: 0.0379 - val_recall: 0.9300 - val_tn: 54498.0000 - val_tp: 93.0000
## Epoch 3/30
## 112/112 - 0s - 2ms/step - fn: 28.0000 - fp: 7927.0000 - loss: 1.2033e-06 - precision: 0.0439 - recall: 0.9286 - tn: 219527.0000 - tp: 364.0000 - val_fn: 5.0000 - val_fp: 1738.0000 - val_loss: 0.1034 - val_precision: 0.0518 - val_recall: 0.9500 - val_tn: 55123.0000 - val_tp: 95.0000
## Epoch 4/30
## 112/112 - 0s - 2ms/step - fn: 24.0000 - fp: 7830.0000 - loss: 1.0319e-06 - precision: 0.0449 - recall: 0.9388 - tn: 219624.0000 - tp: 368.0000 - val_fn: 5.0000 - val_fp: 1963.0000 - val_loss: 0.0938 - val_precision: 0.0462 - val_recall: 0.9500 - val_tn: 54898.0000 - val_tp: 95.0000
## Epoch 5/30
## 112/112 - 0s - 2ms/step - fn: 18.0000 - fp: 7930.0000 - loss: 9.3155e-07 - precision: 0.0450 - recall: 0.9541 - tn: 219524.0000 - tp: 374.0000 - val_fn: 5.0000 - val_fp: 2081.0000 - val_loss: 0.1072 - val_precision: 0.0437 - val_recall: 0.9500 - val_tn: 54780.0000 - val_tp: 95.0000
## Epoch 6/30
## 112/112 - 0s - 2ms/step - fn: 30.0000 - fp: 12948.0000 - loss: 1.9802e-06 - precision: 0.0272 - recall: 0.9235 - tn: 214506.0000 - tp: 362.0000 - val_fn: 9.0000 - val_fp: 456.0000 - val_loss: 0.0579 - val_precision: 0.1664 - val_recall: 0.9100 - val_tn: 56405.0000 - val_tp: 91.0000
## Epoch 7/30
## 112/112 - 0s - 2ms/step - fn: 29.0000 - fp: 6673.0000 - loss: 1.1701e-06 - precision: 0.0516 - recall: 0.9260 - tn: 220781.0000 - tp: 363.0000 - val_fn: 2.0000 - val_fp: 2806.0000 - val_loss: 0.1205 - val_precision: 0.0337 - val_recall: 0.9800 - val_tn: 54055.0000 - val_tp: 98.0000
## Epoch 8/30
## 112/112 - 0s - 2ms/step - fn: 17.0000 - fp: 8006.0000 - loss: 8.4931e-07 - precision: 0.0447 - recall: 0.9566 - tn: 219448.0000 - tp: 375.0000 - val_fn: 7.0000 - val_fp: 1537.0000 - val_loss: 0.1357 - val_precision: 0.0571 - val_recall: 0.9300 - val_tn: 55324.0000 - val_tp: 93.0000
## Epoch 9/30
## 112/112 - 0s - 2ms/step - fn: 21.0000 - fp: 7221.0000 - loss: 8.8134e-07 - precision: 0.0489 - recall: 0.9464 - tn: 220233.0000 - tp: 371.0000 - val_fn: 3.0000 - val_fp: 3622.0000 - val_loss: 0.1621 - val_precision: 0.0261 - val_recall: 0.9700 - val_tn: 53239.0000 - val_tp: 97.0000
## Epoch 10/30
## 112/112 - 0s - 2ms/step - fn: 17.0000 - fp: 6942.0000 - loss: 7.0770e-07 - precision: 0.0513 - recall: 0.9566 - tn: 220512.0000 - tp: 375.0000 - val_fn: 5.0000 - val_fp: 1104.0000 - val_loss: 0.0528 - val_precision: 0.0792 - val_recall: 0.9500 - val_tn: 55757.0000 - val_tp: 95.0000
## Epoch 11/30
## 112/112 - 0s - 2ms/step - fn: 9.0000 - fp: 5439.0000 - loss: 5.0431e-07 - precision: 0.0658 - recall: 0.9770 - tn: 222015.0000 - tp: 383.0000 - val_fn: 5.0000 - val_fp: 1476.0000 - val_loss: 0.0539 - val_precision: 0.0605 - val_recall: 0.9500 - val_tn: 55385.0000 - val_tp: 95.0000
## Epoch 12/30
## 112/112 - 0s - 2ms/step - fn: 7.0000 - fp: 4509.0000 - loss: 4.0972e-07 - precision: 0.0787 - recall: 0.9821 - tn: 222945.0000 - tp: 385.0000 - val_fn: 6.0000 - val_fp: 1010.0000 - val_loss: 0.0451 - val_precision: 0.0851 - val_recall: 0.9400 - val_tn: 55851.0000 - val_tp: 94.0000
## Epoch 13/30
## 112/112 - 0s - 2ms/step - fn: 14.0000 - fp: 8478.0000 - loss: 1.0058e-06 - precision: 0.0427 - recall: 0.9643 - tn: 218976.0000 - tp: 378.0000 - val_fn: 10.0000 - val_fp: 246.0000 - val_loss: 0.0315 - val_precision: 0.2679 - val_recall: 0.9000 - val_tn: 56615.0000 - val_tp: 90.0000
## Epoch 14/30
## 112/112 - 0s - 2ms/step - fn: 12.0000 - fp: 5530.0000 - loss: 6.2484e-07 - precision: 0.0643 - recall: 0.9694 - tn: 221924.0000 - tp: 380.0000 - val_fn: 9.0000 - val_fp: 1220.0000 - val_loss: 0.0556 - val_precision: 0.0694 - val_recall: 0.9100 - val_tn: 55641.0000 - val_tp: 91.0000
## Epoch 15/30
## 112/112 - 0s - 2ms/step - fn: 6.0000 - fp: 4306.0000 - loss: 3.9004e-07 - precision: 0.0823 - recall: 0.9847 - tn: 223148.0000 - tp: 386.0000 - val_fn: 5.0000 - val_fp: 1861.0000 - val_loss: 0.0758 - val_precision: 0.0486 - val_recall: 0.9500 - val_tn: 55000.0000 - val_tp: 95.0000
## Epoch 16/30
## 112/112 - 0s - 2ms/step - fn: 12.0000 - fp: 9652.0000 - loss: 1.2719e-06 - precision: 0.0379 - recall: 0.9694 - tn: 217802.0000 - tp: 380.0000 - val_fn: 6.0000 - val_fp: 899.0000 - val_loss: 0.0829 - val_precision: 0.0947 - val_recall: 0.9400 - val_tn: 55962.0000 - val_tp: 94.0000
## Epoch 17/30
## 112/112 - 0s - 2ms/step - fn: 9.0000 - fp: 4728.0000 - loss: 6.0103e-07 - precision: 0.0749 - recall: 0.9770 - tn: 222726.0000 - tp: 383.0000 - val_fn: 6.0000 - val_fp: 1538.0000 - val_loss: 0.0880 - val_precision: 0.0576 - val_recall: 0.9400 - val_tn: 55323.0000 - val_tp: 94.0000
## Epoch 18/30
## 112/112 - 0s - 2ms/step - fn: 9.0000 - fp: 4919.0000 - loss: 4.5287e-07 - precision: 0.0722 - recall: 0.9770 - tn: 222535.0000 - tp: 383.0000 - val_fn: 8.0000 - val_fp: 530.0000 - val_loss: 0.0411 - val_precision: 0.1479 - val_recall: 0.9200 - val_tn: 56331.0000 - val_tp: 92.0000
## Epoch 19/30
## 112/112 - 0s - 2ms/step - fn: 5.0000 - fp: 4110.0000 - loss: 4.4215e-07 - precision: 0.0861 - recall: 0.9872 - tn: 223344.0000 - tp: 387.0000 - val_fn: 6.0000 - val_fp: 1348.0000 - val_loss: 0.0669 - val_precision: 0.0652 - val_recall: 0.9400 - val_tn: 55513.0000 - val_tp: 94.0000
## Epoch 20/30
## 112/112 - 0s - 2ms/step - fn: 3.0000 - fp: 3850.0000 - loss: 3.6038e-07 - precision: 0.0918 - recall: 0.9923 - tn: 223604.0000 - tp: 389.0000 - val_fn: 6.0000 - val_fp: 1253.0000 - val_loss: 0.0658 - val_precision: 0.0698 - val_recall: 0.9400 - val_tn: 55608.0000 - val_tp: 94.0000
## Epoch 21/30
## 112/112 - 0s - 2ms/step - fn: 5.0000 - fp: 4006.0000 - loss: 4.0618e-07 - precision: 0.0881 - recall: 0.9872 - tn: 223448.0000 - tp: 387.0000 - val_fn: 8.0000 - val_fp: 1383.0000 - val_loss: 0.0866 - val_precision: 0.0624 - val_recall: 0.9200 - val_tn: 55478.0000 - val_tp: 92.0000
## Epoch 22/30
## 112/112 - 0s - 2ms/step - fn: 1.0000 - fp: 2347.0000 - loss: 2.6096e-07 - precision: 0.1428 - recall: 0.9974 - tn: 225107.0000 - tp: 391.0000 - val_fn: 7.0000 - val_fp: 1416.0000 - val_loss: 0.0825 - val_precision: 0.0616 - val_recall: 0.9300 - val_tn: 55445.0000 - val_tp: 93.0000
## Epoch 23/30
## 112/112 - 0s - 2ms/step - fn: 1.0000 - fp: 3463.0000 - loss: 2.8579e-07 - precision: 0.1015 - recall: 0.9974 - tn: 223991.0000 - tp: 391.0000 - val_fn: 9.0000 - val_fp: 462.0000 - val_loss: 0.0321 - val_precision: 0.1646 - val_recall: 0.9100 - val_tn: 56399.0000 - val_tp: 91.0000
## Epoch 24/30
## 112/112 - 0s - 2ms/step - fn: 5.0000 - fp: 3741.0000 - loss: 5.0086e-07 - precision: 0.0938 - recall: 0.9872 - tn: 223713.0000 - tp: 387.0000 - val_fn: 8.0000 - val_fp: 814.0000 - val_loss: 0.0350 - val_precision: 0.1015 - val_recall: 0.9200 - val_tn: 56047.0000 - val_tp: 92.0000
## Epoch 25/30
## 112/112 - 0s - 2ms/step - fn: 6.0000 - fp: 3771.0000 - loss: 5.1523e-07 - precision: 0.0929 - recall: 0.9847 - tn: 223683.0000 - tp: 386.0000 - val_fn: 7.0000 - val_fp: 1805.0000 - val_loss: 0.1076 - val_precision: 0.0490 - val_recall: 0.9300 - val_tn: 55056.0000 - val_tp: 93.0000
## Epoch 26/30
## 112/112 - 0s - 2ms/step - fn: 2.0000 - fp: 3296.0000 - loss: 2.9389e-07 - precision: 0.1058 - recall: 0.9949 - tn: 224158.0000 - tp: 390.0000 - val_fn: 9.0000 - val_fp: 528.0000 - val_loss: 0.0336 - val_precision: 0.1470 - val_recall: 0.9100 - val_tn: 56333.0000 - val_tp: 91.0000
## Epoch 27/30
## 112/112 - 0s - 2ms/step - fn: 2.0000 - fp: 3672.0000 - loss: 3.3291e-07 - precision: 0.0960 - recall: 0.9949 - tn: 223782.0000 - tp: 390.0000 - val_fn: 8.0000 - val_fp: 664.0000 - val_loss: 0.0357 - val_precision: 0.1217 - val_recall: 0.9200 - val_tn: 56197.0000 - val_tp: 92.0000
## Epoch 28/30
## 112/112 - 0s - 2ms/step - fn: 1.0000 - fp: 2200.0000 - loss: 2.1719e-07 - precision: 0.1509 - recall: 0.9974 - tn: 225254.0000 - tp: 391.0000 - val_fn: 7.0000 - val_fp: 561.0000 - val_loss: 0.0290 - val_precision: 0.1422 - val_recall: 0.9300 - val_tn: 56300.0000 - val_tp: 93.0000
## Epoch 29/30
## 112/112 - 0s - 2ms/step - fn: 1.0000 - fp: 2350.0000 - loss: 1.9201e-07 - precision: 0.1426 - recall: 0.9974 - tn: 225104.0000 - tp: 391.0000 - val_fn: 8.0000 - val_fp: 308.0000 - val_loss: 0.0178 - val_precision: 0.2300 - val_recall: 0.9200 - val_tn: 56553.0000 - val_tp: 92.0000
## Epoch 30/30
## 112/112 - 0s - 2ms/step - fn: 7.0000 - fp: 4779.0000 - loss: 7.9794e-07 - precision: 0.0746 - recall: 0.9821 - tn: 222675.0000 - tp: 385.0000 - val_fn: 4.0000 - val_fp: 3944.0000 - val_loss: 0.2297 - val_precision: 0.0238 - val_recall: 0.9600 - val_tn: 52917.0000 - val_tp: 96.0000
```


``` r
val_pred <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }
```

```
## 1781/1781 - 1s - 584us/step
```

``` r
pred_correct <- val_df$Class == val_pred
cat(sprintf("Validation accuracy: %.2f", mean(pred_correct)))
```

```
## Validation accuracy: 0.93
```

``` r
fraudulent <- val_df$Class == 1

n_fraudulent_detected <- sum(fraudulent & pred_correct)
n_fraudulent_missed <- sum(fraudulent & !pred_correct)
n_legitimate_flagged <- sum(!fraudulent & !pred_correct)
```

## Conclusions

At the end of training, out of
56,961 validation transactions, we
are:

- Correctly identifying
  96 of them as
  fraudulent
- Missing 4
  fraudulent transactions
- At the cost of incorrectly flagging
  3,944 legitimate
  transactions

In the real world, one would put an even higher weight on class 1,
so as to reflect that False Negatives are more costly than False Positives.

Next time your credit card gets declined in an online purchase -- this is why.

<!-- | Trained Model                                                                                                                                                          | Demo                                                                                                                                                                             | -->
<!-- |------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| -->
<!-- | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Imbalanced%20Classification-black.svg)](https://huggingface.co/keras-io/imbalanced_classification) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Imbalanced%20Classification-black.svg)](https://huggingface.co/spaces/keras-io/Credit_Card_Fraud_Detection) | -->

<!-- : Example available on HuggingFace. -->
