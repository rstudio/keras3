---
title: 'Imbalanced classification: credit card fraud detection'
date-created: 2019/05/28
last-modified: 2020/04/17
description: Demonstration of how to handle highly imbalanced classification problems.
output: rmarkdown::html_vignette
domain: structured data
category: basic
backend: jax
vignette: >
  %\VignetteIndexEntry{Imbalanced classification: credit card fraud detection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


``` r
library(keras3)
use_backend("jax")
```
## Introduction

This example looks at the
[Kaggle Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud/)
dataset to demonstrate how
to train a classification model on data with highly imbalanced classes.
You can download the data by clicking "Download" at
the link, or if you're setup with a kaggle API key at
`"~/.kaggle/kagle.json"`, you can run the following:


``` r
reticulate::py_install("kaggle", pip = TRUE)
reticulate::py_available(TRUE) # ensure 'kaggle' is on the PATH
system("kaggle datasets download -d mlg-ulb/creditcardfraud")
zip::unzip("creditcardfraud.zip", files = "creditcard.csv")
```

## First, load the data


``` r
library(readr)
df <- read_csv("creditcard.csv", col_types = cols(
  Class = col_integer(),
  .default = col_double()
))
tibble::glimpse(df)
```

```
## Rows: 284,807
## Columns: 31
## $ Time   [3m[38;5;246m<dbl>[39m[23m 0, 0, 1, 1, 2, 2, 4, 7, 7, 9, 10, 10, 10, 11, 12, 12, 12, 1â€¦
## $ V1     [3m[38;5;246m<dbl>[39m[23m -1.3598071, 1.1918571, -1.3583541, -0.9662717, -1.1582331, â€¦
## $ V2     [3m[38;5;246m<dbl>[39m[23m -0.07278117, 0.26615071, -1.34016307, -0.18522601, 0.877736â€¦
## $ V3     [3m[38;5;246m<dbl>[39m[23m 2.53634674, 0.16648011, 1.77320934, 1.79299334, 1.54871785,â€¦
## $ V4     [3m[38;5;246m<dbl>[39m[23m 1.37815522, 0.44815408, 0.37977959, -0.86329128, 0.40303393â€¦
## $ V5     [3m[38;5;246m<dbl>[39m[23m -0.33832077, 0.06001765, -0.50319813, -0.01030888, -0.40719â€¦
## $ V6     [3m[38;5;246m<dbl>[39m[23m 0.46238778, -0.08236081, 1.80049938, 1.24720317, 0.09592146â€¦
## $ V7     [3m[38;5;246m<dbl>[39m[23m 0.239598554, -0.078802983, 0.791460956, 0.237608940, 0.5929â€¦
## $ V8     [3m[38;5;246m<dbl>[39m[23m 0.098697901, 0.085101655, 0.247675787, 0.377435875, -0.2705â€¦
## $ V9     [3m[38;5;246m<dbl>[39m[23m 0.3637870, -0.2554251, -1.5146543, -1.3870241, 0.8177393, -â€¦
## $ V10    [3m[38;5;246m<dbl>[39m[23m 0.09079417, -0.16697441, 0.20764287, -0.05495192, 0.7530744â€¦
## $ V11    [3m[38;5;246m<dbl>[39m[23m -0.55159953, 1.61272666, 0.62450146, -0.22648726, -0.822842â€¦
## $ V12    [3m[38;5;246m<dbl>[39m[23m -0.61780086, 1.06523531, 0.06608369, 0.17822823, 0.53819555â€¦
## $ V13    [3m[38;5;246m<dbl>[39m[23m -0.99138985, 0.48909502, 0.71729273, 0.50775687, 1.34585159â€¦
## $ V14    [3m[38;5;246m<dbl>[39m[23m -0.31116935, -0.14377230, -0.16594592, -0.28792375, -1.1196â€¦
## $ V15    [3m[38;5;246m<dbl>[39m[23m 1.468176972, 0.635558093, 2.345864949, -0.631418118, 0.1751â€¦
## $ V16    [3m[38;5;246m<dbl>[39m[23m -0.47040053, 0.46391704, -2.89008319, -1.05964725, -0.45144â€¦
## $ V17    [3m[38;5;246m<dbl>[39m[23m 0.207971242, -0.114804663, 1.109969379, -0.684092786, -0.23â€¦
## $ V18    [3m[38;5;246m<dbl>[39m[23m 0.02579058, -0.18336127, -0.12135931, 1.96577500, -0.038194â€¦
## $ V19    [3m[38;5;246m<dbl>[39m[23m 0.40399296, -0.14578304, -2.26185710, -1.23262197, 0.803486â€¦
## $ V20    [3m[38;5;246m<dbl>[39m[23m 0.25141210, -0.06908314, 0.52497973, -0.20803778, 0.4085423â€¦
## $ V21    [3m[38;5;246m<dbl>[39m[23m -0.018306778, -0.225775248, 0.247998153, -0.108300452, -0.0â€¦
## $ V22    [3m[38;5;246m<dbl>[39m[23m 0.277837576, -0.638671953, 0.771679402, 0.005273597, 0.7982â€¦
## $ V23    [3m[38;5;246m<dbl>[39m[23m -0.110473910, 0.101288021, 0.909412262, -0.190320519, -0.13â€¦
## $ V24    [3m[38;5;246m<dbl>[39m[23m 0.06692807, -0.33984648, -0.68928096, -1.17557533, 0.141266â€¦
## $ V25    [3m[38;5;246m<dbl>[39m[23m 0.12853936, 0.16717040, -0.32764183, 0.64737603, -0.2060095â€¦
## $ V26    [3m[38;5;246m<dbl>[39m[23m -0.18911484, 0.12589453, -0.13909657, -0.22192884, 0.502292â€¦
## $ V27    [3m[38;5;246m<dbl>[39m[23m 0.133558377, -0.008983099, -0.055352794, 0.062722849, 0.219â€¦
## $ V28    [3m[38;5;246m<dbl>[39m[23m -0.021053053, 0.014724169, -0.059751841, 0.061457629, 0.215â€¦
## $ Amount [3m[38;5;246m<dbl>[39m[23m 149.62, 2.69, 378.66, 123.50, 69.99, 3.67, 4.99, 40.80, 93.â€¦
## $ Class  [3m[38;5;246m<int>[39m[23m 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,â€¦
```

## Prepare a validation set


``` r
val_idx <- nrow(df) %>% sample.int(., round( . * 0.2))
val_df <- df[val_idx, ]
train_df <- df[-val_idx, ]

cat("Number of training samples:", nrow(train_df), "\n")
```

```
## Number of training samples: 227846
```

``` r
cat("Number of validation samples:", nrow(val_df), "\n")
```

```
## Number of validation samples: 56961
```

## Analyze class imbalance in the targets


``` r
counts <- table(train_df$Class)
counts
```

```
##
##      0      1
## 227455    391
```

``` r
cat(sprintf("Number of positive samples in training data: %i (%.2f%% of total)",
            counts["1"], 100 * counts["1"] / sum(counts)))
```

```
## Number of positive samples in training data: 391 (0.17% of total)
```

``` r
weight_for_0 = 1 / counts["0"]
weight_for_1 = 1 / counts["1"]
```

## Normalize the data using training set statistics


``` r
feature_names <- colnames(train_df) %>% setdiff("Class")

train_features <- as.matrix(train_df[feature_names])
train_targets <- as.matrix(train_df$Class)

val_features <- as.matrix(val_df[feature_names])
val_targets <- as.matrix(val_df$Class)

train_features %<>% scale()
val_features %<>% scale(center = attr(train_features, "scaled:center"),
                        scale = attr(train_features, "scaled:scale"))
```

## Build a binary classification model


``` r
model <-
  keras_model_sequential(input_shape = ncol(train_features)) |>
  layer_dense(256, activation = "relu") |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(256, activation = "relu") |>
  layer_dropout(0.3) |>
  layer_dense(1, activation = "sigmoid")

model
```

```
## [1mModel: "sequential"[0m
## â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
## â”ƒ[1m [0m[1mLayer (type)                   [0m[1m [0mâ”ƒ[1m [0m[1mOutput Shape          [0m[1m [0mâ”ƒ[1m [0m[1m      Param #[0m[1m [0mâ”ƒ
## â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
## â”‚ dense ([38;5;33mDense[0m)                   â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚         [38;5;34m7,936[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_1 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚        [38;5;34m65,792[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dropout ([38;5;33mDropout[0m)               â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚             [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_2 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚        [38;5;34m65,792[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dropout_1 ([38;5;33mDropout[0m)             â”‚ ([38;5;45mNone[0m, [38;5;34m256[0m)            â”‚             [38;5;34m0[0m â”‚
## â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
## â”‚ dense_3 ([38;5;33mDense[0m)                 â”‚ ([38;5;45mNone[0m, [38;5;34m1[0m)              â”‚           [38;5;34m257[0m â”‚
## â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
## [1m Total params: [0m[38;5;34m139,777[0m (546.00 KB)
## [1m Trainable params: [0m[38;5;34m139,777[0m (546.00 KB)
## [1m Non-trainable params: [0m[38;5;34m0[0m (0.00 B)
```

## Train the model with `class_weight` argument


``` r
metrics <- list(
  metric_false_negatives(name = "fn"),
  metric_false_positives(name = "fp"),
  metric_true_negatives(name = "tn"),
  metric_true_positives(name = "tp"),
  metric_precision(name = "precision"),
  metric_recall(name = "recall")
)
model |> compile(
  optimizer = optimizer_adam(1e-2),
  loss = "binary_crossentropy",
  metrics = metrics
)
callbacks <- list(
  callback_model_checkpoint("fraud_model_at_epoch_{epoch}.keras")
)

class_weight <- list("0" = weight_for_0,
                     "1" = weight_for_1)

model |> fit(
  train_features, train_targets,
  validation_data = list(val_features, val_targets),
  class_weight = class_weight,
  batch_size = 2048,
  epochs = 30,
  callbacks = callbacks,
  verbose = 2
)
```

```
## Epoch 1/30
## 112/112 - 3s - 27ms/step - fn: 48.0000 - fp: 25075.0000 - loss: 2.4315e-06 - precision: 0.0135 - recall: 0.8772 - tn: 202380.0000 - tp: 343.0000 - val_fn: 6.0000 - val_fp: 1675.0000 - val_loss: 0.1624 - val_precision: 0.0537 - val_recall: 0.9406 - val_tn: 55185.0000 - val_tp: 95.0000
## Epoch 2/30
## 112/112 - 1s - 6ms/step - fn: 32.0000 - fp: 8352.0000 - loss: 1.3447e-06 - precision: 0.0412 - recall: 0.9182 - tn: 219103.0000 - tp: 359.0000 - val_fn: 7.0000 - val_fp: 1734.0000 - val_loss: 0.1158 - val_precision: 0.0514 - val_recall: 0.9307 - val_tn: 55126.0000 - val_tp: 94.0000
## Epoch 3/30
## 112/112 - 0s - 2ms/step - fn: 30.0000 - fp: 8111.0000 - loss: 1.1916e-06 - precision: 0.0426 - recall: 0.9233 - tn: 219344.0000 - tp: 361.0000 - val_fn: 9.0000 - val_fp: 774.0000 - val_loss: 0.0782 - val_precision: 0.1062 - val_recall: 0.9109 - val_tn: 56086.0000 - val_tp: 92.0000
## Epoch 4/30
## 112/112 - 0s - 2ms/step - fn: 26.0000 - fp: 7593.0000 - loss: 1.0158e-06 - precision: 0.0459 - recall: 0.9335 - tn: 219862.0000 - tp: 365.0000 - val_fn: 6.0000 - val_fp: 2726.0000 - val_loss: 0.1151 - val_precision: 0.0337 - val_recall: 0.9406 - val_tn: 54134.0000 - val_tp: 95.0000
## Epoch 5/30
## 112/112 - 0s - 2ms/step - fn: 20.0000 - fp: 8398.0000 - loss: 9.4607e-07 - precision: 0.0423 - recall: 0.9488 - tn: 219057.0000 - tp: 371.0000 - val_fn: 10.0000 - val_fp: 603.0000 - val_loss: 0.0334 - val_precision: 0.1311 - val_recall: 0.9010 - val_tn: 56257.0000 - val_tp: 91.0000
## Epoch 6/30
## 112/112 - 0s - 2ms/step - fn: 16.0000 - fp: 7796.0000 - loss: 8.3846e-07 - precision: 0.0459 - recall: 0.9591 - tn: 219659.0000 - tp: 375.0000 - val_fn: 8.0000 - val_fp: 2267.0000 - val_loss: 0.0939 - val_precision: 0.0394 - val_recall: 0.9208 - val_tn: 54593.0000 - val_tp: 93.0000
## Epoch 7/30
## 112/112 - 0s - 2ms/step - fn: 10.0000 - fp: 6677.0000 - loss: 5.8589e-07 - precision: 0.0540 - recall: 0.9744 - tn: 220778.0000 - tp: 381.0000 - val_fn: 8.0000 - val_fp: 2472.0000 - val_loss: 0.0961 - val_precision: 0.0363 - val_recall: 0.9208 - val_tn: 54388.0000 - val_tp: 93.0000
## Epoch 8/30
## 112/112 - 0s - 2ms/step - fn: 11.0000 - fp: 8095.0000 - loss: 7.6202e-07 - precision: 0.0448 - recall: 0.9719 - tn: 219360.0000 - tp: 380.0000 - val_fn: 10.0000 - val_fp: 1295.0000 - val_loss: 0.0809 - val_precision: 0.0657 - val_recall: 0.9010 - val_tn: 55565.0000 - val_tp: 91.0000
## Epoch 9/30
## 112/112 - 0s - 2ms/step - fn: 7.0000 - fp: 5626.0000 - loss: 5.2205e-07 - precision: 0.0639 - recall: 0.9821 - tn: 221829.0000 - tp: 384.0000 - val_fn: 9.0000 - val_fp: 1014.0000 - val_loss: 0.0509 - val_precision: 0.0832 - val_recall: 0.9109 - val_tn: 55846.0000 - val_tp: 92.0000
## Epoch 10/30
## 112/112 - 0s - 2ms/step - fn: 9.0000 - fp: 6099.0000 - loss: 5.7624e-07 - precision: 0.0589 - recall: 0.9770 - tn: 221356.0000 - tp: 382.0000 - val_fn: 10.0000 - val_fp: 1141.0000 - val_loss: 0.0485 - val_precision: 0.0739 - val_recall: 0.9010 - val_tn: 55719.0000 - val_tp: 91.0000
## Epoch 11/30
## 112/112 - 0s - 2ms/step - fn: 8.0000 - fp: 5545.0000 - loss: 5.0995e-07 - precision: 0.0646 - recall: 0.9795 - tn: 221910.0000 - tp: 383.0000 - val_fn: 9.0000 - val_fp: 1951.0000 - val_loss: 0.0903 - val_precision: 0.0450 - val_recall: 0.9109 - val_tn: 54909.0000 - val_tp: 92.0000
## Epoch 12/30
## 112/112 - 0s - 2ms/step - fn: 13.0000 - fp: 8476.0000 - loss: 1.0215e-06 - precision: 0.0427 - recall: 0.9668 - tn: 218979.0000 - tp: 378.0000 - val_fn: 9.0000 - val_fp: 2929.0000 - val_loss: 0.1346 - val_precision: 0.0305 - val_recall: 0.9109 - val_tn: 53931.0000 - val_tp: 92.0000
## Epoch 13/30
## 112/112 - 0s - 2ms/step - fn: 13.0000 - fp: 7170.0000 - loss: 8.1795e-07 - precision: 0.0501 - recall: 0.9668 - tn: 220285.0000 - tp: 378.0000 - val_fn: 10.0000 - val_fp: 1355.0000 - val_loss: 0.0592 - val_precision: 0.0629 - val_recall: 0.9010 - val_tn: 55505.0000 - val_tp: 91.0000
## Epoch 14/30
## 112/112 - 0s - 2ms/step - fn: 5.0000 - fp: 4084.0000 - loss: 3.7254e-07 - precision: 0.0864 - recall: 0.9872 - tn: 223371.0000 - tp: 386.0000 - val_fn: 11.0000 - val_fp: 428.0000 - val_loss: 0.0233 - val_precision: 0.1737 - val_recall: 0.8911 - val_tn: 56432.0000 - val_tp: 90.0000
## Epoch 15/30
## 112/112 - 0s - 2ms/step - fn: 6.0000 - fp: 4073.0000 - loss: 4.3721e-07 - precision: 0.0864 - recall: 0.9847 - tn: 223382.0000 - tp: 385.0000 - val_fn: 12.0000 - val_fp: 659.0000 - val_loss: 0.0302 - val_precision: 0.1190 - val_recall: 0.8812 - val_tn: 56201.0000 - val_tp: 89.0000
## Epoch 16/30
## 112/112 - 0s - 2ms/step - fn: 11.0000 - fp: 5797.0000 - loss: 5.6359e-07 - precision: 0.0615 - recall: 0.9719 - tn: 221658.0000 - tp: 380.0000 - val_fn: 9.0000 - val_fp: 2041.0000 - val_loss: 0.1492 - val_precision: 0.0431 - val_recall: 0.9109 - val_tn: 54819.0000 - val_tp: 92.0000
## Epoch 17/30
## 112/112 - 0s - 2ms/step - fn: 8.0000 - fp: 6758.0000 - loss: 6.8748e-07 - precision: 0.0536 - recall: 0.9795 - tn: 220697.0000 - tp: 383.0000 - val_fn: 8.0000 - val_fp: 2855.0000 - val_loss: 0.1215 - val_precision: 0.0315 - val_recall: 0.9208 - val_tn: 54005.0000 - val_tp: 93.0000
## Epoch 18/30
## 112/112 - 0s - 2ms/step - fn: 6.0000 - fp: 4190.0000 - loss: 3.8817e-07 - precision: 0.0842 - recall: 0.9847 - tn: 223265.0000 - tp: 385.0000 - val_fn: 13.0000 - val_fp: 615.0000 - val_loss: 0.0280 - val_precision: 0.1252 - val_recall: 0.8713 - val_tn: 56245.0000 - val_tp: 88.0000
## Epoch 19/30
## 112/112 - 0s - 2ms/step - fn: 3.0000 - fp: 4748.0000 - loss: 5.6549e-07 - precision: 0.0755 - recall: 0.9923 - tn: 222707.0000 - tp: 388.0000 - val_fn: 10.0000 - val_fp: 1481.0000 - val_loss: 0.1558 - val_precision: 0.0579 - val_recall: 0.9010 - val_tn: 55379.0000 - val_tp: 91.0000
## Epoch 20/30
## 112/112 - 0s - 2ms/step - fn: 11.0000 - fp: 5168.0000 - loss: 7.1889e-07 - precision: 0.0685 - recall: 0.9719 - tn: 222287.0000 - tp: 380.0000 - val_fn: 10.0000 - val_fp: 994.0000 - val_loss: 0.0616 - val_precision: 0.0839 - val_recall: 0.9010 - val_tn: 55866.0000 - val_tp: 91.0000
## Epoch 21/30
## 112/112 - 0s - 2ms/step - fn: 4.0000 - fp: 3523.0000 - loss: 3.6302e-07 - precision: 0.0990 - recall: 0.9898 - tn: 223932.0000 - tp: 387.0000 - val_fn: 10.0000 - val_fp: 577.0000 - val_loss: 0.0370 - val_precision: 0.1362 - val_recall: 0.9010 - val_tn: 56283.0000 - val_tp: 91.0000
## Epoch 22/30
## 112/112 - 0s - 2ms/step - fn: 4.0000 - fp: 2947.0000 - loss: 3.0530e-07 - precision: 0.1161 - recall: 0.9898 - tn: 224508.0000 - tp: 387.0000 - val_fn: 11.0000 - val_fp: 822.0000 - val_loss: 0.0355 - val_precision: 0.0987 - val_recall: 0.8911 - val_tn: 56038.0000 - val_tp: 90.0000
## Epoch 23/30
## 112/112 - 0s - 2ms/step - fn: 2.0000 - fp: 2258.0000 - loss: 2.1487e-07 - precision: 0.1470 - recall: 0.9949 - tn: 225197.0000 - tp: 389.0000 - val_fn: 10.0000 - val_fp: 839.0000 - val_loss: 0.0462 - val_precision: 0.0978 - val_recall: 0.9010 - val_tn: 56021.0000 - val_tp: 91.0000
## Epoch 24/30
## 112/112 - 0s - 2ms/step - fn: 2.0000 - fp: 3157.0000 - loss: 2.5914e-07 - precision: 0.1097 - recall: 0.9949 - tn: 224298.0000 - tp: 389.0000 - val_fn: 10.0000 - val_fp: 667.0000 - val_loss: 0.0316 - val_precision: 0.1201 - val_recall: 0.9010 - val_tn: 56193.0000 - val_tp: 91.0000
## Epoch 25/30
## 112/112 - 0s - 2ms/step - fn: 4.0000 - fp: 3326.0000 - loss: 3.8410e-07 - precision: 0.1042 - recall: 0.9898 - tn: 224129.0000 - tp: 387.0000 - val_fn: 8.0000 - val_fp: 1191.0000 - val_loss: 0.0587 - val_precision: 0.0724 - val_recall: 0.9208 - val_tn: 55669.0000 - val_tp: 93.0000
## Epoch 26/30
## 112/112 - 0s - 2ms/step - fn: 5.0000 - fp: 4168.0000 - loss: 3.8526e-07 - precision: 0.0848 - recall: 0.9872 - tn: 223287.0000 - tp: 386.0000 - val_fn: 11.0000 - val_fp: 670.0000 - val_loss: 0.0311 - val_precision: 0.1184 - val_recall: 0.8911 - val_tn: 56190.0000 - val_tp: 90.0000
## Epoch 27/30
## 112/112 - 0s - 2ms/step - fn: 2.0000 - fp: 3433.0000 - loss: 4.0116e-07 - precision: 0.1018 - recall: 0.9949 - tn: 224022.0000 - tp: 389.0000 - val_fn: 10.0000 - val_fp: 1393.0000 - val_loss: 0.1997 - val_precision: 0.0613 - val_recall: 0.9010 - val_tn: 55467.0000 - val_tp: 91.0000
## Epoch 28/30
## 112/112 - 0s - 2ms/step - fn: 7.0000 - fp: 5294.0000 - loss: 6.9852e-07 - precision: 0.0676 - recall: 0.9821 - tn: 222161.0000 - tp: 384.0000 - val_fn: 11.0000 - val_fp: 1467.0000 - val_loss: 0.1060 - val_precision: 0.0578 - val_recall: 0.8911 - val_tn: 55393.0000 - val_tp: 90.0000
## Epoch 29/30
## 112/112 - 0s - 2ms/step - fn: 4.0000 - fp: 4214.0000 - loss: 4.4154e-07 - precision: 0.0841 - recall: 0.9898 - tn: 223241.0000 - tp: 387.0000 - val_fn: 11.0000 - val_fp: 729.0000 - val_loss: 0.0520 - val_precision: 0.1099 - val_recall: 0.8911 - val_tn: 56131.0000 - val_tp: 90.0000
## Epoch 30/30
## 112/112 - 0s - 2ms/step - fn: 3.0000 - fp: 2367.0000 - loss: 3.0825e-07 - precision: 0.1408 - recall: 0.9923 - tn: 225088.0000 - tp: 388.0000 - val_fn: 12.0000 - val_fp: 1998.0000 - val_loss: 0.1274 - val_precision: 0.0426 - val_recall: 0.8812 - val_tn: 54862.0000 - val_tp: 89.0000
```


``` r
val_pred <- model %>%
  predict(val_features) %>%
  { as.integer(. > 0.5) }
```

```
## 1781/1781 - 1s - 314us/step
```

``` r
pred_correct <- val_df$Class == val_pred
cat(sprintf("Validation accuracy: %.2f", mean(pred_correct)))
```

```
## Validation accuracy: 0.96
```

``` r
fraudulent <- val_df$Class == 1

n_fraudulent_detected <- sum(fraudulent & pred_correct)
n_fraudulent_missed <- sum(fraudulent & !pred_correct)
n_legitimate_flagged <- sum(!fraudulent & !pred_correct)
```

## Conclusions

At the end of training, out of
56,961 validation transactions, we
are:

- Correctly identifying
  89 of them as
  fraudulent
- Missing 12
  fraudulent transactions
- At the cost of incorrectly flagging
  1,998 legitimate
  transactions

In the real world, one would put an even higher weight on class 1,
so as to reflect that False Negatives are more costly than False Positives.

Next time your credit card gets declined in an online purchase -- this is why.

<!-- | Trained Model                                                                                                                                                          | Demo                                                                                                                                                                             | -->
<!-- |------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| -->
<!-- | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Imbalanced%20Classification-black.svg)](https://huggingface.co/keras-io/imbalanced_classification) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Imbalanced%20Classification-black.svg)](https://huggingface.co/spaces/keras-io/Credit_Card_Fraud_Detection) | -->

<!-- : Example available on HuggingFace. -->
