<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Complete guide to training &amp; evaluation with `fit()` and `evaluate()`.">
<title>Training &amp; evaluation with the built-in methods • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Fira_Mono-0.4.9/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Training &amp; evaluation with the built-in methods">
<meta property="og:description" content="Complete guide to training &amp; evaluation with `fit()` and `evaluate()`.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Getting Started</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Training &amp; evaluation with the built-in methods</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes-src/training_with_built_in_methods.Rmd" class="external-link"><code>vignettes-src/training_with_built_in_methods.Rmd</code></a></small>
      <div class="d-none name"><code>training_with_built_in_methods.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://keras.posit.co/">keras3</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This guide covers training, evaluation, and prediction (inference)
models when using built-in APIs for training &amp; validation (such as
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, <code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code> and
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>).</p>
<p>If you are interested in leveraging <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> while
specifying your own training step function, see the <a href="custom_train_step_in_tensorflow.html">Customizing what happens in
<code>fit()</code> guide</a>.</p>
<p>If you are interested in writing your own training &amp; evaluation
loops from scratch, see the guide <a href="writing_a_custom_training_loop_in_tensorflow.html">“writing a
training loop from scratch”</a>.</p>
<p>In general, whether you are using built-in loops or writing your own,
model training &amp; evaluation works strictly in the same way across
every kind of Keras model – Sequential models, models built with the
Functional API, and models written from scratch via model
subclassing.</p>
<p>This guide doesn’t cover distributed training, which is covered in
our <a href="distributed_training_with_tensorflow.html">guide to
multi-GPU &amp; distributed training</a>.</p>
</div>
<div class="section level2">
<h2 id="api-overview-a-first-end-to-end-example">API overview: a first end-to-end example<a class="anchor" aria-label="anchor" href="#api-overview-a-first-end-to-end-example"></a>
</h2>
<p>When passing data to the built-in training loops of a model, you
should either use:</p>
<ul>
<li>Arrays (if your data is small and fits in memory)</li>
<li>
<code>tf_dataset</code> objects</li>
<li>PyTorch <code>DataLoader</code> instances</li>
</ul>
<p>In the next few paragraphs, we’ll use the MNIST dataset as NumPy
arrays, in order to demonstrate how to use optimizers, losses, and
metrics. Afterwards, we’ll take a close look at each of the other
options.</p>
<p>Let’s consider the following model (here, we build in with the
Functional API, but it could be a Sequential model or a subclassed model
as well):</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">784</span>, name<span class="op">=</span><span class="st">"digits"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_1"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_2"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span>, activation <span class="op">=</span> <span class="st">"softmax"</span>, name <span class="op">=</span> <span class="st">"predictions"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="va">inputs</span>, outputs <span class="op">=</span> <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="font-weight: bold;">Model: "functional_1"</span></span></span>
<span><span class="co">## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓</span></span>
<span><span class="co">## ┃<span style="font-weight: bold;"> Layer (type)                    </span>┃<span style="font-weight: bold;"> Output Shape           </span>┃<span style="font-weight: bold;">       Param # </span>┃</span></span>
<span><span class="co">## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩</span></span>
<span><span class="co">## │ digits (<span style="color: #0087FF;">InputLayer</span>)             │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">784</span>)            │             <span style="color: #00AF00;">0</span> │</span></span>
<span><span class="co">## ├─────────────────────────────────┼────────────────────────┼───────────────┤</span></span>
<span><span class="co">## │ dense_1 (<span style="color: #0087FF;">Dense</span>)                 │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">64</span>)             │        <span style="color: #00AF00;">50,240</span> │</span></span>
<span><span class="co">## ├─────────────────────────────────┼────────────────────────┼───────────────┤</span></span>
<span><span class="co">## │ dense_2 (<span style="color: #0087FF;">Dense</span>)                 │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">64</span>)             │         <span style="color: #00AF00;">4,160</span> │</span></span>
<span><span class="co">## ├─────────────────────────────────┼────────────────────────┼───────────────┤</span></span>
<span><span class="co">## │ predictions (<span style="color: #0087FF;">Dense</span>)             │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">10</span>)             │           <span style="color: #00AF00;">650</span> │</span></span>
<span><span class="co">## └─────────────────────────────────┴────────────────────────┴───────────────┘</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Total params: </span><span style="color: #00AF00;">55,050</span> (215.04 KB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Trainable params: </span><span style="color: #00AF00;">55,050</span> (215.04 KB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Non-trainable params: </span><span style="color: #00AF00;">0</span> (0.00 B)</span></span></code></pre>
<p>Here’s what the typical end-to-end workflow looks like, consisting
of:</p>
<ul>
<li>Training</li>
<li>Validation on a holdout set generated from the original training
data</li>
<li>Evaluation on the test data</li>
</ul>
<p>We’ll use MNIST data for this example.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="va">y_test</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Preprocess the data (these are NumPy arrays)</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span><span class="va">x_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">10000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="co"># Reserve 10,000 samples for validation</span></span>
<span><span class="va">x_val</span> <span class="op">&lt;-</span> <span class="va">x_train</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10000</span>,<span class="op">]</span></span>
<span><span class="va">y_val</span> <span class="op">&lt;-</span> <span class="va">y_train</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10000</span><span class="op">]</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="va">x_train</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10000</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="va">y_train</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10000</span><span class="op">)</span><span class="op">]</span></span></code></pre></div>
<p>We specify the training configuration (optimizer, loss, metrics):</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  <span class="co"># Optimizer</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  <span class="co"># Loss function to minimize</span></span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  <span class="co"># List of metrics to monitor</span></span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_sparse_categorical_accuracy.html">metric_sparse_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>We call <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, which will train the model by slicing the
data into “batches” of size <code>batch_size</code>, and repeatedly
iterating over the entire dataset for a given number of
<code>epochs</code>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">history</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="va">x_train</span>, <span class="va">y_train</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  <span class="co"># We pass some validation for</span></span>
<span>  <span class="co"># monitoring validation loss and metrics</span></span>
<span>  <span class="co"># at the end of each epoch</span></span>
<span>  validation_data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_val</span>, <span class="va">y_val</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/2</span></span>
<span><span class="co">## 782/782 - 2s - 3ms/step - loss: 0.3481 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.1968 - val_sparse_categorical_accuracy: 0.9444</span></span>
<span><span class="co">## Epoch 2/2</span></span>
<span><span class="co">## 782/782 - 1s - 756us/step - loss: 0.1651 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.1389 - val_sparse_categorical_accuracy: 0.9598</span></span></code></pre>
<p>The returned <code>history</code> object holds a record of the loss
values and metric values during training:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">history</span></span></code></pre></div>
<pre><code><span><span class="co">##</span></span>
<span><span class="co">## Final epoch (plot to see history):</span></span>
<span><span class="co">##                            loss: 0.1651</span></span>
<span><span class="co">##     sparse_categorical_accuracy: 0.952</span></span>
<span><span class="co">##                        val_loss: 0.1389</span></span>
<span><span class="co">## val_sparse_categorical_accuracy: 0.9598</span></span></code></pre>
<p>We evaluate the model on the test data via
<code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code>:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Evaluate the model on the test data using `evaluate`</span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="va">y_test</span>, batch_size<span class="op">=</span><span class="fl">128</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 79/79 - 0s - 3ms/step - loss: 0.1344 - sparse_categorical_accuracy: 0.9591</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">results</span></span></code></pre></div>
<pre><code><span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.1343915</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## $sparse_categorical_accuracy</span></span>
<span><span class="co">## [1] 0.9591</span></span></code></pre>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Generate predictions (probabilities -- the output of the last layer)</span></span>
<span><span class="co"># on new data using `predict`</span></span>
<span><span class="va">predictions</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">x_test</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>,<span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 1/1 - 0s - 134ms/step</span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">predictions</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1]  2 10</span></span></code></pre>
<p>Now, let’s review each piece of this workflow in detail.</p>
</div>
<div class="section level2">
<h2 id="the-compile-method-specifying-a-loss-metrics-and-an-optimizer">The <code>compile()</code> method: specifying a loss, metrics, and
an optimizer<a class="anchor" aria-label="anchor" href="#the-compile-method-specifying-a-loss-metrics-and-an-optimizer"></a>
</h2>
<p>To train a model with <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, you need to specify a loss
function, an optimizer, and optionally, some metrics to monitor.</p>
<p>You pass these to the model as arguments to the
<code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> method:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span>learning_rate <span class="op">=</span> <span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_sparse_categorical_accuracy.html">metric_sparse_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The <code>metrics</code> argument should be a list – your model can
have any number of metrics.</p>
<p>If your model has multiple outputs, you can specify different losses
and metrics for each output, and you can modulate the contribution of
each output to the total loss of the model. You will find more details
about this in the <strong>Passing data to multi-input, multi-output
models</strong> section.</p>
<p>Note that if you’re satisfied with the default settings, in many
cases the optimizer, loss, and metrics can be specified via string
identifiers as a shortcut:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="st">"rmsprop"</span>,</span>
<span>  loss <span class="op">=</span> <span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sparse_categorical_accuracy"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>For later reuse, let’s put our model definition and compile step in
functions; we will call them several times across different examples in
this guide.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">get_uncompiled_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">784</span>, name <span class="op">=</span> <span class="st">"digits"</span><span class="op">)</span></span>
<span>  <span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_1"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_2"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span>, activation <span class="op">=</span> <span class="st">"softmax"</span>, name <span class="op">=</span> <span class="st">"predictions"</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="va">inputs</span>, outputs <span class="op">=</span> <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">get_compiled_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_uncompiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>    optimizer <span class="op">=</span> <span class="st">"rmsprop"</span>,</span>
<span>    loss <span class="op">=</span> <span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span>    metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sparse_categorical_accuracy"</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span>  <span class="va">model</span></span>
<span><span class="op">}</span></span></code></pre></div>
<div class="section level3">
<h3 id="many-built-in-optimizers-losses-and-metrics-are-available">Many built-in optimizers, losses, and metrics are available<a class="anchor" aria-label="anchor" href="#many-built-in-optimizers-losses-and-metrics-are-available"></a>
</h3>
<p>In general, you won’t have to create your own losses, metrics, or
optimizers from scratch, because what you need is likely to be already
part of the Keras API:</p>
<p>Optimizers:</p>
<ul>
<li>[<code><a href="../reference/optimizer_sgd.html">optimizer_sgd()</a></code>] (with or without momentum)</li>
<li>[<code><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop()</a></code>]</li>
<li>[<code><a href="../reference/optimizer_adam.html">optimizer_adam()</a></code>]</li>
<li>etc.</li>
</ul>
<p>Losses:</p>
<ul>
<li>[<code><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error()</a></code>]</li>
<li>[<code><a href="../reference/loss_kl_divergence.html">loss_kl_divergence()</a></code>]</li>
<li>[<code><a href="../reference/loss_cosine_similarity.html">loss_cosine_similarity()</a></code>]</li>
<li>etc.</li>
</ul>
<p>Metrics:</p>
<ul>
<li>[<code><a href="../reference/metric_auc.html">metric_auc()</a></code>]</li>
<li>[<code><a href="../reference/metric_precision.html">metric_precision()</a></code>]</li>
<li>[<code><a href="../reference/metric_recall.html">metric_recall()</a></code>]</li>
<li>etc.</li>
</ul>
</div>
<div class="section level3">
<h3 id="custom-losses">Custom losses<a class="anchor" aria-label="anchor" href="#custom-losses"></a>
</h3>
<p>If you need to create a custom loss, Keras provides three ways to do
so.</p>
<p>The first method involves creating a function that accepts inputs
<code>y_true</code> and <code>y_pred</code>. The following example shows
a loss function that computes the mean squared error between the real
data and the predictions:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">custom_mean_squared_error</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y_true</span>, <span class="va">y_pred</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="../reference/op_mean.html">op_mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/op_square.html">op_square</a></span><span class="op">(</span><span class="va">y_true</span> <span class="op">-</span> <span class="va">y_pred</span><span class="op">)</span>, axis <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_uncompiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>optimizer <span class="op">=</span> <span class="st">"adam"</span>,</span>
<span>                 loss <span class="op">=</span> <span class="va">custom_mean_squared_error</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># We need to one-hot encode the labels to use MSE</span></span>
<span><span class="va">y_train_one_hot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_one_hot.html">op_one_hot</a></span><span class="op">(</span><span class="va">y_train</span>, num_classes <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train_one_hot</span>, batch_size <span class="op">=</span> <span class="fl">64</span>, epochs <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/2</span></span>
<span><span class="co">## 782/782 - 2s - 2ms/step - loss: 0.0157</span></span>
<span><span class="co">## Epoch 2/2</span></span>
<span><span class="co">## 782/782 - 0s - 636us/step - loss: 0.0081</span></span></code></pre>
<p>If you need a loss function that takes in parameters beside
<code>y_true</code> and <code>y_pred</code>, you can subclass the Keras
base <code>Loss</code> class using [<code><a href="../reference/Loss.html">Loss()</a></code>] and implement
the following two methods:</p>
<ul>
<li>
<code>initialize()</code>: accept parameters to pass during the call
of your loss function</li>
<li>
<code>call(y_true, y_pred)</code>: use the targets (y_true) and the
model predictions (y_pred) to compute the model’s loss</li>
</ul>
<p>Let’s say you want to use mean squared error, but with an added term
that will de-incentivize prediction values far from 0.5 (we assume that
the categorical targets are one-hot encoded and take values between 0
and 1). This creates an incentive for the model not to be too confident,
which may help reduce overfitting (we won’t know if it works until we
try!).</p>
<p>Here’s how you would do it:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">loss_custom_mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Loss.html">Loss</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">"CustomMSE"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">regularization_factor</span> <span class="op">=</span> <span class="fl">0.1</span>, <span class="va">name</span> <span class="op">=</span> <span class="st">"custom_mse"</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span>name <span class="op">=</span> <span class="va">name</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">regularization_factor</span> <span class="op">&lt;-</span> <span class="va">regularization_factor</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">y_true</span>, <span class="va">y_pred</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_mean.html">op_mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/op_square.html">op_square</a></span><span class="op">(</span><span class="va">y_true</span> <span class="op">-</span> <span class="va">y_pred</span><span class="op">)</span>, axis <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="va">reg</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_mean.html">op_mean</a></span><span class="op">(</span><span class="fu"><a href="../reference/op_square.html">op_square</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">-</span> <span class="va">y_pred</span><span class="op">)</span>, axis <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="va">mse</span> <span class="op">+</span> <span class="va">reg</span> <span class="op">*</span> <span class="va">self</span><span class="op">$</span><span class="va">regularization_factor</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_uncompiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>optimizer<span class="op">=</span><span class="st">"adam"</span>, loss <span class="op">=</span> <span class="fu">loss_custom_mse</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">y_train_one_hot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_one_hot.html">op_one_hot</a></span><span class="op">(</span><span class="va">y_train</span>, num_classes<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train_one_hot</span>, batch_size<span class="op">=</span><span class="fl">64</span>, epochs<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 2s - 2ms/step - loss: 0.0383</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="custom-metrics">Custom metrics<a class="anchor" aria-label="anchor" href="#custom-metrics"></a>
</h3>
<p>If you need a metric that isn’t part of the API, you can easily
create custom metrics by subclassing the Keras base <code>Metric</code>
class using [<code><a href="../reference/Metric.html">Metric()</a></code>]. You will need to implement 4
methods:</p>
<ul>
<li>
<code>initialize()</code>, in which you will create state variables
for your metric.</li>
<li>
<code>update_state(y_true, y_pred, sample_weight = NULL)</code>,
which uses the targets y_true and the model predictions y_pred to update
the state variables.</li>
<li>
<code>result()</code>, which uses the state variables to compute the
final results.</li>
<li>
<code><a href="../reference/reset_state.html">reset_state()</a></code>, which reinitializes the state of the
metric.</li>
</ul>
<p>State update and results computation are kept separate (in
<code>update_state()</code> and <code>result()</code>, respectively)
because in some cases, the results computation might be very expensive
and would only be done periodically.</p>
<p>Here’s a simple example showing how to implement a
<code>CategoricalTruePositives</code> metric that counts how many
samples were correctly classified as belonging to a given class:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">metric_categorical_true_positives</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Metric.html">Metric</a></span><span class="op">(</span></span>
<span>  <span class="st">"CategoricalTruePositives"</span>,</span>
<span></span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">name</span> <span class="op">=</span> <span class="st">"categorical_true_positives"</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span>name <span class="op">=</span> <span class="va">name</span>, <span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_variable</span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>                                             name <span class="op">=</span> <span class="st">"ctp"</span>,</span>
<span>                                             initializer <span class="op">=</span> <span class="st">"zeros"</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span></span>
<span>  update_state <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">y_true</span>, <span class="va">y_pred</span>, <span class="va">sample_weight</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_argmax.html">op_argmax</a></span><span class="op">(</span><span class="va">y_pred</span>, axis <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/op_reshape.html">op_reshape</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">values</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">y_true</span>, <span class="st">"int32"</span><span class="op">)</span> <span class="op">==</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">y_pred</span>, <span class="st">"int32"</span><span class="op">)</span></span>
<span>    <span class="va">values</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">values</span>, <span class="st">"float32"</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">sample_weight</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">sample_weight</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">sample_weight</span>, <span class="st">"float32"</span><span class="op">)</span></span>
<span>      <span class="va">values</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_multiply.html">op_multiply</a></span><span class="op">(</span><span class="va">values</span>, <span class="va">sample_weight</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="fu"><a href="../reference/op_sum.html">op_sum</a></span><span class="op">(</span><span class="va">values</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span></span>
<span>  result <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span></span>
<span>  <span class="op">}</span>,</span>
<span></span>
<span>  reset_state <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span><span class="op">$</span><span class="fu">assign</span><span class="op">(</span><span class="fl">0.0</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_uncompiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span>learning_rate <span class="op">=</span> <span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu">metric_categorical_true_positives</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">history</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>, batch_size <span class="op">=</span> <span class="fl">64</span>, epochs <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/3</span></span>
<span><span class="co">## 782/782 - 1s - 2ms/step - categorical_true_positives: 360544.0000 - loss: 0.3421</span></span>
<span><span class="co">## Epoch 2/3</span></span>
<span><span class="co">## 782/782 - 1s - 651us/step - categorical_true_positives: 362643.0000 - loss: 0.1622</span></span>
<span><span class="co">## Epoch 3/3</span></span>
<span><span class="co">## 782/782 - 0s - 614us/step - categorical_true_positives: 363325.0000 - loss: 0.1179</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="handling-losses-and-metrics-that-dont-fit-the-standard-signature">Handling losses and metrics that don’t fit the standard
signature<a class="anchor" aria-label="anchor" href="#handling-losses-and-metrics-that-dont-fit-the-standard-signature"></a>
</h3>
<p>The overwhelming majority of losses and metrics can be computed from
<code>y_true</code> and <code>y_pred</code>, where <code>y_pred</code>
is an output of your model – but not all of them. For instance, a
regularization loss may only require the activation of a layer (there
are no targets in this case), and this activation may not be a model
output.</p>
<p>In such cases, you can call <code>self$add_loss(loss_value)</code>
from inside the call method of a custom layer. Losses added in this way
get added to the “main” loss during training (the one passed to
<code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code>). Here’s a simple example that adds activity
regularization (note that activity regularization is built-in in all
Keras layers – this layer is just for the sake of providing a concrete
example):</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">layer_custom_activity_regularizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Layer.html">Layer</a></span><span class="op">(</span></span>
<span>  <span class="st">"ActivityRegularization"</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">add_loss</span><span class="op">(</span><span class="fu"><a href="../reference/op_sum.html">op_sum</a></span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">*</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>    <span class="va">inputs</span>  <span class="co"># Pass-through layer.</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">784</span>, name <span class="op">=</span> <span class="st">"digits"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_1"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">layer_custom_activity_regularizer</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"dense_2"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span>, name <span class="op">=</span> <span class="st">"predictions"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="va">inputs</span>, outputs <span class="op">=</span> <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span>learning_rate <span class="op">=</span> <span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>                 loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The displayed loss will be much higher than before</span></span>
<span><span class="co"># due to the regularization component.</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>, batch_size <span class="op">=</span> <span class="fl">64</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 1s - 2ms/step - loss: 2.3949</span></span></code></pre>
<p>Note that when you pass losses via <code>add_loss()</code>, it
becomes possible to call <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> without a loss function,
since the model already has a loss to minimize.</p>
<p>Consider the following <code>LogisticEndpoint</code> layer: it takes
as inputs targets &amp; logits, and it tracks a crossentropy loss via
<code>add_loss()</code>.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">layer_logistic_endpoint</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Layer.html">Layer</a></span><span class="op">(</span></span>
<span>  <span class="st">"LogisticEndpoint"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">name</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span>name <span class="op">=</span> <span class="va">name</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_binary_crossentropy.html">loss_binary_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">targets</span>, <span class="va">logits</span>, <span class="va">sample_weights</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Compute the training-time loss value and add it</span></span>
<span>    <span class="co"># to the layer using `self.add_loss()`.</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">loss_fn</span><span class="op">(</span><span class="va">targets</span>, <span class="va">logits</span>, <span class="va">sample_weights</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">add_loss</span><span class="op">(</span><span class="va">loss</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Return the inference-time prediction tensor (for `predict()`).</span></span>
<span>    <span class="fu"><a href="../reference/op_softmax.html">op_softmax</a></span><span class="op">(</span><span class="va">logits</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>You can use it in a model with two inputs (input data &amp; targets),
compiled without a <code>loss</code> argument, like this:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">3</span>, name <span class="op">=</span> <span class="st">"inputs"</span><span class="op">)</span></span>
<span><span class="va">targets</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">10</span>, name <span class="op">=</span> <span class="st">"targets"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">logits</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">predictions</span> <span class="op">&lt;-</span> <span class="fu">layer_logistic_endpoint</span><span class="op">(</span>name <span class="op">=</span> <span class="st">"predictions"</span><span class="op">)</span><span class="op">(</span><span class="va">targets</span>, <span class="va">logits</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">targets</span><span class="op">)</span>,</span>
<span>                     outputs <span class="op">=</span> <span class="va">predictions</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>optimizer <span class="op">=</span> <span class="st">"adam"</span><span class="op">)</span>  <span class="co"># No loss argument!</span></span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  inputs <span class="op">=</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  targets <span class="op">=</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">data</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 1/1 - 1s - 509ms/step - loss: 1.0566</span></span></code></pre>
<p>For more information about training multi-input models, see the
section <strong>Passing data to multi-input, multi-output
models</strong>.</p>
</div>
<div class="section level3">
<h3 id="automatically-setting-apart-a-validation-holdout-set">Automatically setting apart a validation holdout set<a class="anchor" aria-label="anchor" href="#automatically-setting-apart-a-validation-holdout-set"></a>
</h3>
<p>In the first end-to-end example you saw, we used the
<code>validation_data</code> argument to pass a list of arrays
<code>list(x_val, y_val)</code> to the model for evaluating a validation
loss and validation metrics at the end of each epoch.</p>
<p>Here’s another option: the argument <code>validation_split</code>
allows you to automatically reserve part of your training data for
validation. The argument value represents the fraction of the data to be
reserved for validation, so it should be set to a number higher than 0
and lower than 1. For instance, <code>validation_split = 0.2</code>
means “use 20% of the data for validation”, and
<code>validation_split = 0.6</code> means “use 60% of the data for
validation”.</p>
<p>The way the validation is computed is by taking the last x% samples
of the arrays received by the <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> call, before any
shuffling.</p>
<p>Note that you can only use <code>validation_split</code> when
training with NumPy data.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>,</span>
<span>             batch_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>             validation_split <span class="op">=</span> <span class="fl">0.2</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 625/625 - 1s - 2ms/step - loss: 0.3694 - sparse_categorical_accuracy: 0.8948 - val_loss: 0.1911 - val_sparse_categorical_accuracy: 0.9453</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="training-evaluation-using-tf-dataset-objects">Training &amp; evaluation using TF <code>Dataset</code> objects<a class="anchor" aria-label="anchor" href="#training-evaluation-using-tf-dataset-objects"></a>
</h2>
<p>In the past few paragraphs, you’ve seen how to handle losses,
metrics, and optimizers, and you’ve seen how to use the
<code>validation_data</code> and <code>validation_split</code> arguments
in <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, when your data is passed as arrays.</p>
<p>Another option is to use an iterator-like, such as a
<code>tf.data.Dataset</code>, a PyTorch <code>DataLoader</code>, or an R
generator function. Let’s take look at the former.</p>
<p>The <a href="https://github.com/rstudio/tfdatasets" class="external-link">tfdatasets</a> R package containes a set of utilities
for loading and preprocessing data in a way that’s fast and scalable.
For a complete guide about creating <code>Datasets</code>, see the <a href="https://www.tensorflow.org/guide/data" class="external-link">tf.data
documentation</a>.</p>
<p><strong>You can use <code>tf.data</code> to train your Keras models
regardless of the backend you’re using – whether it’s JAX, PyTorch, or
TensorFlow.</strong> You can pass a <code>Dataset</code> instance
directly to the methods <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>, <code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code>, and
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/tfdatasets" class="external-link">tfdatasets</a></span>, exclude <span class="op">=</span> <span class="st">"shape"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># First, let's create a training Dataset instance.</span></span>
<span><span class="co"># For the sake of our example, we'll use the same MNIST data as before.</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Shuffle and slice the dataset.</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Now we get a test dataset.</span></span>
<span><span class="va">test_dataset</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_test</span>, <span class="va">y_test</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Since the dataset already takes care of batching,</span></span>
<span><span class="co"># we don't pass a `batch_size` argument.</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dataset</span>, epochs <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/3</span></span>
<span><span class="co">## 782/782 - 1s - 2ms/step - loss: 0.3417 - sparse_categorical_accuracy: 0.9033</span></span>
<span><span class="co">## Epoch 2/3</span></span>
<span><span class="co">## 782/782 - 1s - 642us/step - loss: 0.1602 - sparse_categorical_accuracy: 0.9525</span></span>
<span><span class="co">## Epoch 3/3</span></span>
<span><span class="co">## 782/782 - 0s - 597us/step - loss: 0.1166 - sparse_categorical_accuracy: 0.9653</span></span></code></pre>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># You can also evaluate or predict on a dataset.</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">test_dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 157/157 - 0s - 3ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9632</span></span></code></pre>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result</span></span></code></pre></div>
<pre><code><span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.114668</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## $sparse_categorical_accuracy</span></span>
<span><span class="co">## [1] 0.9632</span></span></code></pre>
<p>Note that the <code>Dataset</code> is reset at the end of each epoch,
so it can be reused of the next epoch.</p>
<p>If you want to run training only on a specific number of batches from
this Dataset, you can pass the <code>steps_per_epoch</code> argument,
which specifies how many training steps the model should run using this
Dataset before moving on to the next epoch.</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare the training dataset</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size <span class="op">=</span> <span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Only use the 100 batches per epoch (that's 64 * 100 samples)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dataset</span>, epochs <span class="op">=</span> <span class="fl">3</span>, steps_per_epoch <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/3</span></span>
<span><span class="co">## 100/100 - 1s - 7ms/step - loss: 0.8506 - sparse_categorical_accuracy: 0.7683</span></span>
<span><span class="co">## Epoch 2/3</span></span>
<span><span class="co">## 100/100 - 0s - 639us/step - loss: 0.3705 - sparse_categorical_accuracy: 0.8947</span></span>
<span><span class="co">## Epoch 3/3</span></span>
<span><span class="co">## 100/100 - 0s - 628us/step - loss: 0.3060 - sparse_categorical_accuracy: 0.9130</span></span></code></pre>
<p>You can also pass a <code>Dataset</code> instance as the
<code>validation_data</code> argument in <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare the training dataset</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare the validation dataset</span></span>
<span><span class="va">val_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_val</span>, <span class="va">y_val</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">val_dataset</span> <span class="op">&lt;-</span> <span class="va">val_dataset</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dataset</span>, epochs <span class="op">=</span> <span class="fl">1</span>, validation_data <span class="op">=</span> <span class="va">val_dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 2s - 2ms/step - loss: 0.3355 - sparse_categorical_accuracy: 0.9046 - val_loss: 0.2198 - val_sparse_categorical_accuracy: 0.9313</span></span></code></pre>
<p>At the end of each epoch, the model will iterate over the validation
dataset and compute the validation loss and validation metrics.</p>
<p>If you want to run validation only on a specific number of batches
from this dataset, you can pass the <code>validation_steps</code>
argument, which specifies how many validation steps the model should run
with the validation dataset before interrupting validation and moving on
to the next epoch:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span>  <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare the training dataset</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size <span class="op">=</span> <span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare the validation dataset</span></span>
<span><span class="va">val_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_val</span>, <span class="va">y_val</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">val_dataset</span> <span class="op">&lt;-</span> <span class="va">val_dataset</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="va">train_dataset</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  <span class="co"># Only run validation using the first 10 batches of the dataset</span></span>
<span>  <span class="co"># using the `validation_steps` argument</span></span>
<span>  validation_data <span class="op">=</span> <span class="va">val_dataset</span>,</span>
<span>  validation_steps <span class="op">=</span> <span class="fl">10</span>,</span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 2s - 2ms/step - loss: 0.3354 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.2111 - val_sparse_categorical_accuracy: 0.9375</span></span></code></pre>
<p>Note that the validation dataset will be reset after each use (so
that you will always be evaluating on the same samples from epoch to
epoch).</p>
<p>The argument <code>validation_split</code> (generating a holdout set
from the training data) is not supported when training from
<code>Dataset</code> objects, since this feature requires the ability to
index the samples of the datasets, which is not possible in general with
the <code>Dataset</code> API.</p>
<!-- ## Training & evaluation using `PyDataset` instances -->
<!-- `keras.utils.PyDataset` is a utility that you can subclass to obtain -->
<!-- a Python generator with two important properties: -->
<!-- - It works well with multiprocessing. -->
<!-- - It can be shuffled (e.g. when passing `shuffle=True` in `fit()`). -->
<!-- A `PyDataset` must implement two methods: -->
<!-- - `__getitem__` -->
<!-- - `__len__` -->
<!-- The method `__getitem__` should return a complete batch. -->
<!-- If you want to modify your dataset between epochs, you may implement `on_epoch_end`. -->
<!-- Here's a quick example: -->
<!-- ```{r} -->
<!-- class ExamplePyDataset(keras.utils.PyDataset): -->
<!--     def __init__(self, x, y, batch_size, **kwargs): -->
<!--         super().__init__(**kwargs) -->
<!--         self.x = x -->
<!--         self.y = y -->
<!--         self.batch_size = batch_size -->
<!--     def __len__(self): -->
<!--         return int(np.ceil(len(self.x) / float(self.batch_size))) -->
<!--     def __getitem__(self, idx): -->
<!--         batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size] -->
<!--         batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size] -->
<!--         return batch_x, batch_y -->
<!-- train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32) -->
<!-- val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32) -->
<!-- ``` -->
<!-- To fit the model, pass the dataset instead as the `x` argument (no need for a `y` -->
<!-- argument since the dataset includes the targets), and pass the validation dataset -->
<!-- as the `validation_data` argument. And no need for the `batch_size` argument, since -->
<!-- the dataset is already batched! -->
<!-- ```python -->
<!-- model = get_compiled_model() -->
<!-- model.fit( -->
<!--     train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1 -->
<!-- ) -->
<!-- ``` -->
<!-- Evaluating the model is just as easy: -->
<!-- ```python -->
<!-- model.evaluate(val_py_dataset) -->
<!-- ``` -->
<!-- Importantly, `PyDataset` objects support three common constructor arguments -->
<!-- that handle the parallel processing configuration: -->
<!-- - `workers`: Number of workers to use in multithreading or -->
<!--     multiprocessing. Typically, you'd set it to the number of -->
<!--     cores on your CPU. -->
<!-- - `use_multiprocessing`: Whether to use Python multiprocessing for -->
<!--     parallelism. Setting this to `True` means that your -->
<!--     dataset will be replicated in multiple forked processes. -->
<!--     This is necessary to gain compute-level (rather than I/O level) -->
<!--     benefits from parallelism. However it can only be set to -->
<!--     `True` if your dataset can be safely pickled. -->
<!-- - `max_queue_size`: Maximum number of batches to keep in the queue -->
<!--     when iterating over the dataset in a multithreaded or -->
<!--     multipricessed setting. -->
<!--     You can reduce this value to reduce the CPU memory consumption of -->
<!--     your dataset. It defaults to 10. -->
<!-- By default, multiprocessing is disabled (`use_multiprocessing=False`) and only -->
<!-- one thread is used. You should make sure to only turn on `use_multiprocessing` if -->
<!-- your code is running inside a Python `if __name__ == "__main__":` block in order -->
<!-- to avoid issues. -->
<!-- Here's a 4-thread, non-multiprocessed example: -->
<!-- ```python -->
<!-- train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32, workers=4) -->
<!-- val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32, workers=4) -->
<!-- model = get_compiled_model() -->
<!-- model.fit( -->
<!--     train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1 -->
<!-- ) -->
<!-- ``` -->
<!-- ## Training & evaluation using PyTorch `DataLoader` objects -->
<!-- All built-in training and evaluation APIs are also compatible with `torch.utils.data.Dataset` and -->
<!-- `torch.utils.data.DataLoader` objects -- regardless of whether you're using the PyTorch backend, -->
<!-- or the JAX or TensorFlow backends. Let's take a look at a simple example. -->
<!-- Unlike `PyDataset` which are batch-centric, PyTorch `Dataset` objects are sample-centric: -->
<!-- the `__len__` method returns the number of samples, -->
<!-- and the `__getitem__` method returns a specific sample. -->
<!-- ```python -->
<!-- class ExampleTorchDataset(torch.utils.data.Dataset): -->
<!--     def __init__(self, x, y): -->
<!--         self.x = x -->
<!--         self.y = y -->
<!--     def __len__(self): -->
<!--         return len(self.x) -->
<!--     def __getitem__(self, idx): -->
<!--         return self.x[idx], self.y[idx] -->
<!-- train_torch_dataset = ExampleTorchDataset(x_train, y_train) -->
<!-- val_torch_dataset = ExampleTorchDataset(x_val, y_val) -->
<!-- ``` -->
<!-- To use a PyTorch Dataset, you need to wrap it into a `Dataloader` which takes care -->
<!-- of batching and shuffling: -->
<!-- ```python -->
<!-- train_dataloader = torch.utils.data.DataLoader( -->
<!--     train_torch_dataset, batch_size=32, shuffle=True -->
<!-- ) -->
<!-- val_dataloader = torch.utils.data.DataLoader( -->
<!--     val_torch_dataset, batch_size=32, shuffle=True -->
<!-- ) -->
<!-- ``` -->
<!-- Now you can use them in the Keras API just like any other iterator: -->
<!-- ```python -->
<!-- model = get_compiled_model() -->
<!-- model.fit( -->
<!--     train_dataloader, batch_size=64, validation_data=val_dataloader, epochs=1 -->
<!-- ) -->
<!-- model.evaluate(val_dataloader) -->
<!-- ``` -->
</div>
<div class="section level2">
<h2 id="using-sample-weighting-and-class-weighting">Using sample weighting and class weighting<a class="anchor" aria-label="anchor" href="#using-sample-weighting-and-class-weighting"></a>
</h2>
<p>With the default settings the weight of a sample is decided by its
frequency in the dataset. There are two methods to weight the data,
independent of sample frequency:</p>
<ul>
<li>Class weights</li>
<li>Sample weights</li>
</ul>
<div class="section level3">
<h3 id="class-weights">Class weights<a class="anchor" aria-label="anchor" href="#class-weights"></a>
</h3>
<p>This is set by passing a named list to the <code>class_weight</code>
argument to <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>. This list maps class indices to the
weight that should be used for samples belonging to this class.</p>
<p>This can be used to balance classes without resampling, or to train a
model that gives more importance to a particular class.</p>
<p>For instance, if class “0” is half as represented as class “1” in
your data, you could use
<code>model |&gt; fit(..., class_weight = c("0" = 1, "1" = 0.5))</code>.</p>
<p>Here’s an R example where we use class weights or sample weights to
give more importance to the correct classification of class #5 (which is
the digit “5” in the MNIST dataset).</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">class_weight</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="st">"0"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"1"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"2"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"3"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"4"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="co"># Set weight "2" for class "5",</span></span>
<span>    <span class="co"># making this class 2x more important</span></span>
<span>    <span class="st">"5"</span> <span class="op">=</span> <span class="fl">2.0</span>,</span>
<span>    <span class="st">"6"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"7"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"8"</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span>    <span class="st">"9"</span> <span class="op">=</span> <span class="fl">1.0</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>,</span>
<span>             class_weight <span class="op">=</span> <span class="va">class_weight</span>,</span>
<span>             batch_size <span class="op">=</span> <span class="fl">64</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 1s - 2ms/step - loss: 0.3687 - sparse_categorical_accuracy: 0.9026</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="sample-weights">Sample weights<a class="anchor" aria-label="anchor" href="#sample-weights"></a>
</h3>
<p>For fine grained control, or if you are not building a classifier,
you can use <code>sample_weights</code>.</p>
<ul>
<li>When training from R arrays: Pass the <code>sample_weight</code>
argument to <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>.</li>
<li>When training from <code>tf_dataset</code> or any other sort of
iterator: yield
<code>(input_batch, label_batch, sample_weight_batch)</code>
tuples.</li>
</ul>
<p>A “sample weights” array is an array of numbers that specify how much
weight each sample in a batch should have in computing the total loss.
It is commonly used in imbalanced classification problems (the idea
being to give more weight to rarely-seen classes).</p>
<p>When the weights used are ones and zeros, the array can be used as a
<em>mask</em> for the loss function (entirely discarding the
contribution of certain samples to the total loss).</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sample_weight</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1.0</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sample_weight</span><span class="op">[</span><span class="va">y_train</span> <span class="op">==</span> <span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">2.0</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="va">x_train</span>, <span class="va">y_train</span>,</span>
<span>  sample_weight <span class="op">=</span> <span class="va">sample_weight</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">64</span>, epochs <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 1s - 2ms/step - loss: 0.3685 - sparse_categorical_accuracy: 0.9024</span></span></code></pre>
<p>Here’s a matching <code>Dataset</code> example:</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sample_weight</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1.0</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">y_train</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">sample_weight</span><span class="op">[</span><span class="va">y_train</span> <span class="op">==</span> <span class="fl">5</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">2.0</span></span>
<span></span>
<span><span class="co"># Create a Dataset that includes sample weights</span></span>
<span><span class="co"># (3rd element in the return tuple).</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    <span class="va">x_train</span>, <span class="va">y_train</span>, <span class="va">sample_weight</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Shuffle and slice the dataset.</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size <span class="op">=</span> <span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dataset</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 782/782 - 1s - 2ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8999</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="passing-data-to-multi-input-multi-output-models">Passing data to multi-input, multi-output models<a class="anchor" aria-label="anchor" href="#passing-data-to-multi-input-multi-output-models"></a>
</h2>
<p>In the previous examples, we were considering a model with a single
input (a tensor of shape <code>(764)</code>) and a single output (a
prediction tensor of shape <code>(10)</code>). But what about models
that have multiple inputs or outputs?</p>
<p>Consider the following model, which has an image input of shape
<code>(32, 32, 3)</code> (that’s <code>(height, width, channels)</code>)
and a time series input of shape <code>(NA, 10)</code> (that’s
<code>(timesteps, features)</code>). Our model will have two outputs
computed from the combination of these inputs: a “score” (of shape
<code>(1)</code>) and a probability distribution over five classes (of
shape <code>(5)</code>).</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">image_input</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">3</span><span class="op">)</span>, name <span class="op">=</span> <span class="st">"img_input"</span><span class="op">)</span></span>
<span><span class="va">timeseries_input</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">10</span><span class="op">)</span>, name <span class="op">=</span> <span class="st">"ts_input"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x1</span> <span class="op">&lt;-</span> <span class="va">image_input</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_conv_2d.html">layer_conv_2d</a></span><span class="op">(</span>filters <span class="op">=</span> <span class="fl">3</span>, kernel_size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_global_max_pooling_2d.html">layer_global_max_pooling_2d</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x2</span> <span class="op">&lt;-</span> <span class="va">timeseries_input</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_conv_1d.html">layer_conv_1d</a></span><span class="op">(</span>filters <span class="op">=</span> <span class="fl">3</span>, kernel_size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_global_max_pooling_1d.html">layer_global_max_pooling_1d</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_concatenate.html">layer_concatenate</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">score_output</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">1</span>, name <span class="op">=</span> <span class="st">"score_output"</span><span class="op">)</span></span>
<span><span class="va">class_output</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">5</span>, name <span class="op">=</span> <span class="st">"class_output"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span></span>
<span>  inputs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">image_input</span>, <span class="va">timeseries_input</span><span class="op">)</span>,</span>
<span>  outputs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">score_output</span>, <span class="va">class_output</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Let’s plot this model, so you can clearly see what we’re doing here
(note that the shapes shown in the plot are batch shapes, rather than
per-sample shapes).</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">model</span>, show_shapes <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<img src="training_with_built_in_methods/unnamed-chunk-26-1.png" alt="plot of chunk unnamed-chunk-26" width="744"><p class="caption">
plot of chunk unnamed-chunk-26
</p>
</div>
<p>At compilation time, we can specify different losses to different
outputs, by passing the loss functions as a list:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>If we only passed a single loss function to the model, the same loss
function would be applied to every output (which is not appropriate
here).</p>
<p>Likewise for metrics:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_error.html">metric_mean_absolute_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_percentage_error.html">metric_mean_absolute_percentage_error</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_categorical_accuracy.html">metric_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Since we gave names to our output layers, we could also specify
per-output losses and metrics via a named list:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    score_output <span class="op">=</span> <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    class_output <span class="op">=</span> <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    score_output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_error.html">metric_mean_absolute_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_percentage_error.html">metric_mean_absolute_percentage_error</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    class_output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_categorical_accuracy.html">metric_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>We recommend the use of names if you have more than 2 outputs.</p>
<p>It’s possible to give different weights to different output-specific
losses (for instance, one might wish to privilege the “score” loss in
our example, by giving to 2x the importance of the class loss), using
the <code>loss_weights</code> argument:</p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    score_output <span class="op">=</span> <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    class_output <span class="op">=</span> <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    score_output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_error.html">metric_mean_absolute_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>      <span class="fu"><a href="../reference/metric_mean_absolute_percentage_error.html">metric_mean_absolute_percentage_error</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span>,</span>
<span>    class_output <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_categorical_accuracy.html">metric_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">)</span>,</span>
<span>  loss_weights <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>score_output <span class="op">=</span> <span class="fl">2.0</span>, class_output <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>You could also choose not to compute a loss for certain outputs, if
these outputs are meant for prediction but not for training:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># loss list, positional version</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="cn">NULL</span>, <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Or loss list, named version</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>class_output <span class="op">=</span> <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Passing data to a multi-input or multi-output model in
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> works in a similar way as specifying a loss function
in compile: you can pass <strong>lists of arrays</strong> (with 1:1
mapping to the outputs that received a loss function) or <strong>dicts
mapping output names to arrays</strong>.</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="../reference/loss_categorical_crossentropy.html">loss_categorical_crossentropy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Generate dummy data</span></span>
<span><span class="va">img_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">32</span>, <span class="fl">32</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">ts_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">20</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">score_targets</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">class_targets</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit on unnamed lists (positional matching)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">img_data</span>, <span class="va">ts_data</span><span class="op">)</span>,</span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">score_targets</span>, <span class="va">class_targets</span><span class="op">)</span>,</span>
<span>    batch_size<span class="op">=</span><span class="fl">32</span>,</span>
<span>    epochs<span class="op">=</span><span class="fl">1</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 4/4 - 2s - 472ms/step - loss: 0.5888</span></span></code></pre>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Alternatively, fit on named lists (names matching)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>img_input <span class="op">=</span> <span class="va">img_data</span>, ts_input <span class="op">=</span> <span class="va">ts_data</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>score_output <span class="op">=</span> <span class="va">score_targets</span>, class_output <span class="op">=</span> <span class="va">class_targets</span><span class="op">)</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">1</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 4/4 - 1s - 222ms/step - loss: -1.3254e+00</span></span></code></pre>
<p>Here’s the <code>Dataset</code> use case: similarly as what we did
for R arrays, the <code>Dataset</code> should return a tuple of named
lists (dicts).</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>img_input <span class="op">=</span> <span class="va">img_data</span>, ts_input <span class="op">=</span> <span class="va">ts_data</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>score_output <span class="op">=</span> <span class="va">score_targets</span>, class_output <span class="op">=</span> <span class="va">class_targets</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train_dataset</span> <span class="op">&lt;-</span> <span class="va">train_dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size <span class="op">=</span> <span class="fl">1024</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_dataset</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 2/2 - 1s - 574ms/step - loss: 1.7663</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="using-callbacks">Using callbacks<a class="anchor" aria-label="anchor" href="#using-callbacks"></a>
</h2>
<p>Callbacks in Keras are objects that are called at different points
during training (at the start of an epoch, at the end of a batch, at the
end of an epoch, etc.). They can be used to implement certain behaviors,
such as:</p>
<ul>
<li>Doing validation at different points during training (beyond the
built-in per-epoch validation)</li>
<li>Checkpointing the model at regular intervals or when it exceeds a
certain accuracy threshold</li>
<li>Changing the learning rate of the model when training seems to be
plateauing</li>
<li>Doing fine-tuning of the top layers when training seems to be
plateauing</li>
<li>Sending email or instant message notifications when training ends or
where a certain performance threshold is exceeded</li>
<li>Etc.</li>
</ul>
<p>Callbacks can be passed as a list to your call to
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>:</p>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">callbacks</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/callback_early_stopping.html">callback_early_stopping</a></span><span class="op">(</span></span>
<span>    <span class="co"># Stop training when `val_loss` is no longer improving</span></span>
<span>    monitor <span class="op">=</span> <span class="st">"val_loss"</span>,</span>
<span>    <span class="co"># "no longer improving" being defined as "no better than 1e-2 less"</span></span>
<span>    min_delta <span class="op">=</span> <span class="fl">1e-2</span>,</span>
<span>    <span class="co"># "no longer improving" being further defined as "for at least 2 epochs"</span></span>
<span>    patience <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    verbose <span class="op">=</span> <span class="fl">1</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="va">x_train</span>,</span>
<span>  <span class="va">y_train</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">20</span>,</span>
<span>  batch_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>  callbacks <span class="op">=</span> <span class="va">callbacks</span>,</span>
<span>  validation_split <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/20</span></span>
<span><span class="co">## 625/625 - 1s - 2ms/step - loss: 0.3727 - sparse_categorical_accuracy: 0.8946 - val_loss: 0.1909 - val_sparse_categorical_accuracy: 0.9440</span></span>
<span><span class="co">## Epoch 2/20</span></span>
<span><span class="co">## 625/625 - 0s - 777us/step - loss: 0.1740 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1441 - val_sparse_categorical_accuracy: 0.9572</span></span>
<span><span class="co">## Epoch 3/20</span></span>
<span><span class="co">## 625/625 - 0s - 791us/step - loss: 0.1252 - sparse_categorical_accuracy: 0.9626 - val_loss: 0.1294 - val_sparse_categorical_accuracy: 0.9619</span></span>
<span><span class="co">## Epoch 4/20</span></span>
<span><span class="co">## 625/625 - 1s - 841us/step - loss: 0.0984 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.1213 - val_sparse_categorical_accuracy: 0.9647</span></span>
<span><span class="co">## Epoch 5/20</span></span>
<span><span class="co">## 625/625 - 1s - 811us/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9762 - val_loss: 0.1153 - val_sparse_categorical_accuracy: 0.9667</span></span>
<span><span class="co">## Epoch 6/20</span></span>
<span><span class="co">## 625/625 - 0s - 790us/step - loss: 0.0683 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.1142 - val_sparse_categorical_accuracy: 0.9680</span></span>
<span><span class="co">## Epoch 7/20</span></span>
<span><span class="co">## 625/625 - 1s - 805us/step - loss: 0.0578 - sparse_categorical_accuracy: 0.9835 - val_loss: 0.1142 - val_sparse_categorical_accuracy: 0.9694</span></span>
<span><span class="co">## Epoch 8/20</span></span>
<span><span class="co">## 625/625 - 1s - 808us/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9861 - val_loss: 0.1156 - val_sparse_categorical_accuracy: 0.9708</span></span>
<span><span class="co">## Epoch 9/20</span></span>
<span><span class="co">## 625/625 - 1s - 808us/step - loss: 0.0423 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.1175 - val_sparse_categorical_accuracy: 0.9716</span></span>
<span><span class="co">## Epoch 9: early stopping</span></span></code></pre>
<div class="section level3">
<h3 id="many-built-in-callbacks-are-available">Many built-in callbacks are available<a class="anchor" aria-label="anchor" href="#many-built-in-callbacks-are-available"></a>
</h3>
<p>There are many built-in callbacks already available in Keras, such
as:</p>
<ul>
<li>
<code><a href="../reference/callback_model_checkpoint.html">callback_model_checkpoint()</a></code>: Periodically save the
model.</li>
<li>
<code><a href="../reference/callback_early_stopping.html">callback_early_stopping()</a></code>: Stop training when training
is no longer improving the validation metrics.</li>
<li>
<code><a href="../reference/callback_tensorboard.html">callback_tensorboard()</a></code>: periodically write model logs
that can be visualized in <a href="https://www.tensorflow.org/tensorboard" class="external-link">TensorBoard</a> (more
details in the section “Visualization”).</li>
<li>
<code><a href="../reference/callback_csv_logger.html">callback_csv_logger()</a></code>: streams loss and metrics data to
a CSV file.</li>
<li>etc.</li>
</ul>
<p>See the <a href="https://keras.posit.co/reference/index.html#callbacks">callbacks
documentation</a> for the complete list.</p>
</div>
<div class="section level3">
<h3 id="writing-your-own-callback">Writing your own callback<a class="anchor" aria-label="anchor" href="#writing-your-own-callback"></a>
</h3>
<p>You can create a custom callback by subclassing the base
[<code><a href="../reference/Callback.html">Callback()</a></code>] class. A callback has access to its associated
model through the class property <code>self$model</code>.</p>
<p>Make sure to read the <a href="../writing_your_own_callbacks.html">complete guide to writing
custom callbacks</a>.</p>
<p>Here’s a simple example saving a list of per-batch loss values during
training:</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">callback_loss_history</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/Callback.html">Callback</a></span><span class="op">(</span></span>
<span>  classname <span class="op">=</span> <span class="st">"LossHistory"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">file</span> <span class="op">=</span> <span class="st">"per_training_batch_losses.txt"</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">private</span><span class="op">$</span><span class="va">file</span> <span class="op">&lt;-</span> <span class="va">file</span></span>
<span>  <span class="op">}</span>,</span>
<span>  on_train_begin <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">logs</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">private</span><span class="op">$</span><span class="va">per_batch_losses</span> <span class="op">&lt;-</span> <span class="fu">fastmap</span><span class="fu">::</span><span class="fu"><a href="https://r-lib.github.io/fastmap/reference/faststack.html" class="external-link">faststack</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  on_train_batch_begin <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">batch</span>, <span class="va">logs</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">private</span><span class="op">$</span><span class="va">per_batch_losses</span><span class="op">$</span><span class="fu">push</span><span class="op">(</span><span class="va">logs</span><span class="op">$</span><span class="va">loss</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  on_train_end <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">logs</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">per_batch_losses</span> <span class="op">&lt;-</span> <span class="va">private</span><span class="op">$</span><span class="va">per_batch_losses</span><span class="op">$</span><span class="fu">as_list</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/write.html" class="external-link">write</a></span><span class="op">(</span><span class="va">per_batch_losses</span>, <span class="va">private</span><span class="op">$</span><span class="va">file</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="checkpointing-models">Checkpointing models<a class="anchor" aria-label="anchor" href="#checkpointing-models"></a>
</h2>
<p>When you’re training model on relatively large datasets, it’s crucial
to save checkpoints of your model at frequent intervals.</p>
<p>The easiest way to achieve this is with
[<code><a href="../reference/callback_model_checkpoint.html">callback_model_checkpoint()</a></code>]:</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">callbacks</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/callback_model_checkpoint.html">callback_model_checkpoint</a></span><span class="op">(</span></span>
<span>    <span class="co"># Path where to save the model</span></span>
<span>    <span class="co"># The two parameters below mean that we will overwrite</span></span>
<span>    <span class="co"># the current checkpoint if and only if</span></span>
<span>    <span class="co"># the `val_loss` score has improved.</span></span>
<span>    <span class="co"># The saved model name will include the current epoch.</span></span>
<span>    filepath <span class="op">=</span> <span class="st">"mymodel_{epoch}.keras"</span>,</span>
<span>    save_best_only <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>    <span class="co"># Only save a model if `val_loss` has improved.</span></span>
<span>    monitor <span class="op">=</span> <span class="st">"val_loss"</span>,</span>
<span>    verbose <span class="op">=</span> <span class="fl">1</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span></span>
<span>  <span class="va">x_train</span>, <span class="va">y_train</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">2</span>, batch_size <span class="op">=</span> <span class="fl">64</span>,</span>
<span>  callbacks <span class="op">=</span> <span class="va">callbacks</span>,</span>
<span>  validation_split <span class="op">=</span> <span class="fl">0.2</span></span>
<span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/2</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## Epoch 1: val_loss improved from inf to 0.19317, saving model to mymodel_1.keras</span></span>
<span><span class="co">## 625/625 - 2s - 3ms/step - loss: 0.3759 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.1932 - val_sparse_categorical_accuracy: 0.9447</span></span>
<span><span class="co">## Epoch 2/2</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## Epoch 2: val_loss improved from 0.19317 to 0.14933, saving model to mymodel_2.keras</span></span>
<span><span class="co">## 625/625 - 1s - 1ms/step - loss: 0.1797 - sparse_categorical_accuracy: 0.9469 - val_loss: 0.1493 - val_sparse_categorical_accuracy: 0.9576</span></span></code></pre>
<p>The <code>ModelCheckpoint</code> callback can be used to implement
fault-tolerance: the ability to restart training from the last saved
state of the model in case training gets randomly interrupted. Here’s a
basic example:</p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Prepare a directory to store all the checkpoints.</span></span>
<span><span class="va">checkpoint_dir</span> <span class="op">&lt;-</span> <span class="st">"./ckpt"</span></span>
<span><span class="fu">fs</span><span class="fu">::</span><span class="fu"><a href="https://fs.r-lib.org/reference/create.html" class="external-link">dir_create</a></span><span class="op">(</span><span class="va">checkpoint_dir</span><span class="op">)</span></span>
<span></span>
<span><span class="va">make_or_restore_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Either restore the latest (best) model, or create a fresh one</span></span>
<span>  <span class="co"># if there is no checkpoint available.</span></span>
<span>  <span class="va">checkpoints</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.glob.html" class="external-link">Sys.glob</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html" class="external-link">file.path</a></span><span class="op">(</span><span class="va">checkpoint_dir</span>, <span class="st">"model-loss=*.keras"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">checkpoints</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">checkpoint_losses</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">sub</a></span><span class="op">(</span><span class="st">"^model-loss=([0-9.]+)\\.keras$"</span>, <span class="st">"\\1"</span>,</span>
<span>                             <span class="fu"><a href="https://rdrr.io/r/base/basename.html" class="external-link">basename</a></span><span class="op">(</span><span class="va">checkpoints</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">best_checkpoint</span> <span class="op">&lt;-</span> <span class="va">checkpoints</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html" class="external-link">which.min</a></span><span class="op">(</span><span class="va">checkpoint_losses</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="fu"><a href="../reference/load_model.html">load_model</a></span><span class="op">(</span><span class="va">best_checkpoint</span><span class="op">)</span></span>
<span>  <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>    <span class="fu">get_compiled_model</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">make_or_restore_model</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">callbacks</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="co"># This callback saves the model every 100 batches.</span></span>
<span>  <span class="co"># We include the training loss in the saved model name.</span></span>
<span>  <span class="fu"><a href="../reference/callback_model_checkpoint.html">callback_model_checkpoint</a></span><span class="op">(</span></span>
<span>    filepath <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/file.path.html" class="external-link">file.path</a></span><span class="op">(</span><span class="va">checkpoint_dir</span>, <span class="st">"model-loss={loss:.2f}.keras"</span><span class="op">)</span>,</span>
<span>    save_freq <span class="op">=</span> <span class="fl">100</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span>, epochs <span class="op">=</span> <span class="fl">1</span>, callbacks <span class="op">=</span> <span class="va">callbacks</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 1563/1563 - 2s - 1ms/step - loss: 0.2949 - sparse_categorical_accuracy: 0.9148</span></span></code></pre>
<p>You call also write your own callback for saving and restoring
models.</p>
<p>For a complete guide on serialization and saving, see the <a href="serialization_and_saving.html">guide to saving and serializing
Models</a>.</p>
</div>
<div class="section level2">
<h2 id="using-learning-rate-schedules">Using learning rate schedules<a class="anchor" aria-label="anchor" href="#using-learning-rate-schedules"></a>
</h2>
<p>A common pattern when training deep learning models is to gradually
reduce the learning as training progresses. This is generally known as
“learning rate decay”.</p>
<p>The learning decay schedule could be static (fixed in advance, as a
function of the current epoch or the current batch index), or dynamic
(responding to the current behavior of the model, in particular the
validation loss).</p>
<div class="section level3">
<h3 id="passing-a-schedule-to-an-optimizer">Passing a schedule to an optimizer<a class="anchor" aria-label="anchor" href="#passing-a-schedule-to-an-optimizer"></a>
</h3>
<p>You can easily use a static learning rate decay schedule by passing a
schedule object as the <code>learning_rate</code> argument in your
optimizer:</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">initial_learning_rate</span> <span class="op">&lt;-</span> <span class="fl">0.1</span></span>
<span><span class="va">lr_schedule</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/learning_rate_schedule_exponential_decay.html">learning_rate_schedule_exponential_decay</a></span><span class="op">(</span></span>
<span>    <span class="va">initial_learning_rate</span>, decay_steps<span class="op">=</span><span class="fl">100000</span>, decay_rate<span class="op">=</span><span class="fl">0.96</span>,</span>
<span>    staircase<span class="op">=</span><span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_rmsprop.html">optimizer_rmsprop</a></span><span class="op">(</span>learning_rate <span class="op">=</span> <span class="va">lr_schedule</span><span class="op">)</span></span></code></pre></div>
<p>Several built-in schedules are available:
<code>ExponentialDecay</code>, <code>PiecewiseConstantDecay</code>,
<code>PolynomialDecay</code>, and <code>InverseTimeDecay</code>.</p>
</div>
<div class="section level3">
<h3 id="using-callbacks-to-implement-a-dynamic-learning-rate-schedule">Using callbacks to implement a dynamic learning rate schedule<a class="anchor" aria-label="anchor" href="#using-callbacks-to-implement-a-dynamic-learning-rate-schedule"></a>
</h3>
<p>A dynamic learning rate schedule (for instance, decreasing the
learning rate when the validation loss is no longer improving) cannot be
achieved with these schedule objects, since the optimizer does not have
access to validation metrics.</p>
<p>However, callbacks do have access to all metrics, including
validation metrics! You can thus achieve this pattern by using a
callback that modifies the current learning rate on the optimizer. In
fact, this is even built-in as
[<code><a href="../reference/callback_reduce_lr_on_plateau.html">callback_reduce_lr_on_plateau()</a></code>].</p>
</div>
</div>
<div class="section level2">
<h2 id="visualizing-loss-and-metrics-during-training-with-tensorboard">Visualizing loss and metrics during training with TensorBoard<a class="anchor" aria-label="anchor" href="#visualizing-loss-and-metrics-during-training-with-tensorboard"></a>
</h2>
<p>The best way to keep an eye on your model during training is to use
<a href="https://www.tensorflow.org/tensorboard" class="external-link">TensorBoard</a> – a
browser-based application that you can run locally that provides you
with:</p>
<ul>
<li>Live plots of the loss and metrics for training and evaluation</li>
<li>(optionally) Visualizations of the histograms of your layer
activations</li>
<li>(optionally) 3D visualizations of the embedding spaces learned by
your <code><a href="../reference/layer_embedding.html">layer_embedding()</a></code>
</li>
</ul>
<p>If you have installed TensorFlow with pip, you should be able to
launch TensorBoard from the command line:</p>
<pre><code>tensorboard --logdir=/full_path_to_your_logs</code></pre>
<p>or from R using:</p>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">tensorflow</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/tensorboard.html" class="external-link">tensorboard</a></span><span class="op">(</span>logdir <span class="op">=</span> <span class="st">"/full_path_to_your_logs"</span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="using-the-tensorboard-callback">Using the TensorBoard callback<a class="anchor" aria-label="anchor" href="#using-the-tensorboard-callback"></a>
</h3>
<p>The easiest way to use TensorBoard with a Keras model and the
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> method is with
[<code><a href="../reference/callback_tensorboard.html">callback_tensorboard()</a></code>].</p>
<p>In the simplest case, just specify where you want the callback to
write logs, and you’re good to go:</p>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tb_callback</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/callback_tensorboard.html">callback_tensorboard</a></span><span class="op">(</span></span>
<span>  log_dir <span class="op">=</span> <span class="st">"/full_path_to_your_logs"</span>,</span>
<span>  histogram_freq <span class="op">=</span> <span class="fl">0</span>, <span class="co"># How often to log histogram visualizations</span></span>
<span>  embeddings_freq <span class="op">=</span> <span class="fl">0</span>, <span class="co"># How often to log embedding visualizations</span></span>
<span>  update_freq <span class="op">=</span> <span class="st">"epoch"</span>, <span class="co"># How often to write logs (default: once per epoch)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>For more information, see <code><a href="../reference/callback_tensorboard.html">callback_tensorboard()</a></code>.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
