<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Transfer learning &amp; fine-tuning • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Transfer learning &amp; fine-tuning">
<meta name="description" content="Complete guide to transfer learning &amp; fine-tuning in Keras.">
<meta property="og:description" content="Complete guide to transfer learning &amp; fine-tuning in Keras.">
<!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager -->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides">
<li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Transfer learning &amp; fine-tuning</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/vignettes-src/transfer_learning.Rmd" class="external-link"><code>vignettes-src/transfer_learning.Rmd</code></a></small>
      <div class="d-none name"><code>transfer_learning.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://keras3.posit.co/">keras3</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p><strong>Transfer learning</strong> consists of taking features
learned on one problem, and leveraging them on a new, similar problem.
For instance, features from a model that has learned to identify raccoon
may be useful to kick-start a model meant to identify tanukis.</p>
<p>Transfer learning is usually done for tasks where your dataset has
too little data to train a full-scale model from scratch.</p>
<p>The most common incarnation of transfer learning in the context of
deep learning is the following workflow:</p>
<ol style="list-style-type: decimal">
<li>Take layers from a previously trained model.</li>
<li>Freeze them, so as to avoid destroying any of the information they
contain during future training rounds.</li>
<li>Add some new, trainable layers on top of the frozen layers. They
will learn to turn the old features into predictions on a new
dataset.</li>
<li>Train the new layers on your dataset.</li>
</ol>
<p>A last, optional step, is <strong>fine-tuning</strong>, which
consists of unfreezing the entire model you obtained above (or part of
it), and re-training it on the new data with a very low learning rate.
This can potentially achieve meaningful improvements, by incrementally
adapting the pretrained features to the new data.</p>
<p>First, we will go over the Keras <code>trainable</code> API in
detail, which underlies most transfer learning &amp; fine-tuning
workflows.</p>
<p>Then, we’ll demonstrate the typical workflow by taking a model
pretrained on the ImageNet dataset, and retraining it on the Kaggle
“cats vs dogs” classification dataset.</p>
<p>This is adapted from <a href="https://www.manning.com/books/deep-learning-with-python" class="external-link">Deep
Learning with Python</a> and the 2016 blog post <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" class="external-link">“building
powerful image classification models using very little data”</a>.</p>
</div>
<div class="section level2">
<h2 id="freezing-layers-understanding-the-trainable-attribute">Freezing layers: understanding the <code>trainable</code>
attribute<a class="anchor" aria-label="anchor" href="#freezing-layers-understanding-the-trainable-attribute"></a>
</h2>
<p>Layers &amp; models have three weight attributes:</p>
<ul>
<li>
<code>weights</code> is the list of all weights variables of the
layer.</li>
<li>
<code>trainable_weights</code> is the list of those that are meant
to be updated (via gradient descent) to minimize the loss during
training.</li>
<li>
<code>non_trainable_weights</code> is the list of those that aren’t
meant to be trained. Typically they are updated by the model during the
forward pass.</li>
</ul>
<p><strong>Example: the <code>Dense</code> layer has 2 trainable weights
(kernel &amp; bias)</strong></p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">layer</span><span class="op">$</span><span class="fu">build</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NULL</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Create the weights</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">non_trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 0</span></span></code></pre>
<p>In general, all weights are trainable weights. The only built-in
layer that has non-trainable weights is
[<code><a href="../reference/layer_batch_normalization.html">layer_batch_normalization()</a></code>]. It uses non-trainable
weights to keep track of the mean and variance of its inputs during
training. To learn how to use non-trainable weights in your own custom
layers, see the <a href="making_new_layers_and_models_via_subclassing.html">guide to
writing new layers from scratch</a>.</p>
<p><strong>Example: the <code>BatchNormalization</code> layer has 2
trainable weights and 2 non-trainable weights</strong></p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_batch_normalization.html">layer_batch_normalization</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">layer</span><span class="op">$</span><span class="fu">build</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Create the weights</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 4</span></span></code></pre>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">non_trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<p>Layers &amp; models also feature a boolean attribute
<code>trainable</code>. Its value can be changed. Setting
<code>layer$trainable</code> to <code>FALSE</code> moves all the layer’s
weights from trainable to non-trainable. This is called “freezing” the
layer: the state of a frozen layer won’t be updated during training
(either when training with <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> or when training with any
custom loop that relies on <code>trainable_weights</code> to apply
gradient updates).</p>
<p><strong>Example: setting <code>trainable</code> to
<code>False</code></strong></p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">layer</span><span class="op">$</span><span class="fu">build</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NULL</span>, <span class="fl">4</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Create the weights</span></span>
<span><span class="va">layer</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span>  <span class="co"># Freeze the layer</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 0</span></span></code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">layer</span><span class="op">$</span><span class="va">non_trainable_weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2</span></span></code></pre>
<p>When a trainable weight becomes non-trainable, its value is no longer
updated during training.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make a model with 2 layers</span></span>
<span><span class="va">layer1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span><span class="va">layer2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span>input_shape <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">layer1</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">layer2</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Freeze the first layer</span></span>
<span><span class="va">layer1</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span></span>
<span></span>
<span><span class="co"># Keep a copy of the weights of layer1 for later reference</span></span>
<span><span class="co"># (get_weights() returns a list of R arrays,</span></span>
<span><span class="co">#  layer$weights returns a list of KerasVariables)</span></span>
<span><span class="va">initial_layer1_weights_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_weights.html">get_weights</a></span><span class="op">(</span><span class="va">layer1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Train the model</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>optimizer <span class="op">=</span> <span class="st">"adam"</span>, loss <span class="op">=</span> <span class="st">"mse"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>, epochs <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 1/1 - 0s - 472ms/step - loss: 2.1868</span></span></code></pre>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Check that the weights of layer1 have not changed during training</span></span>
<span><span class="va">final_layer1_weights_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_weights.html">get_weights</a></span><span class="op">(</span><span class="va">layer1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/all.equal.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">initial_layer1_weights_values</span>,</span>
<span>          <span class="va">final_layer1_weights_values</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] TRUE</span></span></code></pre>
<p>Do not confuse the <code>layer$trainable</code> attribute with the
argument <code>training</code> in <code>layer$call()</code> (which
controls whether the layer should run its forward pass in inference mode
or training mode). For more information, see the <a href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" class="external-link">Keras
FAQ</a>.</p>
</div>
<div class="section level2">
<h2 id="recursive-setting-of-the-trainable-attribute">Recursive setting of the <code>trainable</code> attribute<a class="anchor" aria-label="anchor" href="#recursive-setting-of-the-trainable-attribute"></a>
</h2>
<p>If you set <code>$trainable &lt;- FALSE</code> on a model or on any
layer that has sublayers, all children layers become non-trainable as
well.</p>
<p><strong>Example:</strong></p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inner_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span>input_shape <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span>input_shape <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">inner_model</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">3</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span>  <span class="co"># Freeze the outer model</span></span>
<span></span>
<span><span class="va">inner_model</span><span class="op">$</span><span class="va">trainable</span>  <span class="co"># All layers in `model` are now frozen</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] FALSE</span></span></code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inner_model</span><span class="op">$</span><span class="va">layers</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">trainable</span>  <span class="co"># `trainable` is propagated recursively</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] FALSE</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="the-typical-transfer-learning-workflow">The typical transfer-learning workflow<a class="anchor" aria-label="anchor" href="#the-typical-transfer-learning-workflow"></a>
</h2>
<p>This leads us to how a typical transfer learning workflow can be
implemented in Keras:</p>
<ol style="list-style-type: decimal">
<li>Instantiate a base model and load pre-trained weights into it.</li>
<li>Freeze all layers in the base model by setting
<code>trainable &lt;- FALSE</code>.</li>
<li>Create a new model on top of the output of one (or several) layers
from the base model.</li>
<li>Train your new model on your new dataset.</li>
</ol>
<p>Note that an alternative, more lightweight workflow could also
be:</p>
<ol style="list-style-type: decimal">
<li>Instantiate a base model and load pre-trained weights into it.</li>
<li>Run your new dataset through it and record the output of one (or
several) layers from the base model. This is called <strong>feature
extraction</strong>.</li>
<li>Use that output as input data for a new, smaller model.</li>
</ol>
<p>A key advantage of that second workflow is that you only run the base
model once on your data, rather than once per epoch of training. So it’s
a lot faster &amp; cheaper.</p>
<p>An issue with that second workflow, though, is that it doesn’t allow
you to dynamically modify the input data of your new model during
training, which is required when doing data augmentation, for instance.
Transfer learning is typically used for tasks when your new dataset has
too little data to train a full-scale model from scratch, and in such
scenarios data augmentation is very important. So in what follows, we
will focus on the first workflow.</p>
<p>Here’s what the first workflow looks like in Keras:</p>
<p>First, instantiate a base model with pre-trained weights.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">base_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/application_xception.html">application_xception</a></span><span class="op">(</span></span>
<span>  weights <span class="op">=</span> <span class="st">'imagenet'</span>, <span class="co"># Load weights pre-trained on ImageNet.</span></span>
<span>  input_shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">150</span>, <span class="fl">150</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  include_top <span class="op">=</span> <span class="cn">FALSE</span>  <span class="co"># Do not include the ImageNet classifier at the top.</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Then, freeze the base model.</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">base_model</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span></span></code></pre></div>
<p>Create a new model on top.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">150</span>, <span class="fl">150</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># We make sure that the base_model is running in inference mode here,</span></span>
<span><span class="co"># by passing `training &lt;- FALSE`. This is important for fine-tuning, as you will</span></span>
<span><span class="co"># learn in a few paragraphs.</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">base_model</span><span class="op">(</span>training <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="co"># Convert features of shape `base_model$output_shape[-1]` to vectors</span></span>
<span>  <span class="fu"><a href="../reference/layer_global_average_pooling_2d.html">layer_global_average_pooling_2d</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="co"># A Dense classifier with a single unit (binary classification)</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span></code></pre></div>
<p>Train the model on new data.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_binary_crossentropy.html">loss_binary_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_binary_accuracy.html">metric_binary_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">new_dataset</span>, epochs <span class="op">=</span> <span class="fl">20</span>,</span>
<span>             callbacks <span class="op">=</span> <span class="va">...</span>, validation_data <span class="op">=</span> <span class="va">...</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="fine-tuning">Fine-tuning<a class="anchor" aria-label="anchor" href="#fine-tuning"></a>
</h2>
<p>Once your model has converged on the new data, you can try to
unfreeze all or part of the base model and retrain the whole model
end-to-end with a very low learning rate.</p>
<p>This is an optional last step that can potentially give you
incremental improvements. It could also potentially lead to quick
overfitting – keep that in mind.</p>
<p>It is critical to only do this step <em>after</em> the model with
frozen layers has been trained to convergence. If you mix
randomly-initialized trainable layers with trainable layers that hold
pre-trained features, the randomly-initialized layers will cause very
large gradient updates during training, which will destroy your
pre-trained features.</p>
<p>It’s also critical to use a very low learning rate at this stage,
because you are training a much larger model than in the first round of
training, on a dataset that is typically very small. As a result, you
are at risk of overfitting very quickly if you apply large weight
updates. Here, you only want to readapt the pretrained weights in an
incremental way.</p>
<p>This is how to implement fine-tuning of the whole base model:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Unfreeze the base model</span></span>
<span><span class="va">base_model</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span></span>
<span><span class="co"># It's important to recompile your model after you make any changes</span></span>
<span><span class="co"># to the `trainable` attribute of any inner layer, so that your changes</span></span>
<span><span class="co"># are take into account</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span><span class="fl">1e-5</span><span class="op">)</span>, <span class="co"># Very low learning rate</span></span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_binary_crossentropy.html">loss_binary_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_binary_accuracy.html">metric_binary_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Train end-to-end. Be careful to stop before you overfit!</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">new_dataset</span>, epochs <span class="op">=</span> <span class="fl">10</span>,</span>
<span>             callbacks <span class="op">=</span> <span class="va">...</span>, validation_data <span class="op">=</span> <span class="va">...</span><span class="op">)</span></span></code></pre></div>
<p><strong>Important note about <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> and
<code>trainable</code></strong></p>
<p>Calling <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> on a model is meant to “freeze” the
behavior of that model. This implies that the <code>trainable</code>
attribute values at the time the model is compiled should be preserved
throughout the lifetime of that model, until <code>compile</code> is
called again. Hence, if you change any <code>trainable</code> value,
make sure to call <code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> again on your model for your
changes to be taken into account.</p>
<p><strong>Important notes about <code>BatchNormalization</code>
layer</strong></p>
<p>Many image models contain <code>BatchNormalization</code> layers.
That layer is a special case on every imaginable count. Here are a few
things to keep in mind.</p>
<ul>
<li>
<code>BatchNormalization</code> contains 2 non-trainable weights
that get updated during training. These are the variables tracking the
mean and variance of the inputs.</li>
<li>When you set <code>bn_layer$trainable &lt;- FALSE</code>, the
<code>BatchNormalization</code> layer will run in inference mode, and
will not update its mean &amp; variance statistics. This is not the case
for other layers in general, as <a href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute" class="external-link">weight
trainability &amp; inference/training modes are two orthogonal
concepts</a>. But the two are tied in the case of the
<code>BatchNormalization</code> layer.</li>
<li>When you unfreeze a model that contains
<code>BatchNormalization</code> layers in order to do fine-tuning, you
should keep the <code>BatchNormalization</code> layers in inference mode
by passing <code>training = FALSE</code> when calling the base model.
Otherwise the updates applied to the non-trainable weights will suddenly
destroy what the model has learned.</li>
</ul>
<p>You’ll see this pattern in action in the end-to-end example at the
end of this guide.</p>
</div>
<div class="section level2">
<h2 id="an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset">An end-to-end example: fine-tuning an image classification model on
a cats vs. dogs dataset<a class="anchor" aria-label="anchor" href="#an-end-to-end-example-fine-tuning-an-image-classification-model-on-a-cats-vs--dogs-dataset"></a>
</h2>
<p>To solidify these concepts, let’s walk you through a concrete
end-to-end transfer learning &amp; fine-tuning example. We will load the
Xception model, pre-trained on ImageNet, and use it on the Kaggle “cats
vs. dogs” classification dataset.</p>
<div class="section level3">
<h3 id="getting-the-data">Getting the data<a class="anchor" aria-label="anchor" href="#getting-the-data"></a>
</h3>
<p>First, let’s fetch the cats vs. dogs dataset using TFDS. If you have
your own dataset, you’ll probably want to use the utility
[<code><a href="../reference/image_dataset_from_directory.html">image_dataset_from_directory()</a></code>] to generate similar
labeled dataset objects from a set of images on disk filed into
class-specific folders.</p>
<p>Transfer learning is most useful when working with very small
datasets. To keep our dataset small, we will use 40% of the original
training data (25,000 images) for training, 10% for validation, and 10%
for testing.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># reticulate::py_install("tensorflow-datasets")</span></span>
<span><span class="va">tfds</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"tensorflow_datasets"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">train_ds</span>, <span class="va">validation_ds</span>, <span class="va">test_ds</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">tfds</span><span class="op">$</span><span class="fu">load</span><span class="op">(</span></span>
<span>  <span class="st">"cats_vs_dogs"</span>,</span>
<span>  <span class="co"># Reserve 10% for validation and 10% for test</span></span>
<span>  split <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"train[:40%]"</span>, <span class="st">"train[40%:50%]"</span>, <span class="st">"train[50%:60%]"</span><span class="op">)</span>,</span>
<span>  as_supervised <span class="op">=</span> <span class="cn">TRUE</span>  <span class="co"># Include labels</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">train_ds</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 9305</span></span></code></pre>
<p>These are the first 9 images in the training dataset – as you can
see, they’re all different sizes.</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/tfdatasets" class="external-link">tfdatasets</a></span>, exclude <span class="op">=</span> <span class="st">"shape"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">)</span>, mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">0</span>,<span class="fl">1.5</span>,<span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">train_ds</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_take.html" class="external-link">dataset_take</a></span><span class="op">(</span><span class="fl">9</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/as_array_iterator.html" class="external-link">as_array_iterator</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/iterate.html" class="external-link">iterate</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">batch</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">image</span>, <span class="va">label</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">batch</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/as.raster.html" class="external-link">as.raster</a></span><span class="op">(</span><span class="va">image</span>, max <span class="op">=</span> <span class="fl">255L</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/title.html" class="external-link">title</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html" class="external-link">sprintf</a></span><span class="op">(</span></span>
<span>      <span class="st">"label: %s   size: %s"</span>,</span>
<span>      <span class="va">label</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">image</span><span class="op">)</span>, collapse <span class="op">=</span> <span class="st">" x "</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span></code></pre></div>
<div class="float">
<img src="transfer_learning/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13"><div class="figcaption">plot of chunk unnamed-chunk-13</div>
</div>
<p>We can also see that label 1 is “dog” and label 0 is “cat”.</p>
</div>
<div class="section level3">
<h3 id="standardizing-the-data">Standardizing the data<a class="anchor" aria-label="anchor" href="#standardizing-the-data"></a>
</h3>
<p>Our raw images have a variety of sizes. In addition, each pixel
consists of 3 integer values between 0 and 255 (RGB level values). This
isn’t a great fit for feeding a neural network. We need to do 2
things:</p>
<ul>
<li>Standardize to a fixed image size. We pick 150x150.</li>
<li>Normalize pixel values between -1 and 1. We’ll do this using a
<code>Normalization</code> layer as part of the model itself.</li>
</ul>
<p>In general, it’s a good practice to develop models that take raw data
as input, as opposed to models that take already-preprocessed data. The
reason being that, if your model expects preprocessed data, any time you
export your model to use it elsewhere (in a web browser, in a mobile
app), you’ll need to reimplement the exact same preprocessing pipeline.
This gets very tricky very quickly. So we should do the least possible
amount of preprocessing before hitting the model.</p>
<p>Here, we’ll do image resizing in the data pipeline (because a deep
neural network can only process contiguous batches of data), and we’ll
do the input value scaling as part of the model, when we create it.</p>
<p>Let’s resize images to 150x150:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">resize_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_resizing.html">layer_resizing</a></span><span class="op">(</span>width <span class="op">=</span> <span class="fl">150</span>, height <span class="op">=</span> <span class="fl">150</span><span class="op">)</span></span>
<span><span class="va">resize_pair</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">resize_fn</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_ds</span> <span class="op">&lt;-</span> <span class="va">train_ds</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_map.html" class="external-link">dataset_map</a></span><span class="op">(</span><span class="va">resize_pair</span><span class="op">)</span></span>
<span><span class="va">validation_ds</span> <span class="op">&lt;-</span> <span class="va">validation_ds</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_map.html" class="external-link">dataset_map</a></span><span class="op">(</span><span class="va">resize_pair</span><span class="op">)</span></span>
<span><span class="va">test_ds</span> <span class="op">&lt;-</span> <span class="va">test_ds</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_map.html" class="external-link">dataset_map</a></span><span class="op">(</span><span class="va">resize_pair</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="using-random-data-augmentation">Using random data augmentation<a class="anchor" aria-label="anchor" href="#using-random-data-augmentation"></a>
</h3>
<p>When you don’t have a large image dataset, it’s a good practice to
artificially introduce sample diversity by applying random yet realistic
transformations to the training images, such as random horizontal
flipping or small random rotations. This helps expose the model to
different aspects of the training data while slowing down
overfitting.</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data_augmentation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_random_flip.html">layer_random_flip</a></span><span class="op">(</span><span class="st">"horizontal"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_random_rotation.html">layer_random_rotation</a></span><span class="op">(</span><span class="fl">.1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_ds</span> <span class="op">&lt;-</span> <span class="va">train_ds</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_map.html" class="external-link">dataset_map</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu">data_augmentation</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>Let’s batch the data and use prefetching to optimize loading
speed.</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/tensorflow" class="external-link">tensorflow</a></span>, exclude <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"shape"</span>, <span class="st">"set_random_seed"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">batch_size</span> <span class="op">&lt;-</span> <span class="fl">64</span></span>
<span></span>
<span><span class="va">train_ds</span> <span class="op">&lt;-</span> <span class="va">train_ds</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="va">batch_size</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_prefetch.html" class="external-link">dataset_prefetch</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">validation_ds</span> <span class="op">&lt;-</span> <span class="va">validation_ds</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="va">batch_size</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_prefetch.html" class="external-link">dataset_prefetch</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">test_ds</span> <span class="op">&lt;-</span> <span class="va">test_ds</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="va">batch_size</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_prefetch.html" class="external-link">dataset_prefetch</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>Let’s visualize what the first image of the first batch looks like
after various random transformations:</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">batch</span> <span class="op">&lt;-</span> <span class="va">train_ds</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_take.html" class="external-link">dataset_take</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/iterate.html" class="external-link">as_iterator</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/iterate.html" class="external-link">iter_next</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">images</span>, <span class="va">labels</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">batch</span></span>
<span><span class="va">first_image</span> <span class="op">&lt;-</span> <span class="va">images</span><span class="op">[</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/all_dims.html" class="external-link">all_dims</a></span><span class="op">(</span><span class="op">)</span>, drop <span class="op">=</span> <span class="cn">TRUE</span><span class="op">]</span></span>
<span><span class="va">augmented_image</span> <span class="op">&lt;-</span> <span class="fu">data_augmentation</span><span class="op">(</span><span class="va">first_image</span>, training <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="va">plot_image</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">image</span>, <span class="va">main</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html" class="external-link">deparse1</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/substitute.html" class="external-link">substitute</a></span><span class="op">(</span><span class="va">image</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">image</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">as.array</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="co"># convert from tensor to R array</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/grDevices/as.raster.html" class="external-link">as.raster</a></span><span class="op">(</span>max <span class="op">=</span> <span class="fl">255</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">main</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/graphics/title.html" class="external-link">title</a></span><span class="op">(</span><span class="va">main</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span>, mar <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1.5</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">plot_image</span><span class="op">(</span><span class="va">first_image</span><span class="op">)</span></span>
<span><span class="fu">plot_image</span><span class="op">(</span><span class="va">augmented_image</span><span class="op">)</span></span>
<span><span class="fu">plot_image</span><span class="op">(</span><span class="fu">data_augmentation</span><span class="op">(</span><span class="va">first_image</span>, training <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="st">"augmented 2"</span><span class="op">)</span></span>
<span><span class="fu">plot_image</span><span class="op">(</span><span class="fu">data_augmentation</span><span class="op">(</span><span class="va">first_image</span>, training <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="st">"augmented 3"</span><span class="op">)</span></span></code></pre></div>
<div class="float">
<img src="transfer_learning/unnamed-chunk-17-1.png" alt="plot of chunk unnamed-chunk-17"><div class="figcaption">plot of chunk unnamed-chunk-17</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="build-a-model">Build a model<a class="anchor" aria-label="anchor" href="#build-a-model"></a>
</h2>
<p>Now let’s built a model that follows the blueprint we’ve explained
earlier.</p>
<p>Note that:</p>
<ul>
<li>We add a <code>Rescaling</code> layer to scale input values
(initially in the <code>[0, 255]</code> range) to the
<code>[-1, 1]</code> range.</li>
<li>We add a <code>Dropout</code> layer before the classification layer,
for regularization.</li>
<li>We make sure to pass <code>training=FALSE</code> when calling the
base model, so that it runs in inference mode, so that batchnorm
statistics don’t get updated even after we unfreeze the base model for
fine-tuning.</li>
</ul>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">base_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/application_xception.html">application_xception</a></span><span class="op">(</span></span>
<span>  weights <span class="op">=</span> <span class="st">"imagenet"</span>, <span class="co"># Load weights pre-trained on ImageNet.</span></span>
<span>  input_shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">150</span>, <span class="fl">150</span>, <span class="fl">3</span><span class="op">)</span>,</span>
<span>  include_top <span class="op">=</span> <span class="cn">FALSE</span>, <span class="co"># Do not include the ImageNet classifier at the top.</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Freeze the base_model</span></span>
<span><span class="va">base_model</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span></span>
<span></span>
<span><span class="co"># Create new model on top</span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">150</span>, <span class="fl">150</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Pre-trained Xception weights requires that input be scaled</span></span>
<span><span class="co"># from (0, 255) to a range of (-1., +1.), the rescaling layer</span></span>
<span><span class="co"># outputs: `(inputs * scale) + offset`</span></span>
<span><span class="va">scale_layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_rescaling.html">layer_rescaling</a></span><span class="op">(</span>scale <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="fl">127.5</span>, offset <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">scale_layer</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The base model contains batchnorm layers. We want to keep them in inference mode</span></span>
<span><span class="co"># when we unfreeze the base model for fine-tuning, so we make sure that the</span></span>
<span><span class="co"># base_model is running in inference mode here.</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">base_model</span><span class="op">(</span>training <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_global_average_pooling_2d.html">layer_global_average_pooling_2d</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dropout.html">layer_dropout</a></span><span class="op">(</span><span class="fl">0.2</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">model</span>, show_trainable <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="font-weight: bold;">Model: "functional_8"</span></span></span>
<span><span class="co">## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓</span></span>
<span><span class="co">## ┃<span style="font-weight: bold;"> Layer (type)                </span>┃<span style="font-weight: bold;"> Output Shape          </span>┃<span style="font-weight: bold;">    Param # </span>┃<span style="font-weight: bold;"> Trai… </span>┃</span></span>
<span><span class="co">## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩</span></span>
<span><span class="co">## │ input_layer_7 (<span style="color: #0087FF;">InputLayer</span>)  │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">3</span>)   │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ rescaling (<span style="color: #0087FF;">Rescaling</span>)       │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">3</span>)   │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ xception (<span style="color: #0087FF;">Functional</span>)       │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">5</span>, <span style="color: #00AF00;">5</span>, <span style="color: #00AF00;">2048</span>)    │ <span style="color: #00AF00;">20,861,480</span> │   <span style="color: #FF5555; font-weight: bold;">N</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ global_average_pooling2d_1  │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">2048</span>)          │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## │ (<span style="color: #0087FF;">GlobalAveragePooling2D</span>)    │                       │            │       │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ dropout (<span style="color: #0087FF;">Dropout</span>)           │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">2048</span>)          │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ dense_8 (<span style="color: #0087FF;">Dense</span>)             │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">1</span>)             │      <span style="color: #00AF00;">2,049</span> │   <span style="color: #00AF00; font-weight: bold;">Y</span>   │</span></span>
<span><span class="co">## └─────────────────────────────┴───────────────────────┴────────────┴───────┘</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Total params: </span><span style="color: #00AF00;">20,863,529</span> (79.59 MB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Trainable params: </span><span style="color: #00AF00;">2,049</span> (8.00 KB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Non-trainable params: </span><span style="color: #00AF00;">20,861,480</span> (79.58 MB)</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="train-the-top-layer">Train the top layer<a class="anchor" aria-label="anchor" href="#train-the-top-layer"></a>
</h2>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_binary_crossentropy.html">loss_binary_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_binary_accuracy.html">metric_binary_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">epochs</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_ds</span>, epochs <span class="op">=</span> <span class="va">epochs</span>, validation_data <span class="op">=</span> <span class="va">validation_ds</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 146/146 - 39s - 265ms/step - binary_accuracy: 0.9183 - loss: 0.1887 - val_binary_accuracy: 0.9669 - val_loss: 0.0926</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="do-a-round-of-fine-tuning-of-the-entire-model">Do a round of fine-tuning of the entire model<a class="anchor" aria-label="anchor" href="#do-a-round-of-fine-tuning-of-the-entire-model"></a>
</h2>
<p>Finally, let’s unfreeze the base model and train the entire model
end-to-end with a low learning rate.</p>
<p>Importantly, although the base model becomes trainable, it is still
running in inference mode since we passed <code>training=FALSE</code>
when calling it when we built the model. This means that the batch
normalization layers inside won’t update their batch statistics. If they
did, they would wreck havoc on the representations learned by the model
so far.</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Unfreeze the base_model. Note that it keeps running in inference mode</span></span>
<span><span class="co"># since we passed `training=FALSE` when calling it. This means that</span></span>
<span><span class="co"># the batchnorm layers will not update their batch statistics.</span></span>
<span><span class="co"># This prevents the batchnorm layers from undoing all the training</span></span>
<span><span class="co"># we've done so far.</span></span>
<span><span class="va">base_model</span><span class="op">$</span><span class="va">trainable</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">model</span>, show_trainable <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="font-weight: bold;">Model: "functional_8"</span></span></span>
<span><span class="co">## ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┓</span></span>
<span><span class="co">## ┃<span style="font-weight: bold;"> Layer (type)                </span>┃<span style="font-weight: bold;"> Output Shape          </span>┃<span style="font-weight: bold;">    Param # </span>┃<span style="font-weight: bold;"> Trai… </span>┃</span></span>
<span><span class="co">## ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━┩</span></span>
<span><span class="co">## │ input_layer_7 (<span style="color: #0087FF;">InputLayer</span>)  │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">3</span>)   │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ rescaling (<span style="color: #0087FF;">Rescaling</span>)       │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">150</span>, <span style="color: #00AF00;">3</span>)   │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ xception (<span style="color: #0087FF;">Functional</span>)       │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">5</span>, <span style="color: #00AF00;">5</span>, <span style="color: #00AF00;">2048</span>)    │ <span style="color: #00AF00;">20,861,480</span> │   <span style="color: #00AF00; font-weight: bold;">Y</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ global_average_pooling2d_1  │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">2048</span>)          │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## │ (<span style="color: #0087FF;">GlobalAveragePooling2D</span>)    │                       │            │       │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ dropout (<span style="color: #0087FF;">Dropout</span>)           │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">2048</span>)          │          <span style="color: #00AF00;">0</span> │   <span style="font-weight: bold;">-</span>   │</span></span>
<span><span class="co">## ├─────────────────────────────┼───────────────────────┼────────────┼───────┤</span></span>
<span><span class="co">## │ dense_8 (<span style="color: #0087FF;">Dense</span>)             │ (<span style="color: #00D7FF;">None</span>, <span style="color: #00AF00;">1</span>)             │      <span style="color: #00AF00;">2,049</span> │   <span style="color: #00AF00; font-weight: bold;">Y</span>   │</span></span>
<span><span class="co">## └─────────────────────────────┴───────────────────────┴────────────┴───────┘</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Total params: </span><span style="color: #00AF00;">20,867,629</span> (79.60 MB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Trainable params: </span><span style="color: #00AF00;">20,809,001</span> (79.38 MB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Non-trainable params: </span><span style="color: #00AF00;">54,528</span> (213.00 KB)</span></span>
<span><span class="co">## <span style="font-weight: bold;"> Optimizer params: </span><span style="color: #00AF00;">4,100</span> (16.02 KB)</span></span></code></pre>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span><span class="fl">1e-5</span><span class="op">)</span>, <span class="co"># Low learning rate</span></span>
<span>  loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_binary_crossentropy.html">loss_binary_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_binary_accuracy.html">metric_binary_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">epochs</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">train_ds</span>, epochs <span class="op">=</span> <span class="va">epochs</span>, validation_data <span class="op">=</span> <span class="va">validation_ds</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 146/146 - 84s - 577ms/step - binary_accuracy: 0.8660 - loss: 0.3213 - val_binary_accuracy: 0.9652 - val_loss: 0.1022</span></span></code></pre>
<p>After 10 epochs, fine-tuning gains us a nice improvement here. Let’s
evaluate the model on the test dataset:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">test_ds</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 37/37 - 2s - 44ms/step - binary_accuracy: 0.9540 - loss: 0.1103</span></span></code></pre>
<pre><code><span><span class="co">## $binary_accuracy</span></span>
<span><span class="co">## [1] 0.9539983</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.1102648</span></span></code></pre>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
