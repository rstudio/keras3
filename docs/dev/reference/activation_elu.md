# Exponential Linear Unit.

The exponential linear unit (ELU) with `alpha > 0` is defined as:

- `x` if `x > 0`

- `alpha * exp(x) - 1` if `x < 0`

ELUs have negative values which pushes the mean of the activations
closer to zero.

Mean activations that are closer to zero enable faster learning as they
bring the gradient closer to the natural gradient. ELUs saturate to a
negative value when the argument gets smaller. Saturation means a small
derivative which decreases the variation and the information that is
propagated to the next layer.

## Usage

``` r
activation_elu(x, alpha = 1)
```

## Arguments

- x:

  Input tensor.

- alpha:

  A scalar, slope of positive section. Defaults to `1.0`.

## Value

A tensor, the result from applying the activation to the input tensor
`x`.

## Reference

- [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)

## See also

- <https://keras.io/api/layers/activations#elu-function>

Other activations:  
[`activation_celu()`](https://keras3.posit.co/dev/reference/activation_celu.md)  
[`activation_exponential()`](https://keras3.posit.co/dev/reference/activation_exponential.md)  
[`activation_gelu()`](https://keras3.posit.co/dev/reference/activation_gelu.md)  
[`activation_glu()`](https://keras3.posit.co/dev/reference/activation_glu.md)  
[`activation_hard_shrink()`](https://keras3.posit.co/dev/reference/activation_hard_shrink.md)  
[`activation_hard_sigmoid()`](https://keras3.posit.co/dev/reference/activation_hard_sigmoid.md)  
[`activation_hard_tanh()`](https://keras3.posit.co/dev/reference/activation_hard_tanh.md)  
[`activation_leaky_relu()`](https://keras3.posit.co/dev/reference/activation_leaky_relu.md)  
[`activation_linear()`](https://keras3.posit.co/dev/reference/activation_linear.md)  
[`activation_log_sigmoid()`](https://keras3.posit.co/dev/reference/activation_log_sigmoid.md)  
[`activation_log_softmax()`](https://keras3.posit.co/dev/reference/activation_log_softmax.md)  
[`activation_mish()`](https://keras3.posit.co/dev/reference/activation_mish.md)  
[`activation_relu()`](https://keras3.posit.co/dev/reference/activation_relu.md)  
[`activation_relu6()`](https://keras3.posit.co/dev/reference/activation_relu6.md)  
[`activation_selu()`](https://keras3.posit.co/dev/reference/activation_selu.md)  
[`activation_sigmoid()`](https://keras3.posit.co/dev/reference/activation_sigmoid.md)  
[`activation_silu()`](https://keras3.posit.co/dev/reference/activation_silu.md)  
[`activation_soft_shrink()`](https://keras3.posit.co/dev/reference/activation_soft_shrink.md)  
[`activation_softmax()`](https://keras3.posit.co/dev/reference/activation_softmax.md)  
[`activation_softplus()`](https://keras3.posit.co/dev/reference/activation_softplus.md)  
[`activation_softsign()`](https://keras3.posit.co/dev/reference/activation_softsign.md)  
[`activation_sparse_plus()`](https://keras3.posit.co/dev/reference/activation_sparse_plus.md)  
[`activation_sparse_sigmoid()`](https://keras3.posit.co/dev/reference/activation_sparse_sigmoid.md)  
[`activation_sparsemax()`](https://keras3.posit.co/dev/reference/activation_sparsemax.md)  
[`activation_squareplus()`](https://keras3.posit.co/dev/reference/activation_squareplus.md)  
[`activation_tanh()`](https://keras3.posit.co/dev/reference/activation_tanh.md)  
[`activation_tanh_shrink()`](https://keras3.posit.co/dev/reference/activation_tanh_shrink.md)  
[`activation_threshold()`](https://keras3.posit.co/dev/reference/activation_threshold.md)  
