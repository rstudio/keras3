<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts • keras3</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet"><meta property="og:title" content="A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts"><meta name="description" content="See Loshchilov &amp;amp; Hutter, ICLR2016,
SGDR: Stochastic Gradient Descent with Warm Restarts.
When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a step value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.
The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.
The learning rate multiplier first decays
from 1 to alpha for first_decay_steps steps. Then, a warm
restart is performed. Each new warm restart runs for t_mul times more
steps and with m_mul times initial learning rate as the new learning rate."><meta property="og:description" content="See Loshchilov &amp;amp; Hutter, ICLR2016,
SGDR: Stochastic Gradient Descent with Warm Restarts.
When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a step value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.
The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.
The learning rate multiplier first decays
from 1 to alpha for first_decay_steps steps. Then, a warm
restart is performed. Each new warm restart runs for t_mul times more
steps and with m_mul times initial learning rate as the new learning rate."><meta name="robots" content="noindex"><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager --></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.0.0.9001</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides"><li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>A <code>LearningRateSchedule</code> that uses a cosine decay schedule with restarts.</h1>
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/R/optimizers-schedules.R" class="external-link"><code>R/optimizers-schedules.R</code></a></small>
      <div class="d-none name"><code>learning_rate_schedule_cosine_decay_restarts.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>See <a href="https://arxiv.org/abs/1608.03983" class="external-link">Loshchilov &amp; Hutter, ICLR2016</a>,
SGDR: Stochastic Gradient Descent with Warm Restarts.</p>
<p>When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a <code>step</code> value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.</p>
<p>The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.</p>
<p>The learning rate multiplier first decays
from 1 to <code>alpha</code> for <code>first_decay_steps</code> steps. Then, a warm
restart is performed. Each new warm restart runs for <code>t_mul</code> times more
steps and with <code>m_mul</code> times initial learning rate as the new learning rate.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">learning_rate_schedule_cosine_decay_restarts</span><span class="op">(</span></span>
<span>  <span class="va">initial_learning_rate</span>,</span>
<span>  <span class="va">first_decay_steps</span>,</span>
<span>  t_mul <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  m_mul <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  name <span class="op">=</span> <span class="st">"SGDRDecay"</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-initial-learning-rate">initial_learning_rate<a class="anchor" aria-label="anchor" href="#arg-initial-learning-rate"></a></dt>
<dd><p>A float. The initial learning rate.</p></dd>


<dt id="arg-first-decay-steps">first_decay_steps<a class="anchor" aria-label="anchor" href="#arg-first-decay-steps"></a></dt>
<dd><p>An integer. Number of steps to decay over.</p></dd>


<dt id="arg-t-mul">t_mul<a class="anchor" aria-label="anchor" href="#arg-t-mul"></a></dt>
<dd><p>A float. Used to derive the number of iterations in
the i-th period.</p></dd>


<dt id="arg-m-mul">m_mul<a class="anchor" aria-label="anchor" href="#arg-m-mul"></a></dt>
<dd><p>A float. Used to derive the initial learning rate of
the i-th period.</p></dd>


<dt id="arg-alpha">alpha<a class="anchor" aria-label="anchor" href="#arg-alpha"></a></dt>
<dd><p>A float. Minimum learning rate value as a fraction of
the <code>initial_learning_rate</code>.</p></dd>


<dt id="arg-name">name<a class="anchor" aria-label="anchor" href="#arg-name"></a></dt>
<dd><p>String. Optional name of the operation. Defaults to
<code>"SGDRDecay"</code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A 1-arg callable learning rate schedule that takes the current optimizer
step and outputs the decayed learning rate, a scalar tensor of the
same type as <code>initial_learning_rate</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a></h2>
    <p></p><div class="sourceCode r"><pre><code><span><span class="va">first_decay_steps</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">lr_decayed_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/learning_rate_schedule_cosine_decay_restarts.html">learning_rate_schedule_cosine_decay_restarts</a></span><span class="op">(</span></span>
<span>        <span class="fl">0.001</span>,</span>
<span>        <span class="va">first_decay_steps</span><span class="op">)</span></span></code></pre><p></p></div>
<p>You can pass this schedule directly into a <code>optimizer</code>
as the learning rate. The learning rate schedule is also serializable and
deserializable using <code>keras$optimizers$schedules$serialize</code> and
<code>keras$optimizers$schedules$deserialize</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><a href="https://keras.io/api/optimizers/learning_rate_schedules/cosine_decay_restarts#cosinedecayrestarts-class" class="external-link">https://keras.io/api/optimizers/learning_rate_schedules/cosine_decay_restarts#cosinedecayrestarts-class</a></p></li>
</ul><p>Other optimizer learning rate schedules: <br><code><a href="LearningRateSchedule.html">LearningRateSchedule</a>()</code> <br><code><a href="learning_rate_schedule_cosine_decay.html">learning_rate_schedule_cosine_decay</a>()</code> <br><code><a href="learning_rate_schedule_exponential_decay.html">learning_rate_schedule_exponential_decay</a>()</code> <br><code><a href="learning_rate_schedule_inverse_time_decay.html">learning_rate_schedule_inverse_time_decay</a>()</code> <br><code><a href="learning_rate_schedule_piecewise_constant_decay.html">learning_rate_schedule_piecewise_constant_decay</a>()</code> <br><code><a href="learning_rate_schedule_polynomial_decay.html">learning_rate_schedule_polynomial_decay</a>()</code> <br></p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.9000.</p>
</div>

    </footer></div>





  </body></html>

