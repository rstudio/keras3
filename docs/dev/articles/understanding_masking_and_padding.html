<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Understanding masking &amp; padding • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Understanding masking &amp; padding">
<meta name="description" content="Complete guide to using mask-aware sequence layers in Keras.">
<meta property="og:description" content="Complete guide to using mask-aware sequence layers in Keras.">
<meta name="robots" content="noindex">
<!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager -->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.4.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides">
<li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Understanding masking &amp; padding</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/vignettes/understanding_masking_and_padding.Rmd" class="external-link"><code>vignettes/understanding_masking_and_padding.Rmd</code></a></small>
      <div class="d-none name"><code>understanding_masking_and_padding.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://keras3.posit.co/">keras3</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p><strong>Masking</strong> is a way to tell sequence-processing layers
that certain timesteps in an input are missing, and thus should be
skipped when processing the data.</p>
<p><strong>Padding</strong> is a special form of masking where the
masked steps are at the start or the end of a sequence. Padding comes
from the need to encode sequence data into contiguous batches: in order
to make all sequences in a batch fit a given standard length, it is
necessary to pad or truncate some sequences.</p>
<p>Let’s take a close look.</p>
</div>
<div class="section level2">
<h2 id="padding-sequence-data">Padding sequence data<a class="anchor" aria-label="anchor" href="#padding-sequence-data"></a>
</h2>
<p>When processing sequence data, it is very common for individual
samples to have different lengths. Consider the following example (text
tokenized as words):</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"Hello"</span>, <span class="st">"world"</span>, <span class="st">"!"</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"How"</span>, <span class="st">"are"</span>, <span class="st">"you"</span>, <span class="st">"doing"</span>, <span class="st">"today"</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"The"</span>, <span class="st">"weather"</span>, <span class="st">"will"</span>, <span class="st">"be"</span>, <span class="st">"nice"</span>, <span class="st">"tomorrow"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>After vocabulary lookup, the data might be vectorized as integers,
e.g.:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">71</span>, <span class="fl">1331</span>, <span class="fl">4231</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">73</span>, <span class="fl">8</span>, <span class="fl">3215</span>, <span class="fl">55</span>, <span class="fl">927</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">83</span>, <span class="fl">91</span>, <span class="fl">1</span>, <span class="fl">645</span>, <span class="fl">1253</span>, <span class="fl">927</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The data is a nested list where individual samples have length 3, 5,
and 6, respectively. Since the input data for a deep learning model must
be a single tensor (of shape
e.g. <code>(batch_size, 6, vocab_size)</code> in this case), samples
that are shorter than the longest item need to be padded with some
placeholder value (alternatively, one might also truncate long samples
before padding short samples).</p>
<p>Keras provides a utility function to truncate and pad Python lists to
a common length: <code>pad_sequences</code>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">raw_inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">711</span>, <span class="fl">632</span>, <span class="fl">71</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">73</span>, <span class="fl">8</span>, <span class="fl">3215</span>, <span class="fl">55</span>, <span class="fl">927</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">83</span>, <span class="fl">91</span>, <span class="fl">1</span>, <span class="fl">645</span>, <span class="fl">1253</span>, <span class="fl">927</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># By default, this will pad using 0s; it is configurable via the</span></span>
<span><span class="co"># "value" parameter.</span></span>
<span><span class="co"># Note that you could use "pre" padding (at the beginning) or</span></span>
<span><span class="co"># "post" padding (at the end).</span></span>
<span><span class="co"># We recommend using "post" padding when working with RNN layers</span></span>
<span><span class="co"># (in order to be able to use the</span></span>
<span><span class="co"># CuDNN implementation of the layers).</span></span>
<span><span class="va">padded_inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/pad_sequences.html">pad_sequences</a></span><span class="op">(</span><span class="va">raw_inputs</span>, padding<span class="op">=</span><span class="st">"post"</span><span class="op">)</span></span>
<span><span class="va">padded_inputs</span></span></code></pre></div>
<pre><code><span><span class="co">##      [,1] [,2] [,3] [,4] [,5] [,6]</span></span>
<span><span class="co">## [1,]  711  632   71    0    0    0</span></span>
<span><span class="co">## [2,]   73    8 3215   55  927    0</span></span>
<span><span class="co">## [3,]   83   91    1  645 1253  927</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="masking">Masking<a class="anchor" aria-label="anchor" href="#masking"></a>
</h2>
<p>Now that all samples have a uniform length, the model must be
informed that some part of the data is actually padding and should be
ignored. That mechanism is <strong>masking</strong>.</p>
<p>There are three ways to introduce input masks in Keras models:</p>
<ul>
<li>Add a <code>layer_masking</code> layer.</li>
<li>Configure a <code>layer_embedding</code> layer with
<code>mask_zero=TRUE</code>.</li>
<li>Pass a <code>mask</code> argument manually when calling layers that
support this argument (e.g. RNN layers).</li>
</ul>
</div>
<div class="section level2">
<h2 id="mask-generating-layers-embedding-and-masking">Mask-generating layers: <code>Embedding</code> and
<code>Masking</code><a class="anchor" aria-label="anchor" href="#mask-generating-layers-embedding-and-masking"></a>
</h2>
<p>Under the hood, these layers will create a mask tensor (2D tensor
with shape <code>(batch, sequence_length)</code>), and attach it to the
tensor output returned by the <code>Masking</code> or
<code>Embedding</code> layer.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span>input_dim<span class="op">=</span><span class="fl">5000</span>, output_dim<span class="op">=</span><span class="fl">16</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">masked_output</span> <span class="op">&lt;-</span> <span class="fu">embedding</span><span class="op">(</span><span class="va">padded_inputs</span><span class="op">)</span></span>
<span></span>
<span><span class="va">masked_output</span><span class="op">$</span><span class="va">`_keras_mask`</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ True  True  True False False False]</span></span>
<span><span class="co">##  [ True  True  True  True  True False]</span></span>
<span><span class="co">##  [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)</span></span></code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">masking_layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_masking.html">layer_masking</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co"># Simulate the embedding lookup by expanding the 2D input to 3D,</span></span>
<span><span class="co"># with embedding dimension of 10.</span></span>
<span><span class="va">unmasked_embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span></span>
<span>    <span class="fu"><a href="../reference/op_tile.html">op_tile</a></span><span class="op">(</span><span class="fu"><a href="../reference/op_expand_dims.html">op_expand_dims</a></span><span class="op">(</span><span class="va">padded_inputs</span>, axis<span class="op">=</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1L</span>, <span class="fl">1L</span>, <span class="fl">10L</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">masked_embedding</span> <span class="op">&lt;-</span> <span class="fu">masking_layer</span><span class="op">(</span><span class="va">unmasked_embedding</span><span class="op">)</span></span>
<span><span class="va">masked_embedding</span><span class="op">$</span><span class="va">`_keras_mask`</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ True  True  True False False False]</span></span>
<span><span class="co">##  [ True  True  True  True  True False]</span></span>
<span><span class="co">##  [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)</span></span></code></pre>
<p>As you can see from the printed result, the mask is a 2D boolean
tensor with shape <code>(batch_size, sequence_length)</code>, where each
individual <code>FALSE</code> entry indicates that the corresponding
timestep should be ignored during processing.</p>
</div>
<div class="section level2">
<h2 id="mask-propagation-in-the-functional-api-and-sequential-api">Mask propagation in the Functional API and Sequential API<a class="anchor" aria-label="anchor" href="#mask-propagation-in-the-functional-api-and-sequential-api"></a>
</h2>
<p>When using the Functional API or the Sequential API, a mask generated
by an <code>Embedding</code> or <code>Masking</code> layer will be
propagated through the network for any layer that is capable of using
them (for example, RNN layers). Keras will automatically fetch the mask
corresponding to an input and pass it to any layer that knows how to use
it.</p>
<p>For instance, in the following Sequential model, the
<code>LSTM</code> layer will automatically receive a mask, which means
it will ignore padded values:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span>input_dim<span class="op">=</span><span class="fl">5000</span>, output_dim<span class="op">=</span><span class="fl">16</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_lstm.html">layer_lstm</a></span><span class="op">(</span>units<span class="op">=</span><span class="fl">32</span><span class="op">)</span></span></code></pre></div>
<p>This is also the case for the following Functional API model:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span>, dtype<span class="op">=</span><span class="st">"int32"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span>input_dim<span class="op">=</span><span class="fl">5000</span>, output_dim<span class="op">=</span><span class="fl">16</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_lstm.html">layer_lstm</a></span><span class="op">(</span>units<span class="op">=</span><span class="fl">32</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="passing-mask-tensors-directly-to-layers">Passing mask tensors directly to layers<a class="anchor" aria-label="anchor" href="#passing-mask-tensors-directly-to-layers"></a>
</h2>
<p>Layers that can handle masks (such as the <code>LSTM</code> layer)
have a <code>mask</code> argument in their <code>call</code> method.</p>
<p>Meanwhile, layers that produce a mask (e.g. <code>Embedding</code>)
expose a <code>compute_mask(input, previous_mask)</code> method which
you can call.</p>
<p>Thus, you can pass the output of the <code>compute_mask()</code>
method of a mask-producing layer to the <code>call</code> method of a
mask-consuming layer, like this:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">MyLayer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"MyLayer"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">embedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span></span>
<span>      input_dim<span class="op">=</span><span class="fl">5000</span>, output_dim<span class="op">=</span><span class="fl">16</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">lstm</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_lstm.html">layer_lstm</a></span><span class="op">(</span>units<span class="op">=</span><span class="fl">32</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">embedding</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>      <span class="co"># Note that you could also prepare a `mask` tensor manually.</span></span>
<span>      <span class="co"># It only needs to be a boolean tensor</span></span>
<span>      <span class="co"># with the right shape, i.e. (batch_size, timesteps).</span></span>
<span>      <span class="va">self</span><span class="op">$</span><span class="fu">lstm</span><span class="op">(</span>mask<span class="op">=</span><span class="va">self</span><span class="op">$</span><span class="va">embedding</span><span class="op">$</span><span class="fu">compute_mask</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">layer</span> <span class="op">&lt;-</span> <span class="fu">MyLayer</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_integer.html">random_integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">10</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/layer.html" class="external-link">layer</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ 0.00130048 -0.00113367 -0.00715671 ... -0.00107615 -0.00162071</span></span>
<span><span class="co">##    0.00135018]</span></span>
<span><span class="co">##  [-0.004185    0.00726349  0.00520932 ...  0.00119117  0.00230441</span></span>
<span><span class="co">##    0.00174123]</span></span>
<span><span class="co">##  [-0.00537032 -0.00164898 -0.00238435 ... -0.00154158 -0.0038603</span></span>
<span><span class="co">##   -0.00105811]</span></span>
<span><span class="co">##  ...</span></span>
<span><span class="co">##  [ 0.00622133 -0.00905907 -0.00599518 ...  0.00025823 -0.00142478</span></span>
<span><span class="co">##   -0.00125036]</span></span>
<span><span class="co">##  [-0.00523904  0.00336683 -0.00299453 ...  0.00876719  0.00172074</span></span>
<span><span class="co">##    0.00903089]</span></span>
<span><span class="co">##  [-0.00393721  0.00058538  0.00503809 ... -0.00203075  0.00325885</span></span>
<span><span class="co">##   -0.00299755]], shape=(32, 32), dtype=float32)</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="supporting-masking-in-your-custom-layers">Supporting masking in your custom layers<a class="anchor" aria-label="anchor" href="#supporting-masking-in-your-custom-layers"></a>
</h2>
<p>Sometimes, you may need to write layers that generate a mask (like
<code>Embedding</code>), or layers that need to modify the current
mask.</p>
<p>For instance, any layer that produces a tensor with a different time
dimension than its input, such as a <code>Concatenate</code> layer that
concatenates on the time dimension, will need to modify the current mask
so that downstream layers will be able to properly take masked timesteps
into account.</p>
<p>To do this, your layer should implement the
<code>layer.compute_mask()</code> method, which produces a new mask
given the input and the current mask.</p>
<p>Here is an example of a <code>TemporalSplit</code> layer that needs
to modify the current mask.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TemporalSplit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"TemporalSplit"</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Expect the input to be 3D and mask to be 2D, split the input tensor into 2</span></span>
<span>    <span class="co"># subtensors along the time axis (axis 1).</span></span>
<span>    <span class="fu"><a href="../reference/op_split.html">op_split</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="fl">2</span>, axis<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  compute_mask <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">mask</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="co"># Also split the mask into 2 if it presents.</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">mask</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="../reference/op_split.html">op_split</a></span><span class="op">(</span><span class="va">mask</span>, <span class="fl">2</span>, axis<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="cn">NULL</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">first_half</span>, <span class="va">second_half</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu">TemporalSplit</span><span class="op">(</span><span class="va">masked_embedding</span><span class="op">)</span></span>
<span><span class="va">first_half</span><span class="op">$</span><span class="va">`_keras_mask`</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ True  True  True]</span></span>
<span><span class="co">##  [ True  True  True]</span></span>
<span><span class="co">##  [ True  True  True]], shape=(3, 3), dtype=bool)</span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">second_half</span><span class="op">$</span><span class="va">`_keras_mask`</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[False False False]</span></span>
<span><span class="co">##  [ True  True False]</span></span>
<span><span class="co">##  [ True  True  True]], shape=(3, 3), dtype=bool)</span></span></code></pre>
<p>Here is another example of a <code>CustomEmbedding</code> layer that
is capable of generating a mask from input values:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">CustomEmbedding</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"CustomEmbedding"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">input_dim</span>, <span class="va">output_dim</span>, <span class="va">mask_zero</span><span class="op">=</span><span class="cn">FALSE</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">input_dim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">input_dim</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">output_dim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">output_dim</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">mask_zero</span> <span class="op">&lt;-</span> <span class="va">mask_zero</span></span>
<span>  <span class="op">}</span>,</span>
<span>  build <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">input_shape</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">input_dim</span>, <span class="va">self</span><span class="op">$</span><span class="va">output_dim</span><span class="op">)</span>,</span>
<span>      initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span>      dtype<span class="op">=</span><span class="st">"float32"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="st">"int32"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="../reference/op_take.html">op_take</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">embeddings</span>, <span class="va">inputs</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  compute_mask <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">mask</span><span class="op">=</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="va">self</span><span class="op">$</span><span class="va">mask_zero</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="cn">NULL</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="../reference/op_not_equal.html">op_not_equal</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">layer</span> <span class="op">&lt;-</span> <span class="fu">CustomEmbedding</span><span class="op">(</span>input_dim <span class="op">=</span> <span class="fl">10</span>, output_dim <span class="op">=</span> <span class="fl">32</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_integer.html">random_integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">10</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">9</span><span class="op">)</span></span>
<span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/layer.html" class="external-link">layer</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">mask</span> <span class="op">&lt;-</span> <span class="va">layer</span><span class="op">$</span><span class="fu">compute_mask</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mask</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ True  True  True  True  True  True  True  True  True  True]</span></span>
<span><span class="co">##  [ True  True  True  True  True  True  True  True False  True]</span></span>
<span><span class="co">##  [ True  True  True  True False  True  True  True  True  True]], shape=(3, 10), dtype=bool)</span></span></code></pre>
<p>Note: For more details about format limitations related to masking,
see the <a href="serialization_and_saving.html">serialization
guide</a>.</p>
</div>
<div class="section level2">
<h2 id="opting-in-to-mask-propagation-on-compatible-layers">Opting-in to mask propagation on compatible layers<a class="anchor" aria-label="anchor" href="#opting-in-to-mask-propagation-on-compatible-layers"></a>
</h2>
<p>Most layers don’t modify the time dimension, so don’t need to modify
the current mask. However, they may still want to be able to
<strong>propagate</strong> the current mask, unchanged, to the next
layer. <strong>This is an opt-in behavior.</strong> By default, a custom
layer will destroy the current mask (since the framework has no way to
tell whether propagating the mask is safe to do).</p>
<p>If you have a custom layer that does not modify the time dimension,
and if you want it to be able to propagate the current input mask, you
should set <code>self.supports_masking = True</code> in the layer
constructor. In this case, the default behavior of
<code>compute_mask()</code> is to just pass the current mask
through.</p>
<p>Here’s an example of a layer that is whitelisted for mask
propagation:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">MyActivation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"MyActivation"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">supports_masking</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="../reference/op_relu.html">op_relu</a></span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>You can now use this custom layer in-between a mask-generating layer
(like <code>Embedding</code>) and a mask-consuming layer (like
<code>LSTM</code>), and it will pass the mask along so that it reaches
the mask-consuming layer.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span>, dtype<span class="op">=</span><span class="st">"int32"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span>input_dim<span class="op">=</span><span class="fl">5000</span>, output_dim<span class="op">=</span><span class="fl">16</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">MyActivation</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_lstm.html">layer_lstm</a></span><span class="op">(</span>units<span class="op">=</span><span class="fl">32</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="fu"><a href="../reference/random_integer.html">random_integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">100</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">5000</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="writing-layers-that-need-mask-information">Writing layers that need mask information<a class="anchor" aria-label="anchor" href="#writing-layers-that-need-mask-information"></a>
</h2>
<p>Some layers are mask <em>consumers</em>: they accept a
<code>mask</code> argument in <code>call</code> and use it to determine
whether to skip certain time steps.</p>
<p>To write such a layer, you can simply add a <code>mask=None</code>
argument in your <code>call</code> signature. The mask associated with
the inputs will be passed to your layer whenever it is available.</p>
<p>Here’s a simple example below: a layer that computes a softmax over
the time dimension (axis 1) of an input sequence, while discarding
masked timesteps.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">TemporalSoftmax</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"TemporalSoftmax"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">supports_masking</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">mask</span><span class="op">=</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">mask</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop</a></span><span class="op">(</span><span class="st">"`TemporalSoftmax` layer requires a previous layer to support masking."</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">broadcast_float_mask</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_expand_dims.html">op_expand_dims</a></span><span class="op">(</span><span class="fu"><a href="../reference/op_cast.html">op_cast</a></span><span class="op">(</span><span class="va">mask</span>, <span class="st">"float32"</span><span class="op">)</span>, <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span>    <span class="va">inputs_exp</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_exp.html">op_exp</a></span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">*</span> <span class="va">broadcast_float_mask</span></span>
<span>    <span class="va">inputs_sum</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_sum.html">op_sum</a></span><span class="op">(</span><span class="va">inputs_exp</span> <span class="op">*</span> <span class="va">broadcast_float_mask</span>, axis<span class="op">=</span><span class="op">-</span><span class="fl">1</span>, keepdims<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span>    <span class="va">inputs_exp</span> <span class="op">/</span> <span class="va">inputs_sum</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="cn">NULL</span><span class="op">)</span>, dtype<span class="op">=</span><span class="st">"int32"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_embedding.html">layer_embedding</a></span><span class="op">(</span>input_dim<span class="op">=</span><span class="fl">10</span>, output_dim<span class="op">=</span><span class="fl">32</span>, mask_zero<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">TemporalSoftmax</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="fu"><a href="../reference/random_integer.html">random_integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">32</span>, <span class="fl">100</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>That is all you need to know about padding &amp; masking in Keras. To
recap:</p>
<ul>
<li>“Masking” is how layers are able to know when to skip / ignore
certain timesteps in sequence inputs.</li>
<li>Some layers are mask-generators: <code>Embedding</code> can generate
a mask from input values (if <code>mask_zero=TRUE</code>), and so can
the <code>Masking</code> layer.</li>
<li>Some layers are mask-consumers: they expose a <code>mask</code>
argument in their <code>call</code> method. This is the case for RNN
layers.</li>
<li>In the Functional API and Sequential API, mask information is
propagated automatically.</li>
<li>When using layers in a standalone way, you can pass the
<code>mask</code> arguments to layers manually.</li>
<li>You can easily write layers that modify the current mask, that
generate a new mask, or that consume the mask associated with the
inputs.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
