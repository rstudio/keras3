<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Introduction to Keras for Researchers • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Introduction to Keras for Researchers">
<meta name="description" content="Everything you need to know to use Keras &amp; TensorFlow for deep learning research.">
<meta property="og:description" content="Everything you need to know to use Keras &amp; TensorFlow for deep learning research.">
<meta name="robots" content="noindex">
<!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager -->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.0.0.9001</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides">
<li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Introduction to Keras for Researchers</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/vignettes/intro_to_keras_for_researchers.Rmd" class="external-link"><code>vignettes/intro_to_keras_for_researchers.Rmd</code></a></small>
      <div class="d-none name"><code>intro_to_keras_for_researchers.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://keras3.posit.co/">keras3</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/tensorflow" class="external-link">tensorflow</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Are you a machine learning researcher? Do you publish at NeurIPS and
push the state-of-the-art in CV and NLP? This guide will serve as your
first introduction to core Keras &amp; TensorFlow API concepts.</p>
<p>In this guide, you will learn about:</p>
<ul>
<li>Tensors, variables, and gradients in TensorFlow</li>
<li>Creating layers by subclassing the [<code>Layer</code>] class</li>
<li>Writing low-level training loops</li>
<li>Tracking losses created by layers via the <code>add_loss()</code>
method</li>
<li>Tracking metrics in a low-level training loop</li>
<li>Speeding up execution with a compiled
[<code><a href="https://rdrr.io/pkg/tensorflow/man/tf_function.html" class="external-link">tensorflow::tf_function()</a></code>]</li>
<li>Executing layers in training or inference mode</li>
<li>The Keras Functional API</li>
</ul>
<p>You will also see the Keras API in action in two end-to-end research
examples: a Variational Autoencoder, and a Hypernetwork.</p>
</div>
<div class="section level2">
<h2 id="tensors">Tensors<a class="anchor" aria-label="anchor" href="#tensors"></a>
</h2>
<p>TensorFlow is an infrastructure layer for differentiable programming.
At its heart, it’s a framework for manipulating N-dimensional arrays
(tensors), much like NumPy.</p>
<p>However, there are three key differences between NumPy and
TensorFlow:</p>
<ul>
<li>TensorFlow can leverage hardware accelerators such as GPUs and
TPUs.</li>
<li>TensorFlow can automatically compute the gradient of arbitrary
differentiable tensor expressions.</li>
<li>TensorFlow computation can be distributed to large numbers of
devices on a single machine, and large number of machines (potentially
with multiple devices each).</li>
</ul>
<p>Let’s take a look at the object that is at the core of TensorFlow:
the Tensor.</p>
<p>Here’s a constant tensor:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">constant</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">2</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[5. 2.]</span></span>
<span><span class="co">##  [1. 3.]], shape=(2, 2), dtype=float64)</span></span></code></pre>
<p>You can get its value as a R array by calling
<code><a href="https://rdrr.io/r/base/array.html" class="external-link">as.array()</a></code>:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">as.array</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##      [,1] [,2]</span></span>
<span><span class="co">## [1,]    5    2</span></span>
<span><span class="co">## [2,]    1    3</span></span></code></pre>
<p>It features the attributes <code>dtype</code> and
<code>shape</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">$</span><span class="va">dtype</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.float64</span></span></code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">$</span><span class="va">shape</span></span></code></pre></div>
<pre><code><span><span class="co">## TensorShape([2, 2])</span></span></code></pre>
<p>A common way to create constant tensors is via <code>tf$ones</code>
and <code>tf$zeros</code>:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[1.]</span></span>
<span><span class="co">##  [1.]], shape=(2, 1), dtype=float32)</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tf</span><span class="op">$</span><span class="fu">zeros</span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[0.]</span></span>
<span><span class="co">##  [0.]], shape=(2, 1), dtype=float32)</span></span></code></pre>
<p>You can also create random constant tensors:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span>, mean <span class="op">=</span> <span class="fl">0.0</span>, stddev <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_uniform.html">random_uniform</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span>, minval <span class="op">=</span> <span class="fl">0</span>, maxval <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="variables">Variables<a class="anchor" aria-label="anchor" href="#variables"></a>
</h2>
<p>Variables are special tensors used to store mutable state (such as
the weights of a neural network). You create a <code>Variable</code>
using some initial value:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">initial_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">Variable</span><span class="op">(</span><span class="va">initial_value</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">a</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## &lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=</span></span>
<span><span class="co">## array([[ 0.9057419 ,  0.7916686 ],</span></span>
<span><span class="co">##        [ 0.28754202, -0.5408822 ]], dtype=float32)&gt;</span></span></code></pre>
<p>You update the value of a <code>Variable</code> by using the methods
<code>$assign(value)</code>, <code>$assign_add(increment)</code>, or
<code>$assign_sub(decrement)</code>:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">new_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">a</span><span class="op">$</span><span class="fu">assign</span><span class="op">(</span><span class="va">new_value</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## &lt;tf.Variable 'UnreadVariable' shape=(2, 2) dtype=float32, numpy=</span></span>
<span><span class="co">## array([[-0.3405368 , -2.1463926 ],</span></span>
<span><span class="co">##        [ 1.2602988 ,  0.12241419]], dtype=float32)&gt;</span></span></code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">added_value</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">a</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="va">added_value</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## &lt;tf.Variable 'UnreadVariable' shape=(2, 2) dtype=float32, numpy=</span></span>
<span><span class="co">## array([[ 0.04820395, -2.6854615 ],</span></span>
<span><span class="co">##        [ 0.23246336,  1.4535258 ]], dtype=float32)&gt;</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="doing-math-in-tensorflow">Doing math in TensorFlow<a class="anchor" aria-label="anchor" href="#doing-math-in-tensorflow"></a>
</h2>
<p>If you’ve used NumPy, doing math in TensorFlow will look very
familiar. The main difference is that your TensorFlow code can run on
GPU and TPU.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">c</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span></span>
<span><span class="va">d</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">c</span><span class="op">)</span></span>
<span><span class="va">e</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">exp</span><span class="op">(</span><span class="va">d</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="gradients">Gradients<a class="anchor" aria-label="anchor" href="#gradients"></a>
</h2>
<p>Here’s another big difference with R: you can automatically retrieve
the gradient of any differentiable expression.</p>
<p>Just open a <code>GradientTape</code>, start “watching” a tensor via
<code>tape$watch()</code>, and compose a differentiable expression using
this tensor as input:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>  <span class="va">tape</span><span class="op">$</span><span class="fu">watch</span><span class="op">(</span><span class="va">a</span><span class="op">)</span>  <span class="co"># Start recording the history of operations applied to `a`</span></span>
<span>  <span class="va">c</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">sqrt</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">a</span><span class="op">)</span> <span class="op">+</span> <span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">b</span><span class="op">)</span><span class="op">)</span>  <span class="co"># Do some math using `a`</span></span>
<span>  <span class="co"># What's the gradient of `c` with respect to `a`?</span></span>
<span>  <span class="va">dc_da</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">c</span>, <span class="va">a</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">dc_da</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ 0.9969011  -0.7707146 ]</span></span>
<span><span class="co">##  [ 0.23378514  0.96255165]], shape=(2, 2), dtype=float32)</span></span></code></pre>
<p>By default, variables are watched automatically, so you don’t need to
manually <code>watch</code> them:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">a</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">Variable</span><span class="op">(</span><span class="va">a</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>  <span class="va">c</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">sqrt</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">a</span><span class="op">)</span> <span class="op">+</span> <span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">b</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">dc_da</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">c</span>, <span class="va">a</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">dc_da</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ 0.9969011  -0.7707146 ]</span></span>
<span><span class="co">##  [ 0.23378514  0.96255165]], shape=(2, 2), dtype=float32)</span></span></code></pre>
<p>Note that you can compute higher-order derivatives by nesting
tapes:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">outer_tape</span>, <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="va">c</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">sqrt</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">a</span><span class="op">)</span> <span class="op">+</span> <span class="va">tf</span><span class="op">$</span><span class="fu">square</span><span class="op">(</span><span class="va">b</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">dc_da</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">c</span>, <span class="va">a</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="va">d2c_da2</span> <span class="op">&lt;-</span> <span class="va">outer_tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">dc_da</span>, <span class="va">a</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">d2c_da2</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[3.3447742e-03 7.1282005e-01]</span></span>
<span><span class="co">##  [5.7464113e+00 5.5013180e-02]], shape=(2, 2), dtype=float32)</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="keras-layers">Keras layers<a class="anchor" aria-label="anchor" href="#keras-layers"></a>
</h2>
<p>While TensorFlow is an <strong>infrastructure layer for
differentiable programming</strong>, dealing with tensors, variables,
and gradients, Keras is a <strong>user interface for deep
learning</strong>, dealing with layers, models, optimizers, loss
functions, metrics, and more.</p>
<p>Keras serves as the high-level API for TensorFlow: Keras is what
makes TensorFlow simple and productive.</p>
<p>The <code>Layer</code> class is the fundamental abstraction in Keras.
A <code>Layer</code> encapsulates a state (weights) and some computation
(defined in the call method).</p>
<p>A simple layer looks like this. The <code>self$add_weight()</code>
method gives you a shortcut for creating weights:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Linear</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Linear"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">units</span> <span class="op">=</span> <span class="fl">32</span>, <span class="va">input_dim</span> <span class="op">=</span> <span class="fl">32</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">w</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="va">input_dim</span>, <span class="va">units</span><span class="op">)</span>,</span>
<span>      initializer <span class="op">=</span> <span class="st">"random_normal"</span>,</span>
<span>      trainable <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">b</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="va">units</span><span class="op">)</span>,</span>
<span>      initializer <span class="op">=</span> <span class="st">"zeros"</span>,</span>
<span>      trainable <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">tf</span><span class="op">$</span><span class="fu">matmul</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">self</span><span class="op">$</span><span class="va">w</span><span class="op">)</span> <span class="op">+</span> <span class="va">self</span><span class="op">$</span><span class="va">b</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>You would use a <code>Layer</code> instance much like a R
function:</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Instantiate our layer.</span></span>
<span><span class="va">linear_layer</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units<span class="op">=</span><span class="fl">4</span>, input_dim<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The layer can be treated as a function.</span></span>
<span><span class="co"># Here we call it on some data.</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">linear_layer</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The weight variables (created in <code>initialize</code>) are
automatically tracked under the <code>weights</code> property:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linear_layer</span><span class="op">$</span><span class="va">weights</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## &lt;KerasVariable shape=(2, 4), dtype=float32, path=linear/variable&gt;</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## [[2]]</span></span>
<span><span class="co">## &lt;KerasVariable shape=(4), dtype=float32, path=linear/variable_1&gt;</span></span></code></pre>
<p>You have many built-in layers available, from <code>Dense</code> to
<code>Conv2D</code> to <code>LSTM</code> to fancier ones like
<code>Conv3DTranspose</code> or <code>ConvLSTM2D</code>. Be smart about
reusing built-in functionality.</p>
</div>
<div class="section level2">
<h2 id="layer-weight-creation-in-buildinput_shape">Layer weight creation in <code>build(input_shape)</code><a class="anchor" aria-label="anchor" href="#layer-weight-creation-in-buildinput_shape"></a>
</h2>
<p>It’s often a good idea to defer weight creation to the
<code>build()</code> method, so that you don’t need to specify the input
dim/shape at layer construction time:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Linear</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Linear"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">units</span> <span class="op">=</span> <span class="fl">32</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">units</span> <span class="op">&lt;-</span> <span class="va">units</span></span>
<span>  <span class="op">}</span>,</span>
<span>  build <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">input_shape</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">w</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="va">input_shape</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">self</span><span class="op">$</span><span class="va">units</span><span class="op">)</span>,</span>
<span>      initializer <span class="op">=</span> <span class="st">"random_normal"</span>,</span>
<span>      trainable <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">b</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">units</span><span class="op">)</span>,</span>
<span>      initializer <span class="op">=</span> <span class="st">"zeros"</span>,</span>
<span>      trainable <span class="op">=</span> <span class="cn">TRUE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">tf</span><span class="op">$</span><span class="fu">matmul</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">self</span><span class="op">$</span><span class="va">w</span><span class="op">)</span> <span class="op">+</span> <span class="va">self</span><span class="op">$</span><span class="va">b</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Instantiate our layer.</span></span>
<span><span class="va">linear_layer</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This will also call `build(input_shape)` and create the weights.</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">linear_layer</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="layer-gradients">Layer gradients<a class="anchor" aria-label="anchor" href="#layer-gradients"></a>
</h2>
<p>You can automatically retrieve the gradients of the weights of a
layer by calling it inside a <code>GradientTape</code>. Using these
gradients, you can update the weights of the layer, either manually, or
using an optimizer object. Of course, you can modify the gradients
before using them, if you need to.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Prepare a dataset.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span>, <span class="va">.</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Instantiate our linear layer (defined above) with 10 units.</span></span>
<span><span class="va">linear_layer</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Instantiate a logistic loss function that expects integer targets.</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Instantiate an optimizer.</span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_sgd.html">optimizer_sgd</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Iterate over the batches of the dataset.</span></span>
<span><span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span><span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Open a GradientTape.</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="co"># Forward pass.</span></span>
<span>    <span class="va">logits</span> <span class="op">&lt;-</span> <span class="fu">linear_layer</span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Loss value for this batch.</span></span>
<span>    <span class="va">loss_value</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">data</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span>, <span class="va">logits</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Get gradients of the loss wrt the weights.</span></span>
<span>  <span class="va">gradients</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss_value</span>, <span class="va">linear_layer</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Update the weights of our linear layer.</span></span>
<span>  <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">gradients</span>, <span class="va">linear_layer</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span><span class="va">loss_value</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(1.2819729, shape=(), dtype=float32)</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="trainable-and-non-trainable-weights">Trainable and non-trainable weights<a class="anchor" aria-label="anchor" href="#trainable-and-non-trainable-weights"></a>
</h2>
<p>Weights created by layers can be either trainable or non-trainable.
They’re exposed in <code>trainable_weights</code> and
<code>non_trainable_weights</code> respectively. Here’s a layer with a
non-trainable weight:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ComputeSum</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"ComputeSum"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">input_dim</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="co"># Create a non-trainable weight.</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">total</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      initializer <span class="op">=</span> <span class="st">"zeros"</span>,</span>
<span>      shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="va">input_dim</span><span class="op">)</span>,</span>
<span>      trainable <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">total</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">reduce_sum</span><span class="op">(</span><span class="va">inputs</span>, axis<span class="op">=</span><span class="fl">0L</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">total</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">my_sum</span> <span class="op">&lt;-</span> <span class="fu">ComputeSum</span><span class="op">(</span>input_dim <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">as.array</a></span><span class="op">(</span><span class="fu">my_sum</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 2 2</span></span></code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/array.html" class="external-link">as.array</a></span><span class="op">(</span><span class="fu">my_sum</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 4 4</span></span></code></pre>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">my_sum</span><span class="op">$</span><span class="va">trainable_weights</span></span></code></pre></div>
<pre><code><span><span class="co">## list()</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="layers-that-own-layers">Layers that own layers<a class="anchor" aria-label="anchor" href="#layers-that-own-layers"></a>
</h2>
<p>Layers can be recursively nested to create bigger computation blocks.
Each layer will track the weights of its sublayers (both trainable and
non-trainable).</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Let's reuse the Linear class</span></span>
<span><span class="co"># with a `build` method that we defined above.</span></span>
<span></span>
<span><span class="va">MLP</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"MLP"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_1</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_2</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_3</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">linear_1</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">linear_2</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="fu">linear_3</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu">MLP</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The first call to the `mlp` object will create the weights.</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span>shape<span class="op">=</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">64</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Weights are recursively tracked.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">mlp</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 6</span></span></code></pre>
<p>Note that our manually-created MLP above is equivalent to the
following built-in option:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tracking-losses-created-by-layers">Tracking losses created by layers<a class="anchor" aria-label="anchor" href="#tracking-losses-created-by-layers"></a>
</h2>
<p>Layers can create losses during the forward pass via the
<code>add_loss()</code> method. This is especially useful for
regularization losses. The losses created by sublayers are recursively
tracked by the parent layers.</p>
<p>Here’s a layer that creates an activity regularization loss:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># A layer that creates an activity sparsity regularization loss</span></span>
<span><span class="va">ActivityRegularization</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"ActivityRegularization"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">rate</span><span class="op">=</span><span class="fl">1e-2</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">rate</span> <span class="op">&lt;-</span> <span class="va">rate</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">add_loss</span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">rate</span> <span class="op">*</span> <span class="va">tf</span><span class="op">$</span><span class="fu">reduce_sum</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">abs</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">inputs</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Any model incorporating this layer will track this regularization
loss:</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Let's use the loss layer in a MLP block.</span></span>
<span><span class="va">SparseMLP</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"SparseMLP"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_1</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">reg</span> <span class="op">&lt;-</span> <span class="fu">ActivityRegularization</span><span class="op">(</span>rate <span class="op">=</span> <span class="fl">1e-2</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_3</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">linear_1</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">reg</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="fu">linear_3</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu">SparseMLP</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">mlp</span><span class="op">$</span><span class="va">losses</span>  <span class="co"># List containing one float32 scalar</span></span></code></pre></div>
<pre><code><span><span class="co">## [[1]]</span></span>
<span><span class="co">## tf.Tensor(0.18065463, shape=(), dtype=float32)</span></span></code></pre>
<p>These losses are cleared by the top-level layer at the start of each
forward pass – they don’t accumulate. <code>layer.losses</code> always
contains only the losses created during the last forward pass. You would
typically use these losses by summing them before computing your
gradients when writing a training loop.</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Losses correspond to the *last* forward pass.</span></span>
<span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu">SparseMLP</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]], shape=(10, 10), dtype=float32)</span></span></code></pre>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">mlp</span><span class="op">$</span><span class="va">losses</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1</span></span></code></pre>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## tf.Tensor(</span></span>
<span><span class="co">## [[ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]</span></span>
<span><span class="co">##  [ 0.0388482  -0.03920118  0.01624808 -0.01361975 -0.01354899  0.07107338</span></span>
<span><span class="co">##   -0.01077365  0.05688906 -0.02838149 -0.04084621]], shape=(10, 10), dtype=float32)</span></span></code></pre>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">mlp</span><span class="op">$</span><span class="va">losses</span><span class="op">)</span>  <span class="co"># No accumulation.</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 1</span></span></code></pre>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Let's demonstrate how to use these losses in a training loop.</span></span>
<span></span>
<span><span class="co"># Prepare a dataset.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span>, <span class="va">.</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A new MLP.</span></span>
<span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu">SparseMLP</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Loss and optimizer.</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_sgd.html">optimizer_sgd</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span><span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="co"># Forward pass.</span></span>
<span>    <span class="va">logits</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># External loss value for this batch.</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y</span>, <span class="va">logits</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Add the losses created during the forward pass.</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="va">loss</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/funprog.html" class="external-link">Reduce</a></span><span class="op">(</span><span class="va">`+`</span>, <span class="va">mlp</span><span class="op">$</span><span class="va">losses</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Get gradients of the loss wrt the weights.</span></span>
<span>    <span class="va">gradients</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss</span>, <span class="va">mlp</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Update the weights of our linear layer.</span></span>
<span>    <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">gradients</span>, <span class="va">mlp</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="keeping-track-of-training-metrics">Keeping track of training metrics<a class="anchor" aria-label="anchor" href="#keeping-track-of-training-metrics"></a>
</h2>
<p>Keras offers a broad range of built-in metrics, like
<code>metric_auc</code> or <code>metric_precision_at_recall</code>. It’s
also easy to create your own metrics in a few lines of code.</p>
<p>To use a metric in a custom training loop, you would:</p>
<ul>
<li>Instantiate the metric object,
e.g. <code>metric = metric_auc()</code>
</li>
<li>Call its <code>metric$udpate_state(targets, predictions)</code>
method for each batch of data</li>
<li>Query its result via <code>metric$result()</code>
</li>
<li>Reset the metric’s state at the end of an epoch or at the start of
an evaluation via <code>metric$reset_state()</code>
</li>
</ul>
<p>Here’s a simple example:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Instantiate a metric object</span></span>
<span><span class="va">accuracy</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/metric_sparse_categorical_accuracy.html">metric_sparse_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare our layer, loss, and optimizer.</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">epoch</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span> <span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>      <span class="co"># Forward pass.</span></span>
<span>      <span class="va">logits</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span>      <span class="co"># External loss value for this batch.</span></span>
<span>      <span class="va">loss_value</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y</span>, <span class="va">logits</span><span class="op">)</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Update the state of the `accuracy` metric.</span></span>
<span>    <span class="va">accuracy</span><span class="op">$</span><span class="fu">update_state</span><span class="op">(</span><span class="va">y</span>, <span class="va">logits</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Update the weights of the model to minimize the loss value.</span></span>
<span>    <span class="va">gradients</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss_value</span>, <span class="va">model</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span>    <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">gradients</span>, <span class="va">model</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Epoch:"</span>, <span class="va">epoch</span>, <span class="st">"Accuracy:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">accuracy</span><span class="op">$</span><span class="fu">result</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="va">accuracy</span><span class="op">$</span><span class="fu">reset_state</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch: 1 Accuracy: 0.8757833</span></span>
<span><span class="co">## Epoch: 2 Accuracy: 0.93915</span></span></code></pre>
<p>You can also define your own metrics by subclassing
<code>keras.metrics.Metric</code>. You need to override the three
functions called above:</p>
<ul>
<li>Override <code>update_state()</code> to update the statistic
values.</li>
<li>Override <code>result()</code> to return the metric value.</li>
<li>Override <code><a href="../reference/reset_state.html">reset_state()</a></code> to reset the metric to its
initial state.</li>
</ul>
<p>Here is an example where we implement the F1-score metric (with
support for sample weighting).</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">F1Score</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_metric_class.html">new_metric_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"F1Score"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">self</span>, <span class="va">name</span><span class="op">=</span><span class="st">"f1_score"</span>, <span class="va">dtype</span><span class="op">=</span><span class="st">"float32"</span>, <span class="va">threshold</span><span class="op">=</span><span class="fl">0.5</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span>name<span class="op">=</span><span class="va">name</span>, dtype<span class="op">=</span><span class="va">dtype</span>, <span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">threshold</span> <span class="op">&lt;-</span> <span class="va">threshold</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      name<span class="op">=</span><span class="st">"tp"</span>, dtype<span class="op">=</span><span class="va">dtype</span>, initializer<span class="op">=</span><span class="st">"zeros"</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_positives</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      name<span class="op">=</span><span class="st">"fp"</span>, dtype<span class="op">=</span><span class="va">dtype</span>, initializer<span class="op">=</span><span class="st">"zeros"</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_negatives</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">add_weight</span><span class="op">(</span></span>
<span>      name<span class="op">=</span><span class="st">"fn"</span>, dtype<span class="op">=</span><span class="va">dtype</span>, initializer<span class="op">=</span><span class="st">"zeros"</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  update_state <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">y_true</span>, <span class="va">y_pred</span>, <span class="va">sample_weight</span><span class="op">=</span><span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="va">math</span><span class="op">$</span><span class="fu">greater_equal</span><span class="op">(</span><span class="va">y_pred</span>, <span class="va">self</span><span class="op">$</span><span class="va">threshold</span><span class="op">)</span></span>
<span>    <span class="va">y_true</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="va">y_true</span>, <span class="va">tf</span><span class="op">$</span><span class="va">bool</span><span class="op">)</span></span>
<span>    <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="va">y_pred</span>, <span class="va">tf</span><span class="op">$</span><span class="va">bool</span><span class="op">)</span></span>
<span></span>
<span>    <span class="va">true_positives</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="va">y_true</span> <span class="op">&amp;</span> <span class="va">y_pred</span>, <span class="va">self</span><span class="op">$</span><span class="va">dtype</span><span class="op">)</span></span>
<span>    <span class="va">false_positives</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="op">(</span><span class="op">!</span><span class="va">y_true</span><span class="op">)</span> <span class="op">&amp;</span> <span class="va">y_pred</span>, <span class="va">self</span><span class="op">$</span><span class="va">dtype</span><span class="op">)</span></span>
<span>    <span class="va">false_negatives</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="va">y_true</span> <span class="op">&amp;</span> <span class="op">(</span><span class="op">!</span><span class="va">y_pred</span><span class="op">)</span>, <span class="va">self</span><span class="op">$</span><span class="va">dtype</span><span class="op">)</span></span>
<span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">sample_weight</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">sample_weight</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">cast</span><span class="op">(</span><span class="va">sample_weight</span>, <span class="va">self</span><span class="op">$</span><span class="va">dtype</span><span class="op">)</span></span>
<span>      <span class="va">true_positives</span> <span class="op">&lt;-</span> <span class="va">true_positives</span> <span class="op">*</span> <span class="va">sample_weight</span></span>
<span>      <span class="va">false_positives</span> <span class="op">&lt;-</span> <span class="va">false_positives</span> <span class="op">*</span> <span class="va">sample_weight</span></span>
<span>      <span class="va">false_negatives</span> <span class="op">&lt;-</span> <span class="va">false_negatives</span> <span class="op">*</span> <span class="va">sample_weight</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">reduce_sum</span><span class="op">(</span><span class="va">true_positives</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_positives</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">reduce_sum</span><span class="op">(</span><span class="va">false_positives</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_negatives</span><span class="op">$</span><span class="fu">assign_add</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">reduce_sum</span><span class="op">(</span><span class="va">false_negatives</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span></span>
<span>  result <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">precision</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">/</span> <span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">+</span> <span class="va">self</span><span class="op">$</span><span class="va">false_positives</span><span class="op">)</span></span>
<span>    <span class="va">recall</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">/</span> <span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">true_positives</span> <span class="op">+</span> <span class="va">self</span><span class="op">$</span><span class="va">false_negatives</span><span class="op">)</span></span>
<span>    <span class="va">f1_score</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">precision</span> <span class="op">*</span> <span class="va">recall</span> <span class="op">/</span> <span class="op">(</span><span class="va">precision</span> <span class="op">+</span> <span class="va">recall</span><span class="op">)</span></span>
<span>    <span class="va">f1_score</span></span>
<span>  <span class="op">}</span>,</span>
<span></span>
<span>  reset_state <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">true_positives</span><span class="op">$</span><span class="fu">assign</span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_positives</span><span class="op">$</span><span class="fu">assign</span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">false_negatives</span><span class="op">$</span><span class="fu">assign</span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Let’s test-drive it:</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu">F1Score</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">m</span><span class="op">$</span><span class="fu">update_state</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">0</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Intermediate result:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">m</span><span class="op">$</span><span class="fu">result</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Intermediate result: 0.5</span></span></code></pre>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m</span><span class="op">$</span><span class="fu">update_state</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.7</span>, <span class="fl">0.6</span>, <span class="fl">0.0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Final result:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">m</span><span class="op">$</span><span class="fu">result</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Final result: 0.6</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="compiled-functions">Compiled functions<a class="anchor" aria-label="anchor" href="#compiled-functions"></a>
</h2>
<p>Running eagerly is great for debugging, but you will get better
performance by compiling your computation into static graphs. Static
graphs are a researcher’s best friends. You can compile any function by
wrapping it in a <code>tf.function</code> decorator.</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Prepare our layer, loss, and optimizer.</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a training step function.</span></span>
<span><span class="va">train_on_batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/tf_function.html" class="external-link">tf_function</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="co"># Forward pass.</span></span>
<span>    <span class="va">logits</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="co"># External loss value for this batch.</span></span>
<span>    <span class="va">loss_value</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y</span>, <span class="va">logits</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="co"># Update the weights of the model to minimize the loss value.</span></span>
<span>  <span class="va">gradients</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss_value</span>, <span class="va">model</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span>  <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">gradients</span>, <span class="va">model</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">loss_value</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Prepare a dataset.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span>, <span class="va">.</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="va">i</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span><span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span> <span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">i</span> <span class="op">&lt;-</span> <span class="va">i</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/train_on_batch.html">train_on_batch</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">i</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">100</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Loss:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Loss: 0.551749</span></span>
<span><span class="co">## Loss: 0.2131135</span></span>
<span><span class="co">## Loss: 0.2765952</span></span>
<span><span class="co">## Loss: 0.1296219</span></span>
<span><span class="co">## Loss: 0.2657076</span></span>
<span><span class="co">## Loss: 0.2683381</span></span>
<span><span class="co">## Loss: 0.1570166</span></span>
<span><span class="co">## Loss: 0.3139241</span></span>
<span><span class="co">## Loss: 0.08981849</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="training-mode-inference-mode">Training mode &amp; inference mode<a class="anchor" aria-label="anchor" href="#training-mode-inference-mode"></a>
</h2>
<p>Some layers, in particular the <code>BatchNormalization</code> layer
and the <code>Dropout</code> layer, have different behaviors during
training and inference. For such layers, it is standard practice to
expose a <code>training</code> (boolean) argument in the
<code>call</code> method.</p>
<p>By exposing this argument in <code>call</code>, you enable the
built-in training and evaluation loops (e.g. fit) to correctly use the
layer in training and inference modes.</p>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Dropout</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Dropout"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">rate</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">rate</span> <span class="op">&lt;-</span> <span class="va">rate</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">training</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html" class="external-link">is.null</a></span><span class="op">(</span><span class="va">training</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="va">training</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">dropout</span><span class="op">(</span><span class="va">inputs</span>, rate <span class="op">=</span> <span class="va">self</span><span class="op">$</span><span class="va">rate</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="va">inputs</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">MLPWithDropout</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"MLPWithDropout"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_1</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dropout</span> <span class="op">&lt;-</span> <span class="fu">Dropout</span><span class="op">(</span>rate <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">linear_3</span> <span class="op">&lt;-</span> <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">training</span> <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">linear_1</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="va">nn</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">dropout</span><span class="op">(</span><span class="va">x</span>, training <span class="op">=</span> <span class="va">training</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">linear_3</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">mlp</span> <span class="op">&lt;-</span> <span class="fu">MLPWithDropout</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">y_train</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>, training<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">y_test</span> <span class="op">&lt;-</span> <span class="fu">mlp</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>, training<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-functional-api-for-model-building">The Functional API for model-building<a class="anchor" aria-label="anchor" href="#the-functional-api-for-model-building"></a>
</h2>
<p>To build deep learning models, you don’t have to use object-oriented
programming all the time. All layers we’ve seen so far can also be
composed functionally, like this (we call it the “Functional API”):</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We use an `Input` object to describe the shape and dtype of the inputs.</span></span>
<span><span class="co"># This is the deep learning equivalent of *declaring a type*.</span></span>
<span><span class="co"># The shape argument is per-sample; it does not include the batch size.</span></span>
<span><span class="co"># The functional API focused on defining per-sample transformations.</span></span>
<span><span class="co"># The model we create will automatically batch the per-sample transformations,</span></span>
<span><span class="co"># so that it can be called on batches of data.</span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_input.html">layer_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">16</span>, dtype <span class="op">=</span> <span class="st">"float32"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># We call layers on these "type" objects</span></span>
<span><span class="co"># and they return updated types (new shapes/dtypes).</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="co"># We are reusing the Linear layer we defined earlier.</span></span>
<span>  <span class="fu">Dropout</span><span class="op">(</span>rate <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="co"># We are reusing the Dropout layer we defined earlier.</span></span>
<span>  <span class="fu">Linear</span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A functional `Model` can be defined by specifying inputs and outputs.</span></span>
<span><span class="co"># A model is itself a layer like any other.</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A functional model already has weights, before being called on any data.</span></span>
<span><span class="co"># That's because we defined its input shape in advance (in `Input`).</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">model</span><span class="op">$</span><span class="va">weights</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] 4</span></span></code></pre>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Let's call our model on some data, for fun.</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">16</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">y</span><span class="op">$</span><span class="va">shape</span></span></code></pre></div>
<pre><code><span><span class="co">## TensorShape([2, 10])</span></span></code></pre>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># You can pass a `training` argument in `__call__`</span></span>
<span><span class="co"># (it will get passed down to the Dropout layer).</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">ones</span><span class="op">(</span><span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">16</span><span class="op">)</span><span class="op">)</span>, training<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>The Functional API tends to be more concise than subclassing, and
provides a few other advantages (generally the same advantages that
functional, typed languages provide over untyped OO development).
However, it can only be used to define DAGs of layers – recursive
networks should be defined as Layer subclasses instead.</p>
<p>Learn more about the Functional API <a href="functional_api.html">here</a>.</p>
<p>In your research workflows, you may often find yourself
mix-and-matching OO models and Functional models.</p>
<p>Note that the <code>Model</code> class also features built-in
training &amp; evaluation loops: <code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code>,
<code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code> and <code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code> (configured via the
<code><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile()</a></code> method). These built-in functions give you access
to the following built-in training infrastructure features:</p>
<ul>
<li>
<a href="https://keras.posit.co/reference/index.html#callbacks" class="external-link">Callbacks</a>.
You can leverage built-in callbacks for early-stopping, model
checkpointing, and monitoring training with TensorBoard. You can also <a href="writing_your_own_callbacks.html">implement custom callbacks</a> if
needed.</li>
<li>
<a href="distributed_training_with_tensorflow.html">Distributed
training</a>. You can easily scale up your training to multiple GPUs,
TPU, or even multiple machines with the <code>tf.distribute</code> API –
with no changes to your code.</li>
<li>
<a href="https://keras.posit.co/reference/compile.keras.src.models.model.Model.html" class="external-link">Step
fusing</a>. With the <code>steps_per_execution</code> argument in
<code>Model.compile()</code>, you can process multiple batches in a
single <code>tf.function</code> call, which greatly improves device
utilization on TPUs.</li>
</ul>
<p>We won’t go into the details, but we provide a simple code example
below. It leverages the built-in training infrastructure to implement
the MNIST example above.</p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_input.html">layer_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">784</span>, dtype<span class="op">=</span><span class="st">"float32"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Specify the loss, optimizer, and metrics with `compile()`.</span></span>
<span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span></span>
<span>    loss <span class="op">=</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>    optimizer<span class="op">=</span><span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span>,</span>
<span>    metrics<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="../reference/metric_sparse_categorical_accuracy.html">metric_sparse_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>,</span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Train the model with the dataset for 2 epochs.</span></span>
<span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">dataset</span>, epochs<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/2</span></span>
<span><span class="co">## 938/938 - 4s - 4ms/step - loss: 0.3958 - sparse_categorical_accuracy: 0.8866</span></span>
<span><span class="co">## Epoch 2/2</span></span>
<span><span class="co">## 938/938 - 1s - 960us/step - loss: 0.1888 - sparse_categorical_accuracy: 0.9443</span></span></code></pre>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">predictions</span> <span class="op">&lt;-</span> <span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 938/938 - 1s - 1ms/step</span></span></code></pre>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 938/938 - 1s - 1ms/step - loss: 0.1763 - sparse_categorical_accuracy: 0.9454</span></span></code></pre>
<pre><code><span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.1763445</span></span>
<span><span class="co">##</span></span>
<span><span class="co">## $sparse_categorical_accuracy</span></span>
<span><span class="co">## [1] 0.9454167</span></span></code></pre>
<p>You can always subclass the <code>Model</code> class (it works
exactly like subclassing <code>Layer</code>) if you want to leverage
built-in training loops for your OO models. Just override the
<code>Model$train_step()</code> to customize what happens in
<code><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit()</a></code> while retaining support for the built-in
infrastructure features outlined above – callbacks, zero-code
distribution support, and step fusing support. You may also override
<code>test_step()</code> to customize what happens in
<code><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate()</a></code>, and override <code>predict_step()</code> to
customize what happens in <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>. For more information,
please refer to <a href="custom_train_step_in_tensorflow.html">this
guide</a>.</p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">CustomModel</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_model_class.html">new_model_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"CustomModel"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">loss_tracker</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/metric_mean.html">metric_mean</a></span><span class="op">(</span>name<span class="op">=</span><span class="st">"loss"</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">accuracy</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/metric_sparse_categorical_accuracy.html">metric_sparse_categorical_accuracy</a></span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  train_step <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x</span>, y <span class="op">=</span> <span class="cn">NULL</span>, sample_weight <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">data</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>      <span class="va">y_pred</span> <span class="op">&lt;-</span> <span class="fu">self</span><span class="op">(</span><span class="va">x</span>, training<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span>      <span class="va">loss</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">loss_fn</span><span class="op">(</span>y <span class="op">=</span> <span class="va">y</span>, y_pred <span class="op">=</span> <span class="va">y_pred</span>, sample_weight<span class="op">=</span><span class="va">sample_weight</span><span class="op">)</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span>    <span class="va">gradients</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss</span>, <span class="va">self</span><span class="op">$</span><span class="va">trainable_variables</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span></span>
<span>      <span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">gradients</span>, <span class="va">self</span><span class="op">$</span><span class="va">trainable_variables</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Update metrics (includes the metric that tracks the loss)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">loss_tracker</span><span class="op">$</span><span class="fu">update_state</span><span class="op">(</span><span class="va">loss</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">accuracy</span><span class="op">$</span><span class="fu">update_state</span><span class="op">(</span><span class="va">y</span>, <span class="va">y_pred</span>, sample_weight<span class="op">=</span><span class="va">sample_weight</span><span class="op">)</span></span>
<span>    <span class="co"># Return a list mapping metric names to current value</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>      loss <span class="op">=</span> <span class="va">self</span><span class="op">$</span><span class="va">loss_tracker</span><span class="op">$</span><span class="fu">result</span><span class="op">(</span><span class="op">)</span>,</span>
<span>      accuracy <span class="op">=</span> <span class="va">self</span><span class="op">$</span><span class="va">accuracy</span><span class="op">$</span><span class="fu">result</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  metrics <span class="op">=</span> <span class="fu"><a href="../reference/mark_active.html">mark_active</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">loss_tracker</span>, <span class="va">self</span><span class="op">$</span><span class="va">accuracy</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_input.html">layer_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fl">784</span>, dtype<span class="op">=</span><span class="st">"float32"</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">32</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu">CustomModel</span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">dataset</span>, epochs<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/2</span></span>
<span><span class="co">## 938/938 - 2s - 2ms/step - loss: 0.3869 - sparse_categorical_accuracy: 0.8924</span></span>
<span><span class="co">## Epoch 2/2</span></span>
<span><span class="co">## 938/938 - 1s - 1ms/step - loss: 0.2163 - sparse_categorical_accuracy: 0.9370</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="end-to-end-experiment-example-1-variational-autoencoders-">End-to-end experiment example 1: variational autoencoders.<a class="anchor" aria-label="anchor" href="#end-to-end-experiment-example-1-variational-autoencoders-"></a>
</h2>
<p>Here are some of the things you’ve learned so far:</p>
<ul>
<li>A <code>Layer</code> encapsulates a state (created in
<code>__init__</code> or <code>build</code>) and some computation
(defined in <code>call</code>).</li>
<li>Layers can be recursively nested to create new, bigger computation
blocks.</li>
<li>You can easily write highly hackable training loops by opening a
<code>GradientTape</code>, calling your model inside the tape’s scope,
then retrieving gradients and applying them via an optimizer.</li>
<li>You can speed up your training loops using the
<code>@tf.function</code> decorator.</li>
<li>Layers can create and track losses (typically regularization losses)
via <code>self.add_loss()</code>.</li>
</ul>
<p>Let’s put all of these things together into an end-to-end example:
we’re going to implement a Variational AutoEncoder (VAE). We’ll train it
on MNIST digits.</p>
<p>Our VAE will be a subclass of <code>Layer</code>, built as a nested
composition of layers that subclass <code>Layer</code>. It will feature
a regularization loss (KL divergence).</p>
<p>Below is our model definition.</p>
<p>First, we have an <code>Encoder</code> class, which uses a
<code>Sampling</code> layer to map a MNIST digit to a latent-space
triplet <code>(z_mean, z_log_var, z)</code>.</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Sampling</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Sampling"</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">z_mean</span>, <span class="va">z_log_var</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">inputs</span></span>
<span>    <span class="va">batch</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_shape.html">op_shape</a></span><span class="op">(</span><span class="va">z_mean</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">dim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/op_shape.html">op_shape</a></span><span class="op">(</span><span class="va">z_mean</span><span class="op">)</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">epsilon</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">batch</span>, <span class="va">dim</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">z_mean</span> <span class="op">+</span> <span class="fu"><a href="../reference/op_exp.html">op_exp</a></span><span class="op">(</span><span class="fl">0.5</span> <span class="op">*</span> <span class="va">z_log_var</span><span class="op">)</span> <span class="op">*</span> <span class="va">epsilon</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">Encoder</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Encoder"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">latent_dim</span> <span class="op">=</span> <span class="fl">32</span>, <span class="va">intermediate_dim</span> <span class="op">=</span> <span class="fl">64</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dense_proj</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">intermediate_dim</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dense_mean</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">latent_dim</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dense_log_var</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">latent_dim</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">sampling</span> <span class="op">&lt;-</span> <span class="fu">Sampling</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">dense_proj</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">z_mean</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">dense_mean</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">z_log_var</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">dense_log_var</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">z</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">sampling</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">z_mean</span>, <span class="va">z_log_var</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">z_mean</span>, <span class="va">z_log_var</span>, <span class="va">z</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Next, we have a <code>Decoder</code> class, which maps the
probabilistic latent space coordinates back to a MNIST digit.</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Decoder</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_layer_class.html">new_layer_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"Decoder"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">original_dim</span>, <span class="va">intermediate_dim</span> <span class="op">=</span> <span class="fl">64</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span><span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dense_proj</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">intermediate_dim</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">dense_output</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">original_dim</span>, activation <span class="op">=</span> <span class="st">"sigmoid"</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">dense_proj</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">dense_output</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Finally, our <code>VariationalAutoEncoder</code> composes together an
encoder and a decoder, and creates a KL divergence regularization loss
via <code>add_loss()</code>.</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">VariationalAutoEncoder</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/new_model_class.html">new_model_class</a></span><span class="op">(</span></span>
<span>  <span class="st">"VariationalAutoEncoder"</span>,</span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">original_dim</span>,</span>
<span>        <span class="va">intermediate_dim</span><span class="op">=</span><span class="fl">64</span>,</span>
<span>        <span class="va">latent_dim</span><span class="op">=</span><span class="fl">32</span>,</span>
<span>        <span class="va">name</span><span class="op">=</span><span class="st">"autoencoder"</span>, <span class="va">...</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">super</span><span class="op">$</span><span class="fu">initialize</span><span class="op">(</span>name <span class="op">=</span> <span class="va">name</span>, <span class="va">...</span><span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">original_dim</span> <span class="op">&lt;-</span> <span class="va">original_dim</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">encoder</span> <span class="op">&lt;-</span> <span class="fu">Encoder</span><span class="op">(</span></span>
<span>      latent_dim <span class="op">=</span> <span class="va">latent_dim</span>,</span>
<span>      intermediate_dim <span class="op">=</span> <span class="va">intermediate_dim</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">decoder</span> <span class="op">&lt;-</span> <span class="fu">Decoder</span><span class="op">(</span></span>
<span>      original_dim <span class="op">=</span> <span class="va">original_dim</span>,</span>
<span>      intermediate_dim <span class="op">=</span> <span class="va">intermediate_dim</span></span>
<span>    <span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  call <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">z_mean</span>, <span class="va">z_log_var</span>, <span class="va">z</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="va">self</span><span class="op">$</span><span class="fu">encoder</span><span class="op">(</span><span class="va">inputs</span><span class="op">)</span></span>
<span>    <span class="va">reconstructed</span> <span class="op">&lt;-</span> <span class="va">self</span><span class="op">$</span><span class="fu">decoder</span><span class="op">(</span><span class="va">z</span><span class="op">)</span></span>
<span>    <span class="co"># Add KL divergence regularization loss.</span></span>
<span>    <span class="va">kl_loss</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="../reference/op_mean.html">op_mean</a></span><span class="op">(</span></span>
<span>      <span class="va">z_log_var</span> <span class="op">-</span> <span class="fu"><a href="../reference/op_square.html">op_square</a></span><span class="op">(</span><span class="va">z_mean</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="../reference/op_exp.html">op_exp</a></span><span class="op">(</span><span class="va">z_log_var</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span></span>
<span>    <span class="op">)</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="fu">add_loss</span><span class="op">(</span><span class="va">kl_loss</span><span class="op">)</span></span>
<span>    <span class="va">reconstructed</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Now, let’s write a training loop. Our training step is decorated with
a <code>@tf.function</code> to compile into a super fast graph
function.</p>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Our model.</span></span>
<span><span class="va">vae</span> <span class="op">&lt;-</span> <span class="fu">VariationalAutoEncoder</span><span class="op">(</span></span>
<span>  original_dim <span class="op">=</span> <span class="fl">784</span>,</span>
<span>  intermediate_dim <span class="op">=</span> <span class="fl">64</span>,</span>
<span>  latent_dim <span class="op">=</span> <span class="fl">32</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Loss and optimizer.</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_mean_squared_error.html">loss_mean_squared_error</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">=</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare a dataset.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">.</span><span class="op">)</span>, <span class="va">.</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="va">x_train</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">32</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">training_step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/tf_function.html" class="external-link">tf_function</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="va">reconstructed</span> <span class="op">&lt;-</span> <span class="fu">vae</span><span class="op">(</span><span class="va">x</span><span class="op">)</span>  <span class="co"># Compute input reconstruction.</span></span>
<span>    <span class="co"># Compute loss.</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">x</span>, <span class="va">reconstructed</span><span class="op">)</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="va">loss</span> <span class="op">+</span> <span class="fu"><a href="../reference/op_sum.html">op_sum</a></span><span class="op">(</span><span class="va">vae</span><span class="op">$</span><span class="va">losses</span><span class="op">)</span>  <span class="co"># Add KLD term.</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="co"># Update the weights of the VAE.</span></span>
<span>  <span class="va">grads</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss</span>, <span class="va">vae</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span>  <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">grads</span>, <span class="va">vae</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">loss</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="va">losses</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">)</span>  <span class="co"># Keep track of the losses over time.</span></span>
<span><span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span><span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu">training_step</span><span class="op">(</span><span class="va">data</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Logging.</span></span>
<span>  <span class="va">losses</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">100</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Step:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span>, <span class="st">"Loss:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="co"># Stop after 1000 steps.</span></span>
<span>  <span class="co"># Training the model to convergence is left</span></span>
<span>  <span class="co"># as an exercise to the reader.</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op">&gt;=</span> <span class="fl">1000</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">break</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Step: 100 Loss: 0.1270978</span></span>
<span><span class="co">## Step: 200 Loss: 0.1003238</span></span>
<span><span class="co">## Step: 300 Loss: 0.09001128</span></span>
<span><span class="co">## Step: 400 Loss: 0.08493649</span></span>
<span><span class="co">## Step: 500 Loss: 0.08171404</span></span>
<span><span class="co">## Step: 600 Loss: 0.07926706</span></span>
<span><span class="co">## Step: 700 Loss: 0.07790599</span></span>
<span><span class="co">## Step: 800 Loss: 0.07670419</span></span>
<span><span class="co">## Step: 900 Loss: 0.07570736</span></span>
<span><span class="co">## Step: 1000 Loss: 0.07476593</span></span></code></pre>
<p>As you can see, building and training this type of model in Keras is
quick and painless.</p>
</div>
<div class="section level2">
<h2 id="end-to-end-experiment-example-2-hypernetworks-">End-to-end experiment example 2: hypernetworks.<a class="anchor" aria-label="anchor" href="#end-to-end-experiment-example-2-hypernetworks-"></a>
</h2>
<p>Let’s take a look at another kind of research experiment:
hypernetworks.</p>
<p>The idea is to use a small deep neural network (the hypernetwork) to
generate the weights for a larger network (the main network).</p>
<p>Let’s implement a really trivial hypernetwork: we’ll use a small
2-layer network to generate the weights of a larger 3-layer network.</p>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">input_dim</span> <span class="op">&lt;-</span> <span class="fl">784</span></span>
<span><span class="va">classes</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span></span>
<span><span class="co"># This is the main network we'll actually use to predict labels.</span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_input.html">layer_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="va">input_dim</span><span class="op">)</span></span>
<span><span class="va">dense1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">64</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span></span>
<span><span class="va">dense1</span><span class="op">$</span><span class="va">built</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span></span>
<span><span class="va">dense2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="va">classes</span><span class="op">)</span></span>
<span><span class="va">dense2</span><span class="op">$</span><span class="va">built</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu">dense1</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span> <span class="fu">dense2</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">main_network</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">outputs</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This is the number of weight coefficients to generate. Each layer in the</span></span>
<span><span class="co"># main network requires output_dim * input_dim + output_dim coefficients.</span></span>
<span><span class="va">num_weights_to_generate</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="va">classes</span> <span class="op">*</span> <span class="fl">64</span> <span class="op">+</span> <span class="va">classes</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">64</span> <span class="op">*</span> <span class="va">input_dim</span> <span class="op">+</span> <span class="fl">64</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This is the hypernetwork that generates the weights of the `main_network` above.</span></span>
<span><span class="va">hypernetwork</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model_sequential.html">keras_model_sequential</a></span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units<span class="op">=</span><span class="fl">16</span>, activation<span class="op">=</span><span class="st">"relu"</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units<span class="op">=</span><span class="va">num_weights_to_generate</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span><span class="op">)</span></span></code></pre></div>
<p>This is our training loop. For each batch of data:</p>
<ul>
<li>We use <code>hypernetwork</code> to generate an array of weight
coefficients, <code>weights_pred</code>
</li>
<li>We reshape these coefficients into kernel &amp; bias tensors for the
<code>main_network</code>
</li>
<li>We run the forward pass of the <code>main_network</code> to compute
the actual MNIST predictions</li>
<li>We run backprop through the weights of the <code>hypernetwork</code>
to minimize the final classification loss</li>
</ul>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Loss and optimizer.</span></span>
<span><span class="va">loss_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/loss_sparse_categorical_crossentropy.html">loss_sparse_categorical_crossentropy</a></span><span class="op">(</span>from_logits <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/optimizer_adam.html">optimizer_adam</a></span><span class="op">(</span>learning_rate<span class="op">=</span><span class="fl">1e-4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Prepare a dataset.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span>, <span class="va">.</span><span class="op">)</span> <span class="op"><a href="../reference/multi-assign.html">%&lt;-%</a></span> <span class="fu"><a href="../reference/dataset_mnist.html">dataset_mnist</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">x_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/array_reshape.html" class="external-link">array_reshape</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">60000</span>, <span class="fl">784</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fl">255</span></span>
<span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x_train</span>, <span class="va">y_train</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_shuffle.html" class="external-link">dataset_shuffle</a></span><span class="op">(</span>buffer_size<span class="op">=</span><span class="fl">1024</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="co"># We'll use a batch size of 1 for this experiment.</span></span>
<span>  <span class="fu">tfdatasets</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">train_step</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">tf</span><span class="op">$</span><span class="fu">GradientTape</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rstudio.github.io/reticulate/reference/with-as-operator.html" class="external-link">%as%</a></span> <span class="va">tape</span>, <span class="op">{</span></span>
<span>    <span class="va">weights_pred</span> <span class="op">&lt;-</span> <span class="fu">hypernetwork</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Reshape them to the expected shapes for w and b for the outer model.</span></span>
<span>    <span class="co"># Layer 1 kernel.</span></span>
<span>    <span class="va">start_index</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span>    <span class="va">w1_shape</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">input_dim</span>, <span class="fl">64</span><span class="op">)</span></span>
<span>    <span class="va">w1_coeffs</span> <span class="op">&lt;-</span> <span class="va">weights_pred</span><span class="op">[</span>, <span class="va">start_index</span><span class="op">:</span><span class="op">(</span><span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">w1_shape</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">w1</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">reshape</span><span class="op">(</span><span class="va">w1_coeffs</span>, <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">w1_shape</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">start_index</span> <span class="op">&lt;-</span> <span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">w1_shape</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Layer 1 bias.</span></span>
<span>    <span class="va">b1_shape</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">64</span><span class="op">)</span></span>
<span>    <span class="va">b1_coeffs</span> <span class="op">&lt;-</span> <span class="va">weights_pred</span><span class="op">[</span>, <span class="va">start_index</span><span class="op">:</span><span class="op">(</span><span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">b1_shape</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">b1</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">reshape</span><span class="op">(</span><span class="va">b1_coeffs</span>, <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">b1_shape</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">start_index</span> <span class="op">&lt;-</span> <span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">b1_shape</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Layer 2 kernel.</span></span>
<span>    <span class="va">w2_shape</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">64</span>, <span class="va">classes</span><span class="op">)</span></span>
<span>    <span class="va">w2_coeffs</span> <span class="op">&lt;-</span> <span class="va">weights_pred</span><span class="op">[</span>, <span class="va">start_index</span><span class="op">:</span><span class="op">(</span><span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">w2_shape</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">w2</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">reshape</span><span class="op">(</span><span class="va">w2_coeffs</span>, <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">w2_shape</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">start_index</span> <span class="op">&lt;-</span> <span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">w2_shape</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Layer 2 bias.</span></span>
<span>    <span class="va">b2_shape</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">classes</span><span class="op">)</span></span>
<span>    <span class="va">b2_coeffs</span> <span class="op">&lt;-</span> <span class="va">weights_pred</span><span class="op">[</span>, <span class="va">start_index</span><span class="op">:</span><span class="op">(</span><span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">b2_shape</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">b2</span> <span class="op">&lt;-</span> <span class="va">tf</span><span class="op">$</span><span class="fu">reshape</span><span class="op">(</span><span class="va">b2_coeffs</span>, <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">b2_shape</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="va">start_index</span> <span class="op">&lt;-</span> <span class="va">start_index</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/prod.html" class="external-link">prod</a></span><span class="op">(</span><span class="va">b2_shape</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># Set the weight predictions as the weight variables on the outer model.</span></span>
<span>    <span class="va">dense1</span><span class="op">$</span><span class="va">kernel</span> <span class="op">&lt;-</span> <span class="va">w1</span></span>
<span>    <span class="va">dense1</span><span class="op">$</span><span class="va">bias</span> <span class="op">&lt;-</span> <span class="va">b1</span></span>
<span>    <span class="va">dense2</span><span class="op">$</span><span class="va">kernel</span> <span class="op">&lt;-</span> <span class="va">w2</span></span>
<span>    <span class="va">dense2</span><span class="op">$</span><span class="va">bias</span> <span class="op">&lt;-</span> <span class="va">b2</span></span>
<span></span>
<span>    <span class="co"># Inference on the outer model.</span></span>
<span>    <span class="va">preds</span> <span class="op">&lt;-</span> <span class="fu">main_network</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>    <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu">loss_fn</span><span class="op">(</span><span class="va">y</span>, <span class="va">preds</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">grads</span> <span class="op">&lt;-</span> <span class="va">tape</span><span class="op">$</span><span class="fu">gradient</span><span class="op">(</span><span class="va">loss</span>, <span class="va">hypernetwork</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span></span>
<span>  <span class="va">optimizer</span><span class="op">$</span><span class="fu">apply_gradients</span><span class="op">(</span><span class="fu"><a href="../reference/zip_lists.html">zip_lists</a></span><span class="op">(</span><span class="va">grads</span>, <span class="va">hypernetwork</span><span class="op">$</span><span class="va">trainable_weights</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">loss</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">losses</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">)</span>  <span class="co"># Keep track of the losses over time.</span></span>
<span><span class="fu">coro</span><span class="fu">::</span><span class="fu">loop</span><span class="op">(</span><span class="kw">for</span> <span class="op">(</span><span class="va">data</span> <span class="kw">in</span> <span class="va">dataset</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>  <span class="va">loss</span> <span class="op">&lt;-</span> <span class="fu">train_step</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Logging.</span></span>
<span>  <span class="va">losses</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">loss</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/Arithmetic.html" class="external-link">%%</a></span> <span class="fl">100</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Step:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span>, <span class="st">"Loss:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="co"># Stop after 1000 steps.</span></span>
<span>  <span class="co"># Training the model to convergence is left</span></span>
<span>  <span class="co"># as an exercise to the reader.</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">losses</span><span class="op">)</span> <span class="op">&gt;=</span> <span class="fl">1000</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">break</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Step: 100 Loss: 2.536778</span></span>
<span><span class="co">## Step: 200 Loss: 2.236472</span></span>
<span><span class="co">## Step: 300 Loss: 2.119417</span></span>
<span><span class="co">## Step: 400 Loss: 2.040341</span></span>
<span><span class="co">## Step: 500 Loss: 1.949125</span></span>
<span><span class="co">## Step: 600 Loss: 1.859384</span></span>
<span><span class="co">## Step: 700 Loss: 1.845726</span></span>
<span><span class="co">## Step: 800 Loss: 1.820594</span></span>
<span><span class="co">## Step: 900 Loss: 1.771334</span></span>
<span><span class="co">## Step: 1000 Loss: 1.730648</span></span></code></pre>
<p>Implementing arbitrary research ideas with Keras is straightforward
and highly productive. Imagine trying out 25 ideas per day (20 minutes
per experiment on average)!</p>
<p>Keras has been designed to go from idea to results as fast as
possible, because we believe this is the key to doing great
research.</p>
<p>We hope you enjoyed this quick introduction. Let us know what you
build with Keras!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
