<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Distributed training with Keras 3 • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Distributed training with Keras 3">
<meta name="description" content="Complete guide to the distribution API for multi-backend Keras.">
<meta property="og:description" content="Complete guide to the distribution API for multi-backend Keras.">
<meta name="robots" content="noindex">
<!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager -->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-danger me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">1.4.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides">
<li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Distributed training with Keras 3</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/vignettes/distribution.Rmd" class="external-link"><code>vignettes/distribution.Rmd</code></a></small>
      <div class="d-none name"><code>distribution.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The Keras distribution API is a new interface designed to facilitate
distributed deep learning across a variety of backends like JAX,
TensorFlow and PyTorch. This powerful API introduces a suite of tools
enabling data and model parallelism, allowing for efficient scaling of
deep learning models on multiple accelerators and hosts. Whether
leveraging the power of GPUs or TPUs, the API provides a streamlined
approach to initializing distributed environments, defining device
meshes, and orchestrating the layout of tensors across computational
resources. Through classes like <code>DataParallel</code> and
<code>ModelParallel</code>, it abstracts the complexity involved in
parallel computation, making it easier for developers to accelerate
their machine learning workflows.</p>
</div>
<div class="section level2">
<h2 id="how-it-works">How it works<a class="anchor" aria-label="anchor" href="#how-it-works"></a>
</h2>
<p>The Keras distribution API provides a global programming model that
allows developers to compose applications that operate on tensors in a
global context (as if working with a single device) while automatically
managing distribution across many devices. The API leverages the
underlying framework (e.g. JAX) to distribute the program and tensors
according to the sharding directives through a procedure called single
program, multiple data (SPMD) expansion.</p>
<p>By decoupling the application from sharding directives, the API
enables running the same application on a single device, multiple
devices, or even multiple clients, while preserving its global
semantics.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># This guide assumes there are 8 GPUs available for testing. If you don't have</span></span>
<span><span class="co"># 8 gpus available locally, you can set the following envvar to</span></span>
<span><span class="co"># make xla initialize the CPU as 8 devices, to enable local testing</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span><span class="st">"CUDA_VISIBLE_DEVICES"</span> <span class="op">=</span> <span class="st">""</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span><span class="st">"XLA_FLAGS"</span> <span class="op">=</span> <span class="st">"--xla_force_host_platform_device_count=8"</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://keras3.posit.co/">keras3</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The distribution API is only implemented for the JAX backend for now.</span></span>
<span><span class="fu"><a href="../reference/use_backend.html">use_backend</a></span><span class="op">(</span><span class="st">"jax"</span>, <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="va">jax</span> <span class="op">&lt;-</span> <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"jax"</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/rstudio/tfdatasets" class="external-link">tfdatasets</a></span>, exclude <span class="op">=</span> <span class="st">"shape"</span><span class="op">)</span> <span class="co"># For dataset input.</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="devicemesh-and-tensorlayout">
<code>DeviceMesh</code> and <code>TensorLayout</code><a class="anchor" aria-label="anchor" href="#devicemesh-and-tensorlayout"></a>
</h2>
<p>The <code>keras$distribution$DeviceMesh</code> class in Keras
distribution API represents a cluster of computational devices
configured for distributed computation. It aligns with similar concepts
in <a href="https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh" class="external-link"><code>jax.sharding.Mesh</code></a>
and <a href="https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh" class="external-link"><code>tf.dtensor.Mesh</code></a>,
where it’s used to map the physical devices to a logical mesh
structure.</p>
<p>The <code>TensorLayout</code> class then specifies how tensors are
distributed across the <code>DeviceMesh</code>, detailing the sharding
of tensors along specified axes that correspond to the names of the axes
in the <code>DeviceMesh</code>.</p>
<p>You can find more detailed concept explainers in the <a href="https://www.tensorflow.org/guide/dtensor_overview#dtensors_model_of_distributed_tensors" class="external-link">TensorFlow
DTensor guide</a>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Retrieve the local available gpu devices.</span></span>
<span><span class="va">devices</span> <span class="op">&lt;-</span> <span class="va">jax</span><span class="op">$</span><span class="fu">devices</span><span class="op">(</span><span class="op">)</span> <span class="co"># "gpu"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">devices</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 8</span></span>
<span><span class="co">##  $ :TFRT_CPU_0</span></span>
<span><span class="co">##  $ :TFRT_CPU_1</span></span>
<span><span class="co">##  $ :TFRT_CPU_2</span></span>
<span><span class="co">##  $ :TFRT_CPU_3</span></span>
<span><span class="co">##  $ :TFRT_CPU_4</span></span>
<span><span class="co">##  $ :TFRT_CPU_5</span></span>
<span><span class="co">##  $ :TFRT_CPU_6</span></span>
<span><span class="co">##  $ :TFRT_CPU_7</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Define a 2x4 device mesh with data and model parallel axes</span></span>
<span><span class="va">mesh</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A 2D layout, which describes how a tensor is distributed across the</span></span>
<span><span class="co"># mesh. The layout can be visualized as a 2D grid with "model" as rows and</span></span>
<span><span class="co"># "data" as columns, and it is a [4, 2] grid when it mapped to the physical</span></span>
<span><span class="co"># devices on the mesh.</span></span>
<span><span class="va">layout_2d</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">TensorLayout</span><span class="op">(</span></span>
<span>  axes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"model"</span>, <span class="st">"data"</span><span class="op">)</span>,</span>
<span>  device_mesh <span class="op">=</span> <span class="va">mesh</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># A 4D layout which could be used for data parallelism of an image input.</span></span>
<span><span class="va">replicated_layout_4d</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">TensorLayout</span><span class="op">(</span></span>
<span>  axes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span>, <span class="cn">NULL</span><span class="op">)</span>,</span>
<span>  device_mesh <span class="op">=</span> <span class="va">mesh</span></span>
<span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="distribution">Distribution<a class="anchor" aria-label="anchor" href="#distribution"></a>
</h2>
<p>The <code>Distribution</code> class in Keras serves as a foundational
abstract class designed for developing custom distribution strategies.
It encapsulates the core logic needed to distribute a model’s variables,
input data, and intermediate computations across a device mesh. As an
end user, you won’t have to interact directly with this class, but its
subclasses like <code>DataParallel</code> or
<code>ModelParallel</code>.</p>
</div>
<div class="section level2">
<h2 id="dataparallel">DataParallel<a class="anchor" aria-label="anchor" href="#dataparallel"></a>
</h2>
<p>The <code>DataParallel</code> class in the Keras distribution API is
designed for the data parallelism strategy in distributed training,
where the model weights are replicated across all devices in the
<code>DeviceMesh</code>, and each device processes a portion of the
input data.</p>
<p>Here is a sample usage of this class.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create DataParallel with list of devices.</span></span>
<span><span class="co"># As a shortcut, the devices can be skipped,</span></span>
<span><span class="co"># and Keras will detect all local available devices.</span></span>
<span><span class="co"># E.g. data_parallel &lt;- DataParallel()</span></span>
<span><span class="va">data_parallel</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DataParallel</span><span class="op">(</span>devices <span class="op">=</span> <span class="va">devices</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Or you can choose to create DataParallel with a 1D `DeviceMesh`.</span></span>
<span><span class="va">mesh_1d</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">8</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span><span class="va">data_parallel</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DataParallel</span><span class="op">(</span>device_mesh <span class="op">=</span> <span class="va">mesh_1d</span><span class="op">)</span></span>
<span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">128</span>, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">labels</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/random_normal.html">random_normal</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">128</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">dataset</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/tensor_slices_dataset.html" class="external-link">tensor_slices_dataset</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="va">inputs</span>, <span class="va">labels</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/tfdatasets/man/dataset_batch.html" class="external-link">dataset_batch</a></span><span class="op">(</span><span class="fl">16</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Set the global distribution.</span></span>
<span><span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">set_distribution</span><span class="op">(</span><span class="va">data_parallel</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Note that all the model weights from here on are replicated to</span></span>
<span><span class="co"># all the devices of the `DeviceMesh`. This includes the RNG</span></span>
<span><span class="co"># state, optimizer states, metrics, etc. The dataset fed into `model |&gt; fit()` or</span></span>
<span><span class="co"># `model |&gt; evaluate()` will be split evenly on the batch dimension, and sent to</span></span>
<span><span class="co"># all the devices. You don't have to do any manual aggregation of losses,</span></span>
<span><span class="co"># since all the computation happens in a global context.</span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_flatten.html">layer_flatten</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">200</span>, use_bias <span class="op">=</span> <span class="cn">FALSE</span>, activation <span class="op">=</span> <span class="st">"relu"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dropout.html">layer_dropout</a></span><span class="op">(</span><span class="fl">0.4</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span>, activation <span class="op">=</span> <span class="st">"softmax"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="va">inputs</span>, outputs <span class="op">=</span> <span class="va">outputs</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>loss <span class="op">=</span> <span class="st">"mse"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">dataset</span>, epochs <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/3</span></span>
<span><span class="co">## 8/8 - 0s - 42ms/step - loss: 1.1575</span></span>
<span><span class="co">## Epoch 2/3</span></span>
<span><span class="co">## 8/8 - 0s - 4ms/step - loss: 1.0557</span></span>
<span><span class="co">## Epoch 3/3</span></span>
<span><span class="co">## 8/8 - 0s - 4ms/step - loss: 1.0179</span></span></code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 8/8 - 0s - 9ms/step - loss: 0.9707</span></span></code></pre>
<pre><code><span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.9707004</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="modelparallel-and-layoutmap">
<code>ModelParallel</code> and <code>LayoutMap</code><a class="anchor" aria-label="anchor" href="#modelparallel-and-layoutmap"></a>
</h2>
<p><code>ModelParallel</code> will be mostly useful when model weights
are too large to fit on a single accelerator. This setting allows you to
spit your model weights or activation tensors across all the devices on
the <code>DeviceMesh</code>, and enable the horizontal scaling for the
large models.</p>
<p>Unlike the <code>DataParallel</code> model where all weights are
fully replicated, the weights layout under <code>ModelParallel</code>
usually need some customization for best performances. We introduce
<code>LayoutMap</code> to let you specify the <code>TensorLayout</code>
for any weights and intermediate tensors from global perspective.</p>
<p><code>LayoutMap</code> is a dict-like object that maps a string to
<code>TensorLayout</code> instances. It behaves differently from a
normal dict in that the string key is treated as a regex when retrieving
the value. The class allows you to define the naming schema of
<code>TensorLayout</code> and then retrieve the corresponding
<code>TensorLayout</code> instance. Typically, the key used to query is
the <code>variable$path</code> attribute, which is the identifier of the
variable. As a shortcut, a list of axis names is also allowed when
inserting a value, and it will be converted to
<code>TensorLayout</code>.</p>
<p>The <code>LayoutMap</code> can also optionally contain a
<code>DeviceMesh</code> to populate the
<code>TensorLayout$device_mesh</code> if it is not set. When retrieving
a layout with a key, and if there isn’t an exact match, all existing
keys in the layout map will be treated as regex and matched against the
input key again. If there are multiple matches, a
<code>ValueError</code> is raised. If no matches are found,
<code>NULL</code> is returned.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mesh_2d</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span><span class="va">layout_map</span>  <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">LayoutMap</span><span class="op">(</span><span class="va">mesh_2d</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The rule below means that for any weights that match with d1/kernel, it</span></span>
<span><span class="co"># will be sharded with model dimensions (4 devices), same for the d1/bias.</span></span>
<span><span class="co"># All other weights will be fully replicated.</span></span>
<span><span class="va">layout_map</span><span class="op">[</span><span class="st">"d1/kernel"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="cn">NULL</span>, <span class="st">"model"</span><span class="op">)</span></span>
<span><span class="va">layout_map</span><span class="op">[</span><span class="st">"d1/bias"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="st">"model"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># You can also set the layout for the layer output like</span></span>
<span><span class="va">layout_map</span><span class="op">[</span><span class="st">"d2/output"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model_parallel</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">ModelParallel</span><span class="op">(</span></span>
<span>  layout_map <span class="op">=</span> <span class="va">layout_map</span>, batch_dim_name <span class="op">=</span> <span class="st">"data"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">set_distribution</span><span class="op">(</span><span class="va">model_parallel</span><span class="op">)</span></span>
<span></span>
<span><span class="va">inputs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_input.html">layer_input</a></span><span class="op">(</span>shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">outputs</span> <span class="op">&lt;-</span> <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_flatten.html">layer_flatten</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">200</span>, use_bias <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>              activation <span class="op">=</span> <span class="st">"relu"</span>, name <span class="op">=</span> <span class="st">"d1"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dropout.html">layer_dropout</a></span><span class="op">(</span><span class="fl">0.4</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_dense.html">layer_dense</a></span><span class="op">(</span>units <span class="op">=</span> <span class="fl">10</span>,</span>
<span>              activation <span class="op">=</span> <span class="st">"softmax"</span>,</span>
<span>              name <span class="op">=</span> <span class="st">"d2"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span>inputs <span class="op">=</span> <span class="va">inputs</span>, outputs <span class="op">=</span> <span class="va">outputs</span><span class="op">)</span></span></code></pre></div>
<p>We can visualize how individual weights will be sharded</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">d1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_layer.html">get_layer</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"d1"</span><span class="op">)</span></span>
<span><span class="va">d1</span><span class="op">$</span><span class="va">kernel</span><span class="op">$</span><span class="va">value</span> <span class="op">|&gt;</span> <span class="va">jax</span><span class="op">$</span><span class="va">debug</span><span class="op">$</span><span class="fu">visualize_array_sharding</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## ┌───────┬───────┬───────┬───────┐</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │CPU 0,4│CPU 1,5│CPU 2,6│CPU 3,7│</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## │       │       │       │       │</span></span>
<span><span class="co">## └───────┴───────┴───────┴───────┘</span></span></code></pre>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">d2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/get_layer.html">get_layer</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"d2"</span><span class="op">)</span></span>
<span><span class="va">d2</span><span class="op">$</span><span class="va">kernel</span><span class="op">$</span><span class="va">value</span> <span class="op">|&gt;</span> <span class="va">jax</span><span class="op">$</span><span class="va">debug</span><span class="op">$</span><span class="fu">visualize_array_sharding</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## ┌───────────────────┐</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │CPU 0,1,2,3,4,5,6,7│</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## │                   │</span></span>
<span><span class="co">## └───────────────────┘</span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">d2</span><span class="op">$</span><span class="va">bias</span><span class="op">$</span><span class="va">value</span> <span class="op">|&gt;</span> <span class="va">jax</span><span class="op">$</span><span class="va">debug</span><span class="op">$</span><span class="fu">visualize_array_sharding</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## ┌───────────────────┐</span></span>
<span><span class="co">## │CPU 0,1,2,3,4,5,6,7│</span></span>
<span><span class="co">## └───────────────────┘</span></span></code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x_batch</span> <span class="op">&lt;-</span> <span class="va">dataset</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/iterate.html" class="external-link">as_iterator</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/iterate.html" class="external-link">iter_next</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="va">_</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/op_convert_to_tensor.html">op_convert_to_tensor</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">output_array</span> <span class="op">&lt;-</span> <span class="fu">model</span><span class="op">(</span><span class="va">x_batch</span><span class="op">)</span></span>
<span><span class="va">output_array</span> <span class="op">|&gt;</span> <span class="va">jax</span><span class="op">$</span><span class="va">debug</span><span class="op">$</span><span class="fu">visualize_array_sharding</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## ┌─────────────┐</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## │ CPU 0,1,2,3 │</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## ├─────────────┤</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## │ CPU 4,5,6,7 │</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## │             │</span></span>
<span><span class="co">## └─────────────┘</span></span></code></pre>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># The data will be sharded across the "data" dimension of the method, which</span></span>
<span><span class="co"># has 2 devices.</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/compile.html" class="external-link">compile</a></span><span class="op">(</span>loss <span class="op">=</span> <span class="st">"mse"</span><span class="op">)</span></span>
<span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://generics.r-lib.org/reference/fit.html" class="external-link">fit</a></span><span class="op">(</span><span class="va">dataset</span>, epochs <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Epoch 1/3</span></span>
<span><span class="co">## 8/8 - 0s - 48ms/step - loss: 1.1454</span></span>
<span><span class="co">## Epoch 2/3</span></span>
<span><span class="co">## 8/8 - 0s - 2ms/step - loss: 1.0527</span></span>
<span><span class="co">## Epoch 3/3</span></span>
<span><span class="co">## 8/8 - 0s - 2ms/step - loss: 1.0115</span></span></code></pre>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/pkg/tensorflow/man/evaluate.html" class="external-link">evaluate</a></span><span class="op">(</span><span class="va">dataset</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## 8/8 - 0s - 10ms/step - loss: 0.9674</span></span></code></pre>
<pre><code><span><span class="co">## $loss</span></span>
<span><span class="co">## [1] 0.9673891</span></span></code></pre>
<p>It is also easy to change the mesh structure to tune the computation
between more data parallel or model parallel. You can do this by
adjusting the shape of the mesh. And no changes are needed for any other
code.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">full_data_parallel_mesh</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">8</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span><span class="va">more_data_parallel_mesh</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">2</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span><span class="va">more_model_parallel_mesh</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">4</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span>
<span><span class="va">full_model_parallel_mesh</span> <span class="op">&lt;-</span> <span class="va">keras</span><span class="op">$</span><span class="va">distribution</span><span class="op">$</span><span class="fu">DeviceMesh</span><span class="op">(</span></span>
<span>  shape <span class="op">=</span> <span class="fu"><a href="../reference/shape.html">shape</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">8</span><span class="op">)</span>,</span>
<span>  axis_names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="st">"data"</span>, <span class="st">"model"</span><span class="op">)</span>,</span>
<span>  devices <span class="op">=</span> <span class="va">devices</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="section level3">
<h3 id="further-reading">Further reading<a class="anchor" aria-label="anchor" href="#further-reading"></a>
</h3>
<ol style="list-style-type: decimal">
<li><a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html" class="external-link">JAX
Distributed arrays and automatic parallelization</a></li>
<li><a href="https://jax.readthedocs.io/en/latest/jax.sharding.html" class="external-link">JAX
sharding module</a></li>
<li><a href="https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial" class="external-link">TensorFlow
Distributed training with DTensors</a></li>
<li><a href="https://www.tensorflow.org/guide/dtensor_overview" class="external-link">TensorFlow
DTensor concepts</a></li>
<li><a href="https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial" class="external-link">Using
DTensors with tf.keras</a></li>
</ol>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
