<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="This layer enables the use of Flax components in the form of
flax.linen.Module
instances within Keras when using JAX as the backend for Keras.
The module method to use for the forward pass can be specified via the
method argument and is __call__ by default. This method must take the
following arguments with these exact names:
self if the method is bound to the module, which is the case for the
default of __call__, and module otherwise to pass the module.
inputs: the inputs to the model, a JAX array or a PyTree of arrays.
training (optional): an argument specifying if we're in training mode
or inference mode, TRUE is passed in training mode.


FlaxLayer handles the non-trainable state of your model and required RNGs
automatically. Note that the mutable parameter of
flax.linen.Module.apply()
is set to DenyList([&quot;params&quot;]), therefore making the assumption that all
the variables outside of the &quot;params&quot; collection are non-trainable weights.
This example shows how to create a FlaxLayer from a Flax Module with
the default __call__ method and no training argument:
# keras3::use_backend(&quot;jax&quot;)
# py_install(&quot;flax&quot;, &quot;r-keras&quot;)

if(config_backend() == &quot;jax&quot; &amp;amp;&amp;amp;
   reticulate::py_module_available(&quot;flax&quot;)) {

flax &amp;lt;- import(&quot;flax&quot;)

MyFlaxModule(flax$linen$Module) %py_class% {
  `__call__` &amp;lt;- flax$linen$compact(\(self, inputs) {
    inputs |&amp;gt;
      (flax$linen$Conv(features = 32L, kernel_size = tuple(3L, 3L)))() |&amp;gt;
      flax$linen$relu() |&amp;gt;
      flax$linen$avg_pool(window_shape = tuple(2L, 2L),
                          strides = tuple(2L, 2L)) |&amp;gt;
      # flatten all except batch_size axis
      (\(x) x$reshape(tuple(x$shape[[1]], -1L)))() |&amp;gt;
      (flax$linen$Dense(features = 200L))() |&amp;gt;
      flax$linen$relu() |&amp;gt;
      (flax$linen$Dense(features = 10L))() |&amp;gt;
      flax$linen$softmax()
  })
}

# typical usage:
input &amp;lt;- keras_input(c(28, 28, 3))
output &amp;lt;- input |&amp;gt;
  layer_flax_module_wrapper(MyFlaxModule())

model &amp;lt;- keras_model(input, output)

# to instantiate the layer before composing:
flax_module &amp;lt;- MyFlaxModule()
keras_layer &amp;lt;- layer_flax_module_wrapper(module = flax_module)

input &amp;lt;- keras_input(c(28, 28, 3))
output &amp;lt;- input |&amp;gt;
  keras_layer()

model &amp;lt;- keras_model(input, output)

}

This example shows how to wrap the module method to conform to the required
signature. This allows having multiple input arguments and a training
argument that has a different name and values. This additionally shows how
to use a function that is not bound to the module.
flax &amp;lt;- import(&quot;flax&quot;)

MyFlaxModule(flax$linen$Module) \%py_class\% {
  forward &amp;lt;-
    flax$linen$compact(\(self, inputs1, input2, deterministic) {
      # do work ....
      outputs # return
    })
}

my_flax_module_wrapper &amp;lt;- function(module, inputs, training) {
  c(input1, input2) \%&amp;lt;-\% inputs
  module$forward(input1, input2,!training)
}

flax_module &amp;lt;- MyFlaxModule()
keras_layer &amp;lt;- layer_flax_module_wrapper(module = flax_module,
                                         method = my_flax_module_wrapper)
"><title>Keras Layer that wraps a Flax module. — layer_flax_module_wrapper • keras3</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Keras Layer that wraps a Flax module. — layer_flax_module_wrapper"><meta property="og:description" content="This layer enables the use of Flax components in the form of
flax.linen.Module
instances within Keras when using JAX as the backend for Keras.
The module method to use for the forward pass can be specified via the
method argument and is __call__ by default. This method must take the
following arguments with these exact names:
self if the method is bound to the module, which is the case for the
default of __call__, and module otherwise to pass the module.
inputs: the inputs to the model, a JAX array or a PyTree of arrays.
training (optional): an argument specifying if we're in training mode
or inference mode, TRUE is passed in training mode.


FlaxLayer handles the non-trainable state of your model and required RNGs
automatically. Note that the mutable parameter of
flax.linen.Module.apply()
is set to DenyList([&quot;params&quot;]), therefore making the assumption that all
the variables outside of the &quot;params&quot; collection are non-trainable weights.
This example shows how to create a FlaxLayer from a Flax Module with
the default __call__ method and no training argument:
# keras3::use_backend(&quot;jax&quot;)
# py_install(&quot;flax&quot;, &quot;r-keras&quot;)

if(config_backend() == &quot;jax&quot; &amp;amp;&amp;amp;
   reticulate::py_module_available(&quot;flax&quot;)) {

flax &amp;lt;- import(&quot;flax&quot;)

MyFlaxModule(flax$linen$Module) %py_class% {
  `__call__` &amp;lt;- flax$linen$compact(\(self, inputs) {
    inputs |&amp;gt;
      (flax$linen$Conv(features = 32L, kernel_size = tuple(3L, 3L)))() |&amp;gt;
      flax$linen$relu() |&amp;gt;
      flax$linen$avg_pool(window_shape = tuple(2L, 2L),
                          strides = tuple(2L, 2L)) |&amp;gt;
      # flatten all except batch_size axis
      (\(x) x$reshape(tuple(x$shape[[1]], -1L)))() |&amp;gt;
      (flax$linen$Dense(features = 200L))() |&amp;gt;
      flax$linen$relu() |&amp;gt;
      (flax$linen$Dense(features = 10L))() |&amp;gt;
      flax$linen$softmax()
  })
}

# typical usage:
input &amp;lt;- keras_input(c(28, 28, 3))
output &amp;lt;- input |&amp;gt;
  layer_flax_module_wrapper(MyFlaxModule())

model &amp;lt;- keras_model(input, output)

# to instantiate the layer before composing:
flax_module &amp;lt;- MyFlaxModule()
keras_layer &amp;lt;- layer_flax_module_wrapper(module = flax_module)

input &amp;lt;- keras_input(c(28, 28, 3))
output &amp;lt;- input |&amp;gt;
  keras_layer()

model &amp;lt;- keras_model(input, output)

}

This example shows how to wrap the module method to conform to the required
signature. This allows having multiple input arguments and a training
argument that has a different name and values. This additionally shows how
to use a function that is not bound to the module.
flax &amp;lt;- import(&quot;flax&quot;)

MyFlaxModule(flax$linen$Module) \%py_class\% {
  forward &amp;lt;-
    flax$linen$compact(\(self, inputs1, input2, deterministic) {
      # do work ....
      outputs # return
    })
}

my_flax_module_wrapper &amp;lt;- function(module, inputs, training) {
  c(input1, input2) \%&amp;lt;-\% inputs
  module$forward(input1, input2,!training)
}

flax_module &amp;lt;- MyFlaxModule()
keras_layer &amp;lt;- layer_flax_module_wrapper(module = flax_module,
                                         method = my_flax_module_wrapper)
"><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary" data-bs-theme="inverse"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../articles/getting_started.html">Getting Started</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
    <a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Keras Layer that wraps a <a href="https://flax.readthedocs.io" class="external-link">Flax</a> module.</h1>
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/R/layers-backend-wrappers.R" class="external-link"><code>R/layers-backend-wrappers.R</code></a></small>
      <div class="d-none name"><code>layer_flax_module_wrapper.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>This layer enables the use of Flax components in the form of
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" class="external-link"><code>flax.linen.Module</code></a>
instances within Keras when using JAX as the backend for Keras.</p>
<p>The module method to use for the forward pass can be specified via the
<code>method</code> argument and is <code>__call__</code> by default. This method must take the
following arguments with these exact names:</p><ul><li><p><code>self</code> if the method is bound to the module, which is the case for the
default of <code>__call__</code>, and <code>module</code> otherwise to pass the module.</p></li>
<li><p><code>inputs</code>: the inputs to the model, a JAX array or a <code>PyTree</code> of arrays.</p></li>
<li><p><code>training</code> <em>(optional)</em>: an argument specifying if we're in training mode
or inference mode, <code>TRUE</code> is passed in training mode.</p></li>
</ul><p><code>FlaxLayer</code> handles the non-trainable state of your model and required RNGs
automatically. Note that the <code>mutable</code> parameter of
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.apply" class="external-link"><code>flax.linen.Module.apply()</code></a>
is set to <code>DenyList(["params"])</code>, therefore making the assumption that all
the variables outside of the "params" collection are non-trainable weights.</p>
<p>This example shows how to create a <code>FlaxLayer</code> from a Flax <code>Module</code> with
the default <code>__call__</code> method and no training argument:</p>
<p></p><div class="sourceCode r"><pre><code><span><span class="co"># keras3::use_backend("jax")</span></span>
<span><span class="co"># py_install("flax", "r-keras")</span></span>
<span></span>
<span><span class="kw">if</span><span class="op">(</span><span class="fu"><a href="../reference/config_backend.html">config_backend</a></span><span class="op">(</span><span class="op">)</span> <span class="op">==</span> <span class="st">"jax"</span> <span class="op">&amp;&amp;</span></span>
<span>   <span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/py_module_available.html" class="external-link">py_module_available</a></span><span class="op">(</span><span class="st">"flax"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span><span class="va">flax</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/import.html" class="external-link">import</a></span><span class="op">(</span><span class="st">"flax"</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">MyFlaxModule</span><span class="op">(</span><span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="va">Module</span><span class="op">)</span> <span class="op"><a href="../reference/grapes-py_class-grapes.html">%py_class%</a></span> <span class="op">{</span></span>
<span>  <span class="va">`__call__`</span> <span class="op">&lt;-</span> <span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">compact</span><span class="op">(</span>\<span class="op">(</span><span class="va">self</span>, <span class="va">inputs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">inputs</span> <span class="op">|&gt;</span></span>
<span>      <span class="op">(</span><span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">Conv</span><span class="op">(</span>features <span class="op">=</span> <span class="fl">32L</span>, kernel_size <span class="op">=</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="fl">3L</span>, <span class="fl">3L</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">avg_pool</span><span class="op">(</span>window_shape <span class="op">=</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="fl">2L</span>, <span class="fl">2L</span><span class="op">)</span>,</span>
<span>                          strides <span class="op">=</span> <span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="fl">2L</span>, <span class="fl">2L</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="co"># flatten all except batch_size axis</span></span>
<span>      <span class="op">(</span>\<span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">$</span><span class="fu">reshape</span><span class="op">(</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/tuple.html" class="external-link">tuple</a></span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">shape</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>, <span class="op">-</span><span class="fl">1L</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="op">(</span><span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">Dense</span><span class="op">(</span>features <span class="op">=</span> <span class="fl">200L</span><span class="op">)</span><span class="op">)</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">relu</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="op">(</span><span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">Dense</span><span class="op">(</span>features <span class="op">=</span> <span class="fl">10L</span><span class="op">)</span><span class="op">)</span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>      <span class="va">flax</span><span class="op">$</span><span class="va">linen</span><span class="op">$</span><span class="fu">softmax</span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># typical usage:</span></span>
<span><span class="va">input</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">input</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/layer_flax_module_wrapper.html">layer_flax_module_wrapper</a></span><span class="op">(</span><span class="fu">MyFlaxModule</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">input</span>, <span class="va">output</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># to instantiate the layer before composing:</span></span>
<span><span class="va">flax_module</span> <span class="op">&lt;-</span> <span class="fu">MyFlaxModule</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">keras_layer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/layer_flax_module_wrapper.html">layer_flax_module_wrapper</a></span><span class="op">(</span>module <span class="op">=</span> <span class="va">flax_module</span><span class="op">)</span></span>
<span></span>
<span><span class="va">input</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_input.html">keras_input</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">output</span> <span class="op">&lt;-</span> <span class="va">input</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">keras_layer</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/keras_model.html">keras_model</a></span><span class="op">(</span><span class="va">input</span>, <span class="va">output</span><span class="op">)</span></span>
<span></span>
<span><span class="op">}</span></span></code></pre><p></p></div>
<p>This example shows how to wrap the module method to conform to the required
signature. This allows having multiple input arguments and a training
argument that has a different name and values. This additionally shows how
to use a function that is not bound to the module.</p>
<p></p><div class="sourceCode r"><pre><code>flax &lt;- import("flax")

MyFlaxModule(flax$linen$Module) \%py_class\% {
  forward &lt;-
    flax$linen$compact(\(self, inputs1, input2, deterministic) {
      # do work ....
      outputs # return
    })
}

my_flax_module_wrapper &lt;- function(module, inputs, training) {
  c(input1, input2) \%&lt;-\% inputs
  module$forward(input1, input2,!training)
}

flax_module &lt;- MyFlaxModule()
keras_layer &lt;- layer_flax_module_wrapper(module = flax_module,
                                         method = my_flax_module_wrapper)
</code></pre><p></p></div>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">layer_flax_module_wrapper</span><span class="op">(</span><span class="va">object</span>, <span class="va">module</span>, method <span class="op">=</span> <span class="cn">NULL</span>, variables <span class="op">=</span> <span class="cn">NULL</span>, <span class="va">...</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>object</dt>
<dd><p>Object to compose the layer with. A tensor, array, or sequential model.</p></dd>


<dt>module</dt>
<dd><p>An instance of <code>flax.linen.Module</code> or subclass.</p></dd>


<dt>method</dt>
<dd><p>The method to call the model. This is generally a method in the
<code>Module</code>. If not provided, the <code>__call__</code> method is used. <code>method</code>
can also be a function not defined in the <code>Module</code>, in which case it
must take the <code>Module</code> as the first argument. It is used for both
<code>Module.init</code> and <code>Module.apply</code>. Details are documented in the
<code>method</code> argument of <a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.apply" class="external-link"><code>flax.linen.Module.apply()</code></a>.</p></dd>


<dt>variables</dt>
<dd><p>A <code>dict</code> (named R list) containing all the variables of the module in the
same format as what is returned by <a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.init" class="external-link"><code>flax.linen.Module.init()</code></a>.
It should contain a <code>"params"</code> key and, if applicable, other keys for
collections of variables for non-trainable state. This allows
passing trained parameters and learned non-trainable state or
controlling the initialization. If <code>NULL</code> is passed, the module's
<code>init</code> function is called at build time to initialize the variables
of the model.</p></dd>


<dt>...</dt>
<dd><p>For forward/backward compatability.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>The return value depends on the value provided for the first argument.
If  <code>object</code> is:</p><ul><li><p>a <code><a href="keras_model_sequential.html">keras_model_sequential()</a></code>, then the layer is added to the sequential model
(which is modified in place). To enable piping, the sequential model is also
returned, invisibly.</p></li>
<li><p>a <code><a href="keras_input.html">keras_input()</a></code>, then the output tensor from calling <code>layer(input)</code> is returned.</p></li>
<li><p><code>NULL</code> or missing, then a <code>Layer</code> instance is returned.</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/FlaxLayer" class="external-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/FlaxLayer</a></p></li>
</ul><p>Other wrapping layers: <br><code><a href="layer_jax_model_wrapper.html">layer_jax_model_wrapper</a>()</code> <br><code><a href="layer_torch_module_wrapper.html">layer_torch_module_wrapper</a>()</code> <br></p>
<p>Other layers: <br><code><a href="Layer.html">Layer</a>()</code> <br><code><a href="layer_activation.html">layer_activation</a>()</code> <br><code><a href="layer_activation_elu.html">layer_activation_elu</a>()</code> <br><code><a href="layer_activation_leaky_relu.html">layer_activation_leaky_relu</a>()</code> <br><code><a href="layer_activation_parametric_relu.html">layer_activation_parametric_relu</a>()</code> <br><code><a href="layer_activation_relu.html">layer_activation_relu</a>()</code> <br><code><a href="layer_activation_softmax.html">layer_activation_softmax</a>()</code> <br><code><a href="layer_activity_regularization.html">layer_activity_regularization</a>()</code> <br><code><a href="layer_add.html">layer_add</a>()</code> <br><code><a href="layer_additive_attention.html">layer_additive_attention</a>()</code> <br><code><a href="layer_alpha_dropout.html">layer_alpha_dropout</a>()</code> <br><code><a href="layer_attention.html">layer_attention</a>()</code> <br><code><a href="layer_average.html">layer_average</a>()</code> <br><code><a href="layer_average_pooling_1d.html">layer_average_pooling_1d</a>()</code> <br><code><a href="layer_average_pooling_2d.html">layer_average_pooling_2d</a>()</code> <br><code><a href="layer_average_pooling_3d.html">layer_average_pooling_3d</a>()</code> <br><code><a href="layer_batch_normalization.html">layer_batch_normalization</a>()</code> <br><code><a href="layer_bidirectional.html">layer_bidirectional</a>()</code> <br><code><a href="layer_category_encoding.html">layer_category_encoding</a>()</code> <br><code><a href="layer_center_crop.html">layer_center_crop</a>()</code> <br><code><a href="layer_concatenate.html">layer_concatenate</a>()</code> <br><code><a href="layer_conv_1d.html">layer_conv_1d</a>()</code> <br><code><a href="layer_conv_1d_transpose.html">layer_conv_1d_transpose</a>()</code> <br><code><a href="layer_conv_2d.html">layer_conv_2d</a>()</code> <br><code><a href="layer_conv_2d_transpose.html">layer_conv_2d_transpose</a>()</code> <br><code><a href="layer_conv_3d.html">layer_conv_3d</a>()</code> <br><code><a href="layer_conv_3d_transpose.html">layer_conv_3d_transpose</a>()</code> <br><code><a href="layer_conv_lstm_1d.html">layer_conv_lstm_1d</a>()</code> <br><code><a href="layer_conv_lstm_2d.html">layer_conv_lstm_2d</a>()</code> <br><code><a href="layer_conv_lstm_3d.html">layer_conv_lstm_3d</a>()</code> <br><code><a href="layer_cropping_1d.html">layer_cropping_1d</a>()</code> <br><code><a href="layer_cropping_2d.html">layer_cropping_2d</a>()</code> <br><code><a href="layer_cropping_3d.html">layer_cropping_3d</a>()</code> <br><code><a href="layer_dense.html">layer_dense</a>()</code> <br><code><a href="layer_depthwise_conv_1d.html">layer_depthwise_conv_1d</a>()</code> <br><code><a href="layer_depthwise_conv_2d.html">layer_depthwise_conv_2d</a>()</code> <br><code><a href="layer_discretization.html">layer_discretization</a>()</code> <br><code><a href="layer_dot.html">layer_dot</a>()</code> <br><code><a href="layer_dropout.html">layer_dropout</a>()</code> <br><code><a href="layer_einsum_dense.html">layer_einsum_dense</a>()</code> <br><code><a href="layer_embedding.html">layer_embedding</a>()</code> <br><code><a href="layer_feature_space.html">layer_feature_space</a>()</code> <br><code><a href="layer_flatten.html">layer_flatten</a>()</code> <br><code><a href="layer_gaussian_dropout.html">layer_gaussian_dropout</a>()</code> <br><code><a href="layer_gaussian_noise.html">layer_gaussian_noise</a>()</code> <br><code><a href="layer_global_average_pooling_1d.html">layer_global_average_pooling_1d</a>()</code> <br><code><a href="layer_global_average_pooling_2d.html">layer_global_average_pooling_2d</a>()</code> <br><code><a href="layer_global_average_pooling_3d.html">layer_global_average_pooling_3d</a>()</code> <br><code><a href="layer_global_max_pooling_1d.html">layer_global_max_pooling_1d</a>()</code> <br><code><a href="layer_global_max_pooling_2d.html">layer_global_max_pooling_2d</a>()</code> <br><code><a href="layer_global_max_pooling_3d.html">layer_global_max_pooling_3d</a>()</code> <br><code><a href="layer_group_normalization.html">layer_group_normalization</a>()</code> <br><code><a href="layer_group_query_attention.html">layer_group_query_attention</a>()</code> <br><code><a href="layer_gru.html">layer_gru</a>()</code> <br><code><a href="layer_hashed_crossing.html">layer_hashed_crossing</a>()</code> <br><code><a href="layer_hashing.html">layer_hashing</a>()</code> <br><code><a href="layer_identity.html">layer_identity</a>()</code> <br><code><a href="layer_integer_lookup.html">layer_integer_lookup</a>()</code> <br><code><a href="layer_jax_model_wrapper.html">layer_jax_model_wrapper</a>()</code> <br><code><a href="layer_lambda.html">layer_lambda</a>()</code> <br><code><a href="layer_layer_normalization.html">layer_layer_normalization</a>()</code> <br><code><a href="layer_lstm.html">layer_lstm</a>()</code> <br><code><a href="layer_masking.html">layer_masking</a>()</code> <br><code><a href="layer_max_pooling_1d.html">layer_max_pooling_1d</a>()</code> <br><code><a href="layer_max_pooling_2d.html">layer_max_pooling_2d</a>()</code> <br><code><a href="layer_max_pooling_3d.html">layer_max_pooling_3d</a>()</code> <br><code><a href="layer_maximum.html">layer_maximum</a>()</code> <br><code><a href="layer_mel_spectrogram.html">layer_mel_spectrogram</a>()</code> <br><code><a href="layer_minimum.html">layer_minimum</a>()</code> <br><code><a href="layer_multi_head_attention.html">layer_multi_head_attention</a>()</code> <br><code><a href="layer_multiply.html">layer_multiply</a>()</code> <br><code><a href="layer_normalization.html">layer_normalization</a>()</code> <br><code><a href="layer_permute.html">layer_permute</a>()</code> <br><code><a href="layer_random_brightness.html">layer_random_brightness</a>()</code> <br><code><a href="layer_random_contrast.html">layer_random_contrast</a>()</code> <br><code><a href="layer_random_crop.html">layer_random_crop</a>()</code> <br><code><a href="layer_random_flip.html">layer_random_flip</a>()</code> <br><code><a href="layer_random_rotation.html">layer_random_rotation</a>()</code> <br><code><a href="layer_random_translation.html">layer_random_translation</a>()</code> <br><code><a href="layer_random_zoom.html">layer_random_zoom</a>()</code> <br><code><a href="layer_repeat_vector.html">layer_repeat_vector</a>()</code> <br><code><a href="layer_rescaling.html">layer_rescaling</a>()</code> <br><code><a href="layer_reshape.html">layer_reshape</a>()</code> <br><code><a href="layer_resizing.html">layer_resizing</a>()</code> <br><code><a href="layer_rnn.html">layer_rnn</a>()</code> <br><code><a href="layer_separable_conv_1d.html">layer_separable_conv_1d</a>()</code> <br><code><a href="layer_separable_conv_2d.html">layer_separable_conv_2d</a>()</code> <br><code><a href="layer_simple_rnn.html">layer_simple_rnn</a>()</code> <br><code><a href="layer_spatial_dropout_1d.html">layer_spatial_dropout_1d</a>()</code> <br><code><a href="layer_spatial_dropout_2d.html">layer_spatial_dropout_2d</a>()</code> <br><code><a href="layer_spatial_dropout_3d.html">layer_spatial_dropout_3d</a>()</code> <br><code><a href="layer_spectral_normalization.html">layer_spectral_normalization</a>()</code> <br><code><a href="layer_string_lookup.html">layer_string_lookup</a>()</code> <br><code><a href="layer_subtract.html">layer_subtract</a>()</code> <br><code><a href="layer_text_vectorization.html">layer_text_vectorization</a>()</code> <br><code><a href="layer_tfsm.html">layer_tfsm</a>()</code> <br><code><a href="layer_time_distributed.html">layer_time_distributed</a>()</code> <br><code><a href="layer_torch_module_wrapper.html">layer_torch_module_wrapper</a>()</code> <br><code><a href="layer_unit_normalization.html">layer_unit_normalization</a>()</code> <br><code><a href="layer_upsampling_1d.html">layer_upsampling_1d</a>()</code> <br><code><a href="layer_upsampling_2d.html">layer_upsampling_2d</a>()</code> <br><code><a href="layer_upsampling_3d.html">layer_upsampling_3d</a>()</code> <br><code><a href="layer_zero_padding_1d.html">layer_zero_padding_1d</a>()</code> <br><code><a href="layer_zero_padding_2d.html">layer_zero_padding_2d</a>()</code> <br><code><a href="layer_zero_padding_3d.html">layer_zero_padding_3d</a>()</code> <br><code><a href="rnn_cell_gru.html">rnn_cell_gru</a>()</code> <br><code><a href="rnn_cell_lstm.html">rnn_cell_lstm</a>()</code> <br><code><a href="rnn_cell_simple.html">rnn_cell_simple</a>()</code> <br><code><a href="rnn_cells_stack.html">rnn_cells_stack</a>()</code> <br></p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.8.</p>
</div>

    </footer></div>

  

  

  </body></html>

