<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Scaled Exponential Linear Unit (SELU). — activation_selu • keras3</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet"><meta property="og:title" content="Scaled Exponential Linear Unit (SELU). — activation_selu"><meta name="description" content='The Scaled Exponential Linear Unit (SELU) activation function is defined as:
scale * x if x &amp;gt; 0
scale * alpha * (exp(x) - 1) if x &amp;lt; 0


where alpha and scale are pre-defined constants
(alpha = 1.67326324 and scale = 1.05070098).
Basically, the SELU activation function multiplies scale (&amp;gt; 1) with the
output of the activation_elu function to ensure a slope larger
than one for positive inputs.
The values of alpha and scale are
chosen so that the mean and variance of the inputs are preserved
between two consecutive layers as long as the weights are initialized
correctly (see initializer_lecun_normal())
and the number of input units is "large enough"
(see reference paper for more information).'><meta property="og:description" content='The Scaled Exponential Linear Unit (SELU) activation function is defined as:
scale * x if x &amp;gt; 0
scale * alpha * (exp(x) - 1) if x &amp;lt; 0


where alpha and scale are pre-defined constants
(alpha = 1.67326324 and scale = 1.05070098).
Basically, the SELU activation function multiplies scale (&amp;gt; 1) with the
output of the activation_elu function to ensure a slope larger
than one for positive inputs.
The values of alpha and scale are
chosen so that the mean and variance of the inputs are preserved
between two consecutive layers as long as the weights are initialized
correctly (see initializer_lecun_normal())
and the number of input units is "large enough"
(see reference paper for more information).'><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-KHBDBW7');</script><!-- End Google Tag Manager --></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-KHBDBW7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) -->


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="inverse" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/getting_started.html">Getting Started</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-guides" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Guides</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-guides"><li><h6 class="dropdown-header" data-toc-skip>Model definition</h6></li>
    <li><a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a></li>
    <li><a class="dropdown-item" href="../articles/functional_api.html">Functional API</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6></li>
    <li><a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a></li>
    <li><a class="dropdown-item" href="../articles/custom_train_step_in_tensorflow.html">Customizing `fit()` with Tensorflow</a></li>
    <li><a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing your own callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a></li>
    <li><a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/serialization_and_saving.html">Serialization and Saving</a></li>
    <li><h6 class="dropdown-header" data-toc-skip>Other topics</h6></li>
    <li><a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a></li>
    <li><a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a></li>
    <li><a class="dropdown-item" href="../articles/distribution.html">Distributed training with Jax</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../articles/examples/index.html">Examples</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/rstudio/keras3/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Scaled Exponential Linear Unit (SELU).</h1>
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras3/blob/HEAD/R/activations.R" class="external-link"><code>R/activations.R</code></a></small>
      <div class="d-none name"><code>activation_selu.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The Scaled Exponential Linear Unit (SELU) activation function is defined as:</p><ul><li><p><code>scale * x</code> if <code>x &gt; 0</code></p></li>
<li><p><code>scale * alpha * (exp(x) - 1)</code> if <code>x &lt; 0</code></p></li>
</ul><p>where <code>alpha</code> and <code>scale</code> are pre-defined constants
(<code>alpha = 1.67326324</code> and <code>scale = 1.05070098</code>).</p>
<p>Basically, the SELU activation function multiplies <code>scale</code> (&gt; 1) with the
output of the <code>activation_elu</code> function to ensure a slope larger
than one for positive inputs.</p>
<p>The values of <code>alpha</code> and <code>scale</code> are
chosen so that the mean and variance of the inputs are preserved
between two consecutive layers as long as the weights are initialized
correctly (see <code><a href="initializer_lecun_normal.html">initializer_lecun_normal()</a></code>)
and the number of input units is "large enough"
(see reference paper for more information).</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">activation_selu</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-x">x<a class="anchor" aria-label="anchor" href="#arg-x"></a></dt>
<dd><p>Input tensor.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>A tensor, the result from applying the activation to the input tensor <code>x</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a></h2>

<ul><li><p>To be used together with
<code><a href="initializer_lecun_normal.html">initializer_lecun_normal()</a></code>.</p></li>
<li><p>To be used together with the dropout variant
<code><a href="layer_alpha_dropout.html">layer_alpha_dropout()</a></code> (legacy, depracated).</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="reference">Reference<a class="anchor" aria-label="anchor" href="#reference"></a></h2>

<ul><li><p><a href="https://arxiv.org/abs/1706.02515" class="external-link">Klambauer et al., 2017</a></p></li>
</ul></div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><a href="https://keras.io/api/layers/activations#selu-function" class="external-link">https://keras.io/api/layers/activations#selu-function</a></p></li>
</ul><p>Other activations: <br><code><a href="activation_elu.html">activation_elu</a>()</code> <br><code><a href="activation_exponential.html">activation_exponential</a>()</code> <br><code><a href="activation_gelu.html">activation_gelu</a>()</code> <br><code><a href="activation_hard_sigmoid.html">activation_hard_sigmoid</a>()</code> <br><code><a href="activation_leaky_relu.html">activation_leaky_relu</a>()</code> <br><code><a href="activation_linear.html">activation_linear</a>()</code> <br><code><a href="activation_log_softmax.html">activation_log_softmax</a>()</code> <br><code><a href="activation_mish.html">activation_mish</a>()</code> <br><code><a href="activation_relu.html">activation_relu</a>()</code> <br><code><a href="activation_relu6.html">activation_relu6</a>()</code> <br><code><a href="activation_sigmoid.html">activation_sigmoid</a>()</code> <br><code><a href="activation_silu.html">activation_silu</a>()</code> <br><code><a href="activation_softmax.html">activation_softmax</a>()</code> <br><code><a href="activation_softplus.html">activation_softplus</a>()</code> <br><code><a href="activation_softsign.html">activation_softsign</a>()</code> <br><code><a href="activation_tanh.html">activation_tanh</a>()</code> <br></p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer></div>





  </body></html>

