<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="description" content="See Loshchilov &amp;amp; Hutter, ICLR2016,
SGDR: Stochastic Gradient Descent with Warm Restarts.
When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a step value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.
The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.
The learning rate multiplier first decays
from 1 to alpha for first_decay_steps steps. Then, a warm
restart is performed. Each new warm restart runs for t_mul times more
steps and with m_mul times initial learning rate as the new learning rate.
Example usage:
first_decay_steps &amp;lt;- 1000
lr_decayed_fn &amp;lt;- learning_rate_schedule_cosine_decay_restarts(
        0.001,
        first_decay_steps)

You can pass this schedule directly into a optimizer
as the learning rate. The learning rate schedule is also serializable and
deserializable using keras$optimizers$schedules$serialize and
keras$optimizers$schedules$deserialize."><title>A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts • keras3</title><!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png"><link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png"><link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png"><link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet"><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="A LearningRateSchedule that uses a cosine decay schedule with restarts. — learning_rate_schedule_cosine_decay_restarts"><meta property="og:description" content="See Loshchilov &amp;amp; Hutter, ICLR2016,
SGDR: Stochastic Gradient Descent with Warm Restarts.
When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a step value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.
The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.
The learning rate multiplier first decays
from 1 to alpha for first_decay_steps steps. Then, a warm
restart is performed. Each new warm restart runs for t_mul times more
steps and with m_mul times initial learning rate as the new learning rate.
Example usage:
first_decay_steps &amp;lt;- 1000
lr_decayed_fn &amp;lt;- learning_rate_schedule_cosine_decay_restarts(
        0.001,
        first_decay_steps)

You can pass this schedule directly into a optimizer
as the learning rate. The learning rate schedule is also serializable and
deserializable using keras$optimizers$schedules$serialize and
keras$optimizers$schedules$deserialize."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item">
  <a class="nav-link" href="../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../articles/examples/index.html">Examples</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">News</a>
</li>
      </ul><form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off"></form>

      <ul class="navbar-nav"><li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div>

    
  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>A <code>LearningRateSchedule</code> that uses a cosine decay schedule with restarts.</h1>
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/R/optimizers-schedules.R" class="external-link"><code>R/optimizers-schedules.R</code></a></small>
      <div class="d-none name"><code>learning_rate_schedule_cosine_decay_restarts.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>See <a href="https://arxiv.org/abs/1608.03983" class="external-link">Loshchilov &amp; Hutter, ICLR2016</a>,
SGDR: Stochastic Gradient Descent with Warm Restarts.</p>
<p>When training a model, it is often useful to lower the learning rate as
the training progresses. This schedule applies a cosine decay function with
restarts to an optimizer step, given a provided initial learning rate.
It requires a <code>step</code> value to compute the decayed learning rate. You can
just pass a backend variable that you increment at each training step.</p>
<p>The schedule is a 1-arg callable that produces a decayed learning
rate when passed the current optimizer step. This can be useful for changing
the learning rate value across different invocations of optimizer functions.</p>
<p>The learning rate multiplier first decays
from 1 to <code>alpha</code> for <code>first_decay_steps</code> steps. Then, a warm
restart is performed. Each new warm restart runs for <code>t_mul</code> times more
steps and with <code>m_mul</code> times initial learning rate as the new learning rate.</p>
<p>Example usage:</p>
<p></p><div class="sourceCode r"><pre><code><span><span class="va">first_decay_steps</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">lr_decayed_fn</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/learning_rate_schedule_cosine_decay_restarts.html">learning_rate_schedule_cosine_decay_restarts</a></span><span class="op">(</span></span>
<span>        <span class="fl">0.001</span>,</span>
<span>        <span class="va">first_decay_steps</span><span class="op">)</span></span></code></pre><p></p></div>
<p>You can pass this schedule directly into a <code>optimizer</code>
as the learning rate. The learning rate schedule is also serializable and
deserializable using <code>keras$optimizers$schedules$serialize</code> and
<code>keras$optimizers$schedules$deserialize</code>.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">learning_rate_schedule_cosine_decay_restarts</span><span class="op">(</span></span>
<span>  <span class="va">initial_learning_rate</span>,</span>
<span>  <span class="va">first_decay_steps</span>,</span>
<span>  t_mul <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  m_mul <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  name <span class="op">=</span> <span class="st">"SGDRDecay"</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>
    <dl><dt>initial_learning_rate</dt>
<dd><p>A float. The initial learning rate.</p></dd>


<dt>first_decay_steps</dt>
<dd><p>An integer. Number of steps to decay over.</p></dd>


<dt>t_mul</dt>
<dd><p>A float. Used to derive the number of iterations in
the i-th period.</p></dd>


<dt>m_mul</dt>
<dd><p>A float. Used to derive the initial learning rate of
the i-th period.</p></dd>


<dt>alpha</dt>
<dd><p>A float. Minimum learning rate value as a fraction of
the <code>initial_learning_rate</code>.</p></dd>


<dt>name</dt>
<dd><p>String. Optional name of the operation. Defaults to
<code>"SGDRDecay"</code>.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    

<p>A 1-arg callable learning rate schedule that takes the current optimizer
step and outputs the decayed learning rate, a scalar tensor of the
same type as <code>initial_learning_rate</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="see-also">See also<a class="anchor" aria-label="anchor" href="#see-also"></a></h2>
    <div class="dont-index">
<ul><li><p><a href="https:/keras.io/api/optimizers/learning_rate_schedules/cosine_decay_restarts#cosinedecayrestarts-class">https:/keras.io/api/optimizers/learning_rate_schedules/cosine_decay_restarts#cosinedecayrestarts-class</a></p></li>
<li><p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecayRestarts" class="external-link">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecayRestarts</a></p></li>
</ul><p>Other learning rate schedule optimizers: 
<code><a href="learning_rate_schedule_cosine_decay.html">learning_rate_schedule_cosine_decay</a>()</code>,
<code><a href="learning_rate_schedule_exponential_decay.html">learning_rate_schedule_exponential_decay</a>()</code>,
<code><a href="learning_rate_schedule_inverse_time_decay.html">learning_rate_schedule_inverse_time_decay</a>()</code>,
<code><a href="learning_rate_schedule_piecewise_constant_decay.html">learning_rate_schedule_piecewise_constant_decay</a>()</code>,
<code><a href="learning_rate_schedule_polynomial_decay.html">learning_rate_schedule_polynomial_decay</a>()</code></p>
<p>Other schedule optimizers: 
<code><a href="learning_rate_schedule_cosine_decay.html">learning_rate_schedule_cosine_decay</a>()</code>,
<code><a href="learning_rate_schedule_exponential_decay.html">learning_rate_schedule_exponential_decay</a>()</code>,
<code><a href="learning_rate_schedule_inverse_time_decay.html">learning_rate_schedule_inverse_time_decay</a>()</code>,
<code><a href="learning_rate_schedule_piecewise_constant_decay.html">learning_rate_schedule_piecewise_constant_decay</a>()</code>,
<code><a href="learning_rate_schedule_polynomial_decay.html">learning_rate_schedule_polynomial_decay</a>()</code></p></div>
    </div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p></p><p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer></div>

  

  

  </body></html>

