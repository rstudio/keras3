<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Complete guide to using &amp; customizing RNN layers.">
<title>Working with RNNs • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Working with RNNs">
<meta property="og:description" content="Complete guide to using &amp; customizing RNN layers.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Working with RNNs</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/guides/working_with_rnns.Rmd" class="external-link"><code>vignettes/guides/working_with_rnns.Rmd</code></a></small>
      <div class="d-none name"><code>working_with_rnns.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Recurrent neural networks (RNN) are a class of neural networks that
is powerful for modeling sequence data such as time series or natural
language.</p>
<p>Schematically, a RNN layer uses a <code>for</code> loop to iterate
over the timesteps of a sequence, while maintaining an internal state
that encodes information about the timesteps it has seen so far.</p>
<p>The Keras RNN API is designed with a focus on:</p>
<ul>
<li><p><strong>Ease of use</strong>: the built-in
<code>keras.layers.RNN</code>, <code>keras.layers.LSTM</code>,
<code>keras.layers.GRU</code> layers enable you to quickly build
recurrent models without having to make difficult configuration
choices.</p></li>
<li><p><strong>Ease of customization</strong>: You can also define your
own RNN cell layer (the inner part of the <code>for</code> loop) with
custom behavior, and use it with the generic
<code>keras.layers.RNN</code> layer (the <code>for</code> loop itself).
This allows you to quickly prototype different research ideas in a
flexible way with minimal code.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="built-in-rnn-layers-a-simple-example">Built-in RNN layers: a simple example<a class="anchor" aria-label="anchor" href="#built-in-rnn-layers-a-simple-example"></a>
</h2>
<p>There are three built-in RNN layers in Keras:</p>
<ol style="list-style-type: decimal">
<li><p><code>keras.layers.SimpleRNN</code>, a fully-connected RNN where
the output from previous timestep is to be fed to next
timestep.</p></li>
<li><p><code>keras.layers.GRU</code>, first proposed in <a href="https://arxiv.org/abs/1406.1078" class="external-link">Cho et al., 2014</a>.</p></li>
<li><p><code>keras.layers.LSTM</code>, first proposed in <a href="https://www.bioinf.jku.at/publications/older/2604.pdf" class="external-link">Hochreiter
&amp; Schmidhuber, 1997</a>.</p></li>
</ol>
<p>In early 2015, Keras had the first reusable open-source Python
implementations of LSTM and GRU.</p>
<p>Here is a simple example of a <code>Sequential</code> model that
processes sequences of integers, embeds each integer into a
64-dimensional vector, then processes the sequence of vectors using a
<code>LSTM</code> layer.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential()</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># Add an Embedding layer expecting input vocab of size 1000, and</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># output embedding dimension of size 64.</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>model.add(layers.Embedding(input_dim<span class="op">=</span><span class="dv">1000</span>, output_dim<span class="op">=</span><span class="dv">64</span>))</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># Add a LSTM layer with 128 internal units.</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>model.add(layers.LSTM(<span class="dv">128</span>))</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co"># Add a Dense layer with 10 units.</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">10</span>))</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>Built-in RNNs support a number of useful features:</p>
<ul>
<li>Recurrent dropout, via the <code>dropout</code> and
<code>recurrent_dropout</code> arguments</li>
<li>Ability to process an input sequence in reverse, via the
<code>go_backwards</code> argument</li>
<li>Loop unrolling (which can lead to a large speedup when processing
short sequences on CPU), via the <code>unroll</code> argument</li>
<li>…and more.</li>
</ul>
<p>For more information, see the <a href="https://keras.io/api/layers/recurrent_layers/" class="external-link">RNN API
documentation</a>.</p>
</div>
<div class="section level2">
<h2 id="outputs-and-states">Outputs and states<a class="anchor" aria-label="anchor" href="#outputs-and-states"></a>
</h2>
<p>By default, the output of a RNN layer contains a single vector per
sample. This vector is the RNN cell output corresponding to the last
timestep, containing information about the entire input sequence. The
shape of this output is <code>(batch_size, units)</code> where
<code>units</code> corresponds to the <code>units</code> argument passed
to the layer’s constructor.</p>
<p>A RNN layer can also return the entire sequence of outputs for each
sample (one vector per timestep per sample), if you set
<code>return_sequences=True</code>. The shape of this output is
<code>(batch_size, timesteps, units)</code>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential()</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>model.add(layers.Embedding(input_dim<span class="op">=</span><span class="dv">1000</span>, output_dim<span class="op">=</span><span class="dv">64</span>))</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>model.add(layers.GRU(<span class="dv">256</span>, return_sequences<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co"># The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>model.add(layers.SimpleRNN(<span class="dv">128</span>))</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">10</span>))</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>In addition, a RNN layer can return its final internal state(s). The
returned states can be used to resume the RNN execution later, or <a href="https://arxiv.org/abs/1409.3215" class="external-link">to initialize another RNN</a>.
This setting is commonly used in the encoder-decoder
sequence-to-sequence model, where the encoder final state is used as the
initial state of the decoder.</p>
<p>To configure a RNN layer to return its internal state, set the
<code>return_state</code> parameter to <code>True</code> when creating
the layer. Note that <code>LSTM</code> has 2 state tensors, but
<code>GRU</code> only has one.</p>
<p>To configure the initial state of the layer, just call the layer with
additional keyword argument <code>initial_state</code>. Note that the
shape of the state needs to match the unit size of the layer, like in
the example below.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>encoder_vocab <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>decoder_vocab <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>encoder_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,))</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>encoder_embedded <span class="op">=</span> layers.Embedding(input_dim<span class="op">=</span>encoder_vocab, output_dim<span class="op">=</span><span class="dv">64</span>)(</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    encoder_input</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># Return states in addition to output</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>output, state_h, state_c <span class="op">=</span> layers.LSTM(<span class="dv">64</span>, return_state<span class="op">=</span><span class="va">True</span>, name<span class="op">=</span><span class="st">"encoder"</span>)(</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    encoder_embedded</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>encoder_state <span class="op">=</span> [state_h, state_c]</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>decoder_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="va">None</span>,))</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>decoder_embedded <span class="op">=</span> layers.Embedding(input_dim<span class="op">=</span>decoder_vocab, output_dim<span class="op">=</span><span class="dv">64</span>)(</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    decoder_input</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="co"># Pass the 2 states to a new LSTM layer, as initial state</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>decoder_output <span class="op">=</span> layers.LSTM(<span class="dv">64</span>, name<span class="op">=</span><span class="st">"decoder"</span>)(</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    decoder_embedded, initial_state<span class="op">=</span>encoder_state</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>)</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">10</span>)(decoder_output)</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>model <span class="op">=</span> keras.Model([encoder_input, decoder_input], output)</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>model.summary()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="rnn-layers-and-rnn-cells">RNN layers and RNN cells<a class="anchor" aria-label="anchor" href="#rnn-layers-and-rnn-cells"></a>
</h2>
<p>In addition to the built-in RNN layers, the RNN API also provides
cell-level APIs. Unlike RNN layers, which processes whole batches of
input sequences, the RNN cell only processes a single timestep.</p>
<p>The cell is the inside of the <code>for</code> loop of a RNN layer.
Wrapping a cell inside a <code>keras.layers.RNN</code> layer gives you a
layer capable of processing batches of sequences,
e.g. <code>RNN(LSTMCell(10))</code>.</p>
<p>Mathematically, <code>RNN(LSTMCell(10))</code> produces the same
result as <code>LSTM(10)</code>. In fact, the implementation of this
layer in TF v1.x was just creating the corresponding RNN cell and
wrapping it in a RNN layer. However using the built-in <code>GRU</code>
and <code>LSTM</code> layers enable the use of CuDNN and you may see
better performance.</p>
<p>There are three built-in RNN cells, each of them corresponding to the
matching RNN layer.</p>
<ul>
<li><p><code>keras.layers.SimpleRNNCell</code> corresponds to the
<code>SimpleRNN</code> layer.</p></li>
<li><p><code>keras.layers.GRUCell</code> corresponds to the
<code>GRU</code> layer.</p></li>
<li><p><code>keras.layers.LSTMCell</code> corresponds to the
<code>LSTM</code> layer.</p></li>
</ul>
<p>The cell abstraction, together with the generic
<code>keras.layers.RNN</code> class, make it very easy to implement
custom RNN architectures for your research.</p>
</div>
<div class="section level2">
<h2 id="cross-batch-statefulness">Cross-batch statefulness<a class="anchor" aria-label="anchor" href="#cross-batch-statefulness"></a>
</h2>
<p>When processing very long sequences (possibly infinite), you may want
to use the pattern of <strong>cross-batch statefulness</strong>.</p>
<p>Normally, the internal state of a RNN layer is reset every time it
sees a new batch (i.e. every sample seen by the layer is assumed to be
independent of the past). The layer will only maintain a state while
processing a given sample.</p>
<p>If you have very long sequences though, it is useful to break them
into shorter sequences, and to feed these shorter sequences sequentially
into a RNN layer without resetting the layer’s state. That way, the
layer can retain information about the entirety of the sequence, even
though it’s only seeing one sub-sequence at a time.</p>
<p>You can do this by setting <code>stateful=True</code> in the
constructor.</p>
<p>If you have a sequence <code>s = [t0, t1, ... t1546, t1547]</code>,
you would split it into e.g.</p>
<pre><code>s1 = [t0, t1, ... t100]
s2 = [t101, ... t201]
...
s16 = [t1501, ... t1547]</code></pre>
<p>Then you would process it via:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>lstm_layer <span class="op">=</span> layers.LSTM(<span class="dv">64</span>, stateful<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="cf">for</span> s <span class="kw">in</span> sub_sequences:</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>  output <span class="op">=</span> lstm_layer(s)</span></code></pre></div>
<p>When you want to clear the state, you can use
<code>layer.reset_states()</code>.</p>
<blockquote>
<p>Note: In this setup, sample <code>i</code> in a given batch is
assumed to be the continuation of sample <code>i</code> in the previous
batch. This means that all batches should contain the same number of
samples (batch size). E.g. if a batch contains
<code>[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]</code>,
the next batch should contain
<code>[sequence_A_from_t101_to_t200,  sequence_B_from_t101_to_t200]</code>.</p>
</blockquote>
<p>Here is a complete example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>paragraph1 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>paragraph2 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>paragraph3 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>lstm_layer <span class="op">=</span> layers.LSTM(<span class="dv">64</span>, stateful<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>output <span class="op">=</span> lstm_layer(paragraph1)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>output <span class="op">=</span> lstm_layer(paragraph2)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>output <span class="op">=</span> lstm_layer(paragraph3)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co"># reset_states() will reset the cached state to the original initial_state.</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="co"># If no initial_state was provided, zero-states will be used by default.</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>lstm_layer.reset_states()</span></code></pre></div>
<div class="section level3">
<h3 id="rnn-state-reuse">RNN State Reuse<a class="anchor" aria-label="anchor" href="#rnn-state-reuse"></a>
</h3>
<p><a id="rnn_state_reuse"></a></p>
<p>The recorded states of the RNN layer are not included in the
<code>layer.weights()</code>. If you would like to reuse the state from
a RNN layer, you can retrieve the states value by
<code>layer.states</code> and use it as the initial state for a new
layer via the Keras functional API like
<code>new_layer(inputs, initial_state=layer.states)</code>, or model
subclassing.</p>
<p>Please also note that sequential model might not be used in this case
since it only supports layers with single input and output, the extra
input of initial state makes it impossible to use here.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>paragraph1 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>paragraph2 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>paragraph3 <span class="op">=</span> np.random.random((<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">50</span>)).astype(np.float32)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>lstm_layer <span class="op">=</span> layers.LSTM(<span class="dv">64</span>, stateful<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>output <span class="op">=</span> lstm_layer(paragraph1)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>output <span class="op">=</span> lstm_layer(paragraph2)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>existing_state <span class="op">=</span> lstm_layer.states</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>new_lstm_layer <span class="op">=</span> layers.LSTM(<span class="dv">64</span>)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>new_output <span class="op">=</span> new_lstm_layer(paragraph3, initial_state<span class="op">=</span>existing_state)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="bidirectional-rnns">Bidirectional RNNs<a class="anchor" aria-label="anchor" href="#bidirectional-rnns"></a>
</h2>
<p>For sequences other than time series (e.g. text), it is often the
case that a RNN model can perform better if it not only processes
sequence from start to end, but also backwards. For example, to predict
the next word in a sentence, it is often useful to have the context
around the word, not only just the words that come before it.</p>
<p>Keras provides an easy API for you to build such bidirectional RNNs:
the <code>keras.layers.Bidirectional</code> wrapper.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential()</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>model.add(</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    layers.Bidirectional(layers.LSTM(<span class="dv">64</span>, return_sequences<span class="op">=</span><span class="va">True</span>), input_shape<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">10</span>))</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>model.add(layers.Bidirectional(layers.LSTM(<span class="dv">32</span>)))</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>model.add(layers.Dense(<span class="dv">10</span>))</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>model.summary()</span></code></pre></div>
<p>Under the hood, <code>Bidirectional</code> will copy the RNN layer
passed in, and flip the <code>go_backwards</code> field of the newly
copied layer, so that it will process the inputs in reverse order.</p>
<p>The output of the <code>Bidirectional</code> RNN will be, by default,
the concatenation of the forward layer output and the backward layer
output. If you need a different merging behavior, e.g. concatenation,
change the <code>merge_mode</code> parameter in the
<code>Bidirectional</code> wrapper constructor. For more details about
<code>Bidirectional</code>, please check <a href="https://keras.io/api/layers/recurrent_layers/bidirectional/" class="external-link">the
API docs</a>.</p>
</div>
<div class="section level2">
<h2 id="performance-optimization-and-cudnn-kernels">Performance optimization and CuDNN kernels<a class="anchor" aria-label="anchor" href="#performance-optimization-and-cudnn-kernels"></a>
</h2>
<p>In TensorFlow 2.0, the built-in LSTM and GRU layers have been updated
to leverage CuDNN kernels by default when a GPU is available. With this
change, the prior <code>keras.layers.CuDNNLSTM/CuDNNGRU</code> layers
have been deprecated, and you can build your model without worrying
about the hardware it will run on.</p>
<p>Since the CuDNN kernel is built with certain assumptions, this means
the layer <strong>will not be able to use the CuDNN kernel if you change
the defaults of the built-in LSTM or GRU layers</strong>. E.g.:</p>
<ul>
<li>Changing the <code>activation</code> function from <code>tanh</code>
to something else.</li>
<li>Changing the <code>recurrent_activation</code> function from
<code>sigmoid</code> to something else.</li>
<li>Using <code>recurrent_dropout</code> &gt; 0.</li>
<li>Setting <code>unroll</code> to True, which forces LSTM/GRU to
decompose the inner <code>tf.while_loop</code> into an unrolled
<code>for</code> loop.</li>
<li>Setting <code>use_bias</code> to False.</li>
<li>Using masking when the input data is not strictly right padded (if
the mask corresponds to strictly right padded data, CuDNN can still be
used. This is the most common case).</li>
</ul>
<p>For the detailed list of constraints, please see the documentation
for the <a href="https://keras.io/api/layers/recurrent_layers/lstm/" class="external-link">LSTM</a> and
<a href="https://keras.io/api/layers/recurrent_layers/gru/" class="external-link">GRU</a>
layers.</p>
<div class="section level3">
<h3 id="using-cudnn-kernels-when-available">Using CuDNN kernels when available<a class="anchor" aria-label="anchor" href="#using-cudnn-kernels-when-available"></a>
</h3>
<p>Let’s build a simple LSTM model to demonstrate the performance
difference.</p>
<p>We’ll use as input sequences the sequence of rows of MNIST digits
(treating each row of pixels as a timestep), and we’ll predict the
digit’s label.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># Each MNIST image batch is a tensor of shape (batch_size, 28, 28).</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co"># Each input sequence will be of size (28, 28) (height is treated like time).</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>input_dim <span class="op">=</span> <span class="dv">28</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>units <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">10</span>  <span class="co"># labels are from 0 to 9</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="co"># Build the RNN model</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="kw">def</span> build_model(allow_cudnn_kernel<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    <span class="co"># CuDNN is only available at the layer level, and not at the cell level.</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    <span class="co"># This means `LSTM(units)` will use the CuDNN kernel,</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    <span class="co"># while RNN(LSTMCell(units)) will run on non-CuDNN kernel.</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>    <span class="cf">if</span> allow_cudnn_kernel:</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>        <span class="co"># The LSTM layer with default options uses CuDNN.</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>        lstm_layer <span class="op">=</span> keras.layers.LSTM(units, input_shape<span class="op">=</span>(<span class="va">None</span>, input_dim))</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>        <span class="co"># Wrapping a LSTMCell in a RNN layer will not use CuDNN.</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>        lstm_layer <span class="op">=</span> keras.layers.RNN(</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>            keras.layers.LSTMCell(units), input_shape<span class="op">=</span>(<span class="va">None</span>, input_dim)</span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>        )</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>    model <span class="op">=</span> keras.models.Sequential(</span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>        [</span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>            lstm_layer,</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>            keras.layers.BatchNormalization(),</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>            keras.layers.Dense(output_size),</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>        ]</span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>    )</span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
<p>Let’s load the MNIST dataset:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>mnist <span class="op">=</span> keras.datasets.mnist</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>x_train, x_test <span class="op">=</span> x_train <span class="op">/</span> <span class="fl">255.0</span>, x_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>sample, sample_label <span class="op">=</span> x_train[<span class="dv">0</span>], y_train[<span class="dv">0</span>]</span></code></pre></div>
<p>Let’s create a model instance and train it.</p>
<p>We choose <code>sparse_categorical_crossentropy</code> as the loss
function for the model. The output of the model has shape of
<code>[batch_size, 10]</code>. The target for the model is an integer
vector, each of the integer is in the range of 0 to 9.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>model <span class="op">=</span> build_model(allow_cudnn_kernel<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"sgd"</span>,</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>model.fit(</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>    x_train, y_train, validation_data<span class="op">=</span>(x_test, y_test), batch_size<span class="op">=</span>batch_size, epochs<span class="op">=</span><span class="dv">1</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>)</span></code></pre></div>
<p>Now, let’s compare to a model that does not use the CuDNN kernel:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>noncudnn_model <span class="op">=</span> build_model(allow_cudnn_kernel<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>noncudnn_model.set_weights(model.get_weights())</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>noncudnn_model.<span class="bu">compile</span>(</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"sgd"</span>,</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>noncudnn_model.fit(</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    x_train, y_train, validation_data<span class="op">=</span>(x_test, y_test), batch_size<span class="op">=</span>batch_size, epochs<span class="op">=</span><span class="dv">1</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>)</span></code></pre></div>
<p>When running on a machine with a NVIDIA GPU and CuDNN installed, the
model built with CuDNN is much faster to train compared to the model
that uses the regular TensorFlow kernel.</p>
<p>The same CuDNN-enabled model can also be used to run inference in a
CPU-only environment. The <code>tf.device</code> annotation below is
just forcing the device placement. The model will run on CPU by default
if no GPU is available.</p>
<p>You simply don’t have to worry about the hardware you’re running on
anymore. Isn’t that pretty cool?</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="cf">with</span> tf.device(<span class="st">"CPU:0"</span>):</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    cpu_model <span class="op">=</span> build_model(allow_cudnn_kernel<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    cpu_model.set_weights(model.get_weights())</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    result <span class="op">=</span> tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, <span class="dv">0</span>)), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>        <span class="st">"Predicted result is: </span><span class="sc">%s</span><span class="st">, target result is: </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> (result.numpy(), sample_label)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    )</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    plt.imshow(sample, cmap<span class="op">=</span>plt.get_cmap(<span class="st">"gray"</span>))</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="rnns-with-listdict-inputs-or-nested-inputs">RNNs with list/dict inputs, or nested inputs<a class="anchor" aria-label="anchor" href="#rnns-with-listdict-inputs-or-nested-inputs"></a>
</h2>
<p>Nested structures allow implementers to include more information
within a single timestep. For example, a video frame could have audio
and video input at the same time. The data shape in this case could
be:</p>
<p><code>[batch, timestep, {"video": [height, width, channel], "audio": [frequency]}]</code></p>
<p>In another example, handwriting data could have both coordinates x
and y for the current position of the pen, as well as pressure
information. So the data representation could be:</p>
<p><code>[batch, timestep, {"location": [x, y], "pressure": [force]}]</code></p>
<p>The following code provides an example of how to build a custom RNN
cell that accepts such structured inputs.</p>
<div class="section level3">
<h3 id="define-a-custom-cell-that-supports-nested-inputoutput">Define a custom cell that supports nested input/output<a class="anchor" aria-label="anchor" href="#define-a-custom-cell-that-supports-nested-inputoutput"></a>
</h3>
<p>See <a href="/guides/making_new_layers_and_models_via_subclassing/">Making new
Layers &amp; Models via subclassing</a> for details on writing your own
layers.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="at">@keras.saving.register_keras_serializable</span>()</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="kw">class</span> NestedCell(keras.layers.Layer):</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, unit_1, unit_2, unit_3, <span class="op">**</span>kwargs):</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>        <span class="va">self</span>.unit_1 <span class="op">=</span> unit_1</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>        <span class="va">self</span>.unit_2 <span class="op">=</span> unit_2</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>        <span class="va">self</span>.unit_3 <span class="op">=</span> unit_3</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>        <span class="va">self</span>.state_size <span class="op">=</span> [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>        <span class="va">self</span>.output_size <span class="op">=</span> [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shapes):</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>        <span class="co"># expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]</span></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>        i1 <span class="op">=</span> input_shapes[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>        i2 <span class="op">=</span> input_shapes[<span class="dv">1</span>][<span class="dv">1</span>]</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>        i3 <span class="op">=</span> input_shapes[<span class="dv">1</span>][<span class="dv">2</span>]</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a>        <span class="va">self</span>.kernel_1 <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>            shape<span class="op">=</span>(i1, <span class="va">self</span>.unit_1), initializer<span class="op">=</span><span class="st">"uniform"</span>, name<span class="op">=</span><span class="st">"kernel_1"</span></span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>        )</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>        <span class="va">self</span>.kernel_2_3 <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>            shape<span class="op">=</span>(i2, i3, <span class="va">self</span>.unit_2, <span class="va">self</span>.unit_3),</span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"uniform"</span>,</span>
<span id="cb15-23"><a href="#cb15-23" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"kernel_2_3"</span>,</span>
<span id="cb15-24"><a href="#cb15-24" tabindex="-1"></a>        )</span>
<span id="cb15-25"><a href="#cb15-25" tabindex="-1"></a></span>
<span id="cb15-26"><a href="#cb15-26" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, states):</span>
<span id="cb15-27"><a href="#cb15-27" tabindex="-1"></a>        <span class="co"># inputs should be in [(batch, input_1), (batch, input_2, input_3)]</span></span>
<span id="cb15-28"><a href="#cb15-28" tabindex="-1"></a>        <span class="co"># state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]</span></span>
<span id="cb15-29"><a href="#cb15-29" tabindex="-1"></a>        input_1, input_2 <span class="op">=</span> tf.nest.flatten(inputs)</span>
<span id="cb15-30"><a href="#cb15-30" tabindex="-1"></a>        s1, s2 <span class="op">=</span> states</span>
<span id="cb15-31"><a href="#cb15-31" tabindex="-1"></a></span>
<span id="cb15-32"><a href="#cb15-32" tabindex="-1"></a>        output_1 <span class="op">=</span> tf.matmul(input_1, <span class="va">self</span>.kernel_1)</span>
<span id="cb15-33"><a href="#cb15-33" tabindex="-1"></a>        output_2_3 <span class="op">=</span> tf.einsum(<span class="st">"bij,ijkl-&gt;bkl"</span>, input_2, <span class="va">self</span>.kernel_2_3)</span>
<span id="cb15-34"><a href="#cb15-34" tabindex="-1"></a>        state_1 <span class="op">=</span> s1 <span class="op">+</span> output_1</span>
<span id="cb15-35"><a href="#cb15-35" tabindex="-1"></a>        state_2_3 <span class="op">=</span> s2 <span class="op">+</span> output_2_3</span>
<span id="cb15-36"><a href="#cb15-36" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" tabindex="-1"></a>        output <span class="op">=</span> (output_1, output_2_3)</span>
<span id="cb15-38"><a href="#cb15-38" tabindex="-1"></a>        new_states <span class="op">=</span> (state_1, state_2_3)</span>
<span id="cb15-39"><a href="#cb15-39" tabindex="-1"></a></span>
<span id="cb15-40"><a href="#cb15-40" tabindex="-1"></a>        <span class="cf">return</span> output, new_states</span>
<span id="cb15-41"><a href="#cb15-41" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb15-43"><a href="#cb15-43" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"unit_1"</span>: <span class="va">self</span>.unit_1, <span class="st">"unit_2"</span>: unit_2, <span class="st">"unit_3"</span>: <span class="va">self</span>.unit_3}</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="build-a-rnn-model-with-nested-inputoutput">Build a RNN model with nested input/output<a class="anchor" aria-label="anchor" href="#build-a-rnn-model-with-nested-inputoutput"></a>
</h3>
<p>Let’s build a Keras model that uses a <code>keras.layers.RNN</code>
layer and the custom cell we just defined.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>unit_1 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>unit_2 <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>unit_3 <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>i1 <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>i2 <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>i3 <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>num_batches <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>timestep <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>cell <span class="op">=</span> NestedCell(unit_1, unit_2, unit_3)</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>rnn <span class="op">=</span> keras.layers.RNN(cell)</span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>input_1 <span class="op">=</span> keras.Input((<span class="va">None</span>, i1))</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>input_2 <span class="op">=</span> keras.Input((<span class="va">None</span>, i2, i3))</span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>outputs <span class="op">=</span> rnn((input_1, input_2))</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a>model <span class="op">=</span> keras.models.Model([input_1, input_2], outputs)</span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"adam"</span>, loss<span class="op">=</span><span class="st">"mse"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="train-the-model-with-randomly-generated-data">Train the model with randomly generated data<a class="anchor" aria-label="anchor" href="#train-the-model-with-randomly-generated-data"></a>
</h3>
<p>Since there isn’t a good candidate dataset for this model, we use
random Numpy data for demonstration.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>input_1_data <span class="op">=</span> np.random.random((batch_size <span class="op">*</span> num_batches, timestep, i1))</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>input_2_data <span class="op">=</span> np.random.random((batch_size <span class="op">*</span> num_batches, timestep, i2, i3))</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>target_1_data <span class="op">=</span> np.random.random((batch_size <span class="op">*</span> num_batches, unit_1))</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>target_2_data <span class="op">=</span> np.random.random((batch_size <span class="op">*</span> num_batches, unit_2, unit_3))</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>input_data <span class="op">=</span> [input_1_data, input_2_data]</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>target_data <span class="op">=</span> [target_1_data, target_2_data]</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>model.fit(input_data, target_data, batch_size<span class="op">=</span>batch_size)</span></code></pre></div>
<p>With the Keras <code>keras.layers.RNN</code> layer, You are only
expected to define the math logic for individual step within the
sequence, and the <code>keras.layers.RNN</code> layer will handle the
sequence iteration for you. It’s an incredibly powerful way to quickly
prototype new kinds of RNNs (e.g. a LSTM variant).</p>
<p>For more details, please visit the <a href="https://keras.io/api/layers/recurrent_layers/rnn/" class="external-link">API
docs</a>.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
