<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Text classification from scratch • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Text classification from scratch">
<meta property="og:description" content="Text sentiment classification starting from raw text files.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Text classification from scratch</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/text_classification_from_scratch.Rmd" class="external-link"><code>vignettes/examples/text_classification_from_scratch.Rmd</code></a></small>
      <div class="hidden name"><code>text_classification_from_scratch.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example shows how to do text classification starting from raw
text (as a set of text files on disk). We demonstrate the workflow on
the IMDB sentiment classification dataset (unprocessed version). We use
the <code>TextVectorization</code> layer for word splitting &amp;
indexing.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-data-imdb-movie-review-sentiment-classification">Load the data: IMDB movie review sentiment classification<a class="anchor" aria-label="anchor" href="#load-the-data-imdb-movie-review-sentiment-classification"></a>
</h2>
<p>Let’s download the data and inspect its structure.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>fpath <span class="op">=</span> keras.utils.get_file(</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    origin<span class="op">=</span><span class="st">"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>dirpath <span class="op">=</span> Path(fpath).parent.absolute()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>os.system(<span class="ss">f"tar -xf </span><span class="sc">{</span>fpath<span class="sc">}</span><span class="ss"> -C </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>The <code>aclImdb</code> folder contains a <code>train</code> and
<code>test</code> subfolder:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>os.system(<span class="ss">f"ls </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/test"</span>)</span></code></pre></div>
<p>The <code>aclImdb/train/pos</code> and <code>aclImdb/train/neg</code>
folders contain text files, each of which represents one review (either
positive or negative):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>os.system(<span class="ss">f"cat </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train/pos/6248_7.txt"</span>)</span></code></pre></div>
<p>We are only interested in the <code>pos</code> and <code>neg</code>
subfolders, so let’s delete the rest:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>os.system(<span class="ss">f"rm -r </span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train/unsup"</span>)</span></code></pre></div>
<p>You can use the utility
<code>keras.utils.text_dataset_from_directory</code> to generate a
labeled <code>tf.data.Dataset</code> object from a set of text files on
disk filed into class-specific folders.</p>
<p>Let’s use it to generate the training, validation, and test datasets.
The validation and training datasets are generated from two subsets of
the <code>train</code> directory, with 20% of samples going to the
validation dataset and 80% going to the training dataset.</p>
<p>Having a validation dataset in addition to the test dataset is useful
for tuning hyperparameters, such as the model architecture, for which
the test dataset should not be used.</p>
<p>Before putting the model out into the real world however, it should
be retrained using all available training data (without creating a
validation dataset), so its performance is maximized.</p>
<p>When using the <code>validation_split</code> &amp;
<code>subset</code> arguments, make sure to either specify a random
seed, or to pass <code>shuffle=False</code>, so that the validation
&amp; training splits you get have no overlap.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>raw_train_ds, raw_val_ds <span class="op">=</span> keras.utils.text_dataset_from_directory(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/train"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    subset<span class="op">=</span><span class="st">"both"</span>,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    seed<span class="op">=</span><span class="dv">1337</span>,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>raw_test_ds <span class="op">=</span> keras.utils.text_dataset_from_directory(</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>dirpath<span class="sc">}</span><span class="ss">/aclImdb/test"</span>, batch_size<span class="op">=</span>batch_size</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_train_ds: </span><span class="sc">{</span>raw_train_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_val_ds: </span><span class="sc">{</span>raw_val_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of batches in raw_test_ds: </span><span class="sc">{</span>raw_test_ds<span class="sc">.</span>cardinality()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>Let’s preview a few samples:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># It's important to take a look at your raw data to ensure your normalization</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># and tokenization will work as expected. We can do that by taking a few</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># examples from the training set and looking at them.</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># This is one of the places where eager execution shines:</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># we can just evaluate these tensors using .numpy()</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co"># instead of needing to evaluate them in a Session/Graph context.</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="cf">for</span> text_batch, label_batch <span class="kw">in</span> raw_train_ds.take(<span class="dv">1</span>):</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="bu">print</span>(text_batch.numpy()[i])</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        <span class="bu">print</span>(label_batch.numpy()[i])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<p>In particular, we remove <code>&lt;br /&gt;</code> tags.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Having looked at our data above, we see that the raw text contains HTML break</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co"># tags of the form '&lt;br /&gt;'. These tags will not be removed by the default</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co"># standardizer (which doesn't strip HTML). Because of this, we will need to</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># create a custom standardization function.</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="kw">def</span> custom_standardization(input_data):</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    stripped_html <span class="op">=</span> tf.strings.regex_replace(lowercase, <span class="st">"&lt;br /&gt;"</span>, <span class="st">" "</span>)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    <span class="cf">return</span> tf.strings.regex_replace(</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        stripped_html, <span class="ss">f"[</span><span class="sc">{</span>re<span class="sc">.</span>escape(string.punctuation)<span class="sc">}</span><span class="ss">]"</span>, <span class="st">""</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    )</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co"># Model constants.</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>max_features <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a><span class="co"># Now that we have our custom standardization, we can instantiate our text</span></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a><span class="co"># vectorization layer. We are using this layer to normalize, split, and map</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a><span class="co"># strings to integers, so we set our 'output_mode' to 'int'.</span></span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a><span class="co"># Note that we're using the default split function,</span></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a><span class="co"># and the custom standardization defined above.</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a><span class="co"># We also set an explicit maximum sequence length, since the CNNs later in our</span></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a><span class="co"># model won't support ragged sequences.</span></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>vectorize_layer <span class="op">=</span> TextVectorization(</span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>    standardize<span class="op">=</span>custom_standardization,</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>    max_tokens<span class="op">=</span>max_features,</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>sequence_length,</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>)</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a><span class="co"># Now that the vocab layer has been created, call `adapt` on a text-only</span></span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a><span class="co"># dataset to create the vocabulary. You don't have to batch, but for very large</span></span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a><span class="co"># datasets this means you're not keeping spare copies of the dataset in memory.</span></span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a><span class="co"># Let's make a text-only dataset (no labels):</span></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a>text_ds <span class="op">=</span> raw_train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x)</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a><span class="co"># Let's call `adapt`:</span></span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>vectorize_layer.adapt(text_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="two-options-to-vectorize-the-data">Two options to vectorize the data<a class="anchor" aria-label="anchor" href="#two-options-to-vectorize-the-data"></a>
</h2>
<p>There are 2 ways we can use our text vectorization layer:</p>
<p><strong>Option 1: Make it part of the model</strong>, so as to obtain
a model that processes raw strings, like this:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>text_input <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf.string, name<span class="op">=</span><span class="st">'text'</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>x <span class="op">=</span> vectorize_layer(text_input)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>x <span class="op">=</span> layers.Embedding(max_features <span class="op">+</span> <span class="dv">1</span>, embedding_dim)(x)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>...</span></code></pre></div>
<p><strong>Option 2: Apply it to the text dataset</strong> to obtain a
dataset of word indices, then feed it into a model that expects integer
sequences as inputs.</p>
<p>An important difference between the two is that option 2 enables you
to do <strong>asynchronous CPU processing and buffering</strong> of your
data when training on GPU. So if you’re training the model on GPU, you
probably want to go with this option to get the best performance. This
is what we will do below.</p>
<p>If we were to export our model to production, we’d ship a model that
accepts raw strings as input, like in the code snippet for option 1
above. This can be done after training. We do this in the last
section.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> vectorize_text(text, label):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    text <span class="op">=</span> tf.expand_dims(text, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="cf">return</span> vectorize_layer(text), label</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># Vectorize the data.</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>train_ds <span class="op">=</span> raw_train_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>val_ds <span class="op">=</span> raw_val_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>test_ds <span class="op">=</span> raw_test_ds.<span class="bu">map</span>(vectorize_text)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="co"># Do async prefetching / buffering of the data for best performance on GPU.</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>val_ds <span class="op">=</span> val_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.cache().prefetch(buffer_size<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-a-model">Build a model<a class="anchor" aria-label="anchor" href="#build-a-model"></a>
</h2>
<p>We choose a simple 1D convnet starting with an <code>Embedding</code>
layer.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># A integer input for vocab indices.</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(sequence_length,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># Next, we add a layer to map those vocab indices into a space of dimensionality</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># 'embedding_dim'.</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Embedding(max_features, embedding_dim)(inputs)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="co"># Conv1D + global max pooling</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">7</span>, padding<span class="op">=</span><span class="st">"valid"</span>, activation<span class="op">=</span><span class="st">"relu"</span>, strides<span class="op">=</span><span class="dv">3</span>)(x)</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>x <span class="op">=</span> layers.Conv1D(<span class="dv">128</span>, <span class="dv">7</span>, padding<span class="op">=</span><span class="st">"valid"</span>, activation<span class="op">=</span><span class="st">"relu"</span>, strides<span class="op">=</span><span class="dv">3</span>)(x)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>x <span class="op">=</span> layers.GlobalMaxPooling1D()(x)</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a><span class="co"># We add a vanilla hidden layer:</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>)(x)</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="co"># We project onto a single unit output layer, and squash it with a sigmoid:</span></span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>predictions <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>, name<span class="op">=</span><span class="st">"predictions"</span>)(x)</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs, predictions)</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>model.summary()</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"adam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-the-model">Train the model<a class="anchor" aria-label="anchor" href="#train-the-model"></a>
</h2>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co"># Fit the model using the train and test datasets.</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>model.fit(train_ds, validation_data<span class="op">=</span>val_ds, epochs<span class="op">=</span>epochs)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="evaluate-the-model-on-the-test-set">Evaluate the model on the test set<a class="anchor" aria-label="anchor" href="#evaluate-the-model-on-the-test-set"></a>
</h2>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>model.evaluate(test_ds)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="make-an-end-to-end-model">Make an end-to-end model<a class="anchor" aria-label="anchor" href="#make-an-end-to-end-model"></a>
</h2>
<p>If you want to obtain a model capable of processing raw strings, you
can simply create a new model (using the weights we just trained):</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># A string input</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">"string"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co"># Turn strings into vocab indices</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>indices <span class="op">=</span> vectorize_layer(inputs)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a><span class="co"># Turn vocab indices into predictions</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>outputs <span class="op">=</span> model(indices)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="co"># Our end to end model</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>end_to_end_model <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>end_to_end_model.<span class="bu">compile</span>(</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, optimizer<span class="op">=</span><span class="st">"adam"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>]</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a><span class="co"># Test it with `raw_test_ds`, which yields raw strings</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>end_to_end_model.evaluate(raw_test_ds)</span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
