<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implement an image captioning model using a CNN and a Transformer.">
<title>Image Captioning • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Image Captioning">
<meta property="og:description" content="Implement an image captioning model using a CNN and a Transformer.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Image Captioning</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/A_K_Nain" class="external-link">A_K_Nain</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/image_captioning.Rmd" class="external-link"><code>vignettes/examples/image_captioning.Rmd</code></a></small>
      <div class="d-none name"><code>image_captioning.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> keras.applications <span class="im">import</span> efficientnet</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">111</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>tf.random.set_seed(seed)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="download-the-dataset">Download the dataset<a class="anchor" aria-label="anchor" href="#download-the-dataset"></a>
</h2>
<p>We will be using the Flickr8K dataset for this tutorial. This dataset
comprises over 8,000 images, that are each paired with five different
captions.</p>
<p>wget -q <a href="https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip" class="external-link uri">https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip</a>
wget -q <a href="https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip" class="external-link uri">https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip</a>
unzip -qq Flickr8k_Dataset.zip unzip -qq Flickr8k_text.zip rm
Flickr8k_Dataset.zip Flickr8k_text.zip</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Path to the images</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>IMAGES_PATH <span class="op">=</span> <span class="st">"Flicker8k_Dataset"</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Desired image dimensions</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> (<span class="dv">299</span>, <span class="dv">299</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># Vocabulary size</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>VOCAB_SIZE <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># Fixed length allowed for any sequence</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>SEQ_LENGTH <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co"># Dimension for the image embeddings and token embeddings</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>EMBED_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># Per-layer units in the feed-forward network</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>FF_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># Other training parameters</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>AUTOTUNE <span class="op">=</span> tf.data.AUTOTUNE</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="preparing-the-dataset">Preparing the dataset<a class="anchor" aria-label="anchor" href="#preparing-the-dataset"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> load_captions_data(filename):</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="co">"""Loads captions (text) data and maps them to corresponding images.</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co">        filename: Path to the text file containing caption data.</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co">        caption_mapping: Dictionary mapping image names and the corresponding captions</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="co">        text_data: List containing all the available captions</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename) <span class="im">as</span> caption_file:</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>        caption_data <span class="op">=</span> caption_file.readlines()</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>        caption_mapping <span class="op">=</span> {}</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>        text_data <span class="op">=</span> []</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>        images_to_skip <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> caption_data:</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>            line <span class="op">=</span> line.rstrip(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>            <span class="co"># Image name and captions are separated using a tab</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>            img_name, caption <span class="op">=</span> line.split(<span class="st">"</span><span class="ch">\t</span><span class="st">"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>            <span class="co"># Each image is repeated five times for the five different captions.</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>            <span class="co"># Each image name has a suffix `#(caption_number)`</span></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>            img_name <span class="op">=</span> img_name.split(<span class="st">"#"</span>)[<span class="dv">0</span>]</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>            img_name <span class="op">=</span> os.path.join(IMAGES_PATH, img_name.strip())</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>            <span class="co"># We will remove caption that are either too short to too long</span></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            tokens <span class="op">=</span> caption.strip().split()</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(tokens) <span class="op">&lt;</span> <span class="dv">5</span> <span class="kw">or</span> <span class="bu">len</span>(tokens) <span class="op">&gt;</span> SEQ_LENGTH:</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>                images_to_skip.add(img_name)</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>            <span class="cf">if</span> img_name.endswith(<span class="st">"jpg"</span>) <span class="kw">and</span> img_name <span class="kw">not</span> <span class="kw">in</span> images_to_skip:</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>                <span class="co"># We will add a start and an end token to each caption</span></span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>                caption <span class="op">=</span> <span class="st">"&lt;start&gt; "</span> <span class="op">+</span> caption.strip() <span class="op">+</span> <span class="st">" &lt;end&gt;"</span></span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>                text_data.append(caption)</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a>                <span class="cf">if</span> img_name <span class="kw">in</span> caption_mapping:</span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>                    caption_mapping[img_name].append(caption)</span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>                    caption_mapping[img_name] <span class="op">=</span> [caption]</span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a>        <span class="cf">for</span> img_name <span class="kw">in</span> images_to_skip:</span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>            <span class="cf">if</span> img_name <span class="kw">in</span> caption_mapping:</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>                <span class="kw">del</span> caption_mapping[img_name]</span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a>        <span class="cf">return</span> caption_mapping, text_data</span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a></span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a><span class="kw">def</span> train_val_split(caption_data, train_size<span class="op">=</span><span class="fl">0.8</span>, shuffle<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>    <span class="co">"""Split the captioning dataset into train and validation sets.</span></span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a><span class="co">        caption_data (dict): Dictionary containing the mapped caption data</span></span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a><span class="co">        train_size (float): Fraction of all the full dataset to use as training data</span></span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a><span class="co">        shuffle (bool): Whether to shuffle the dataset before splitting</span></span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a></span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a><span class="co">        Traning and validation datasets as two separated dicts</span></span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a></span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>    <span class="co"># 1. Get the list of all image names</span></span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a>    all_images <span class="op">=</span> <span class="bu">list</span>(caption_data.keys())</span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a></span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a>    <span class="co"># 2. Shuffle if necessary</span></span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a>    <span class="cf">if</span> shuffle:</span>
<span id="cb3-69"><a href="#cb3-69" tabindex="-1"></a>        np.random.shuffle(all_images)</span>
<span id="cb3-70"><a href="#cb3-70" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" tabindex="-1"></a>    <span class="co"># 3. Split into training and validation sets</span></span>
<span id="cb3-72"><a href="#cb3-72" tabindex="-1"></a>    train_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(caption_data) <span class="op">*</span> train_size)</span>
<span id="cb3-73"><a href="#cb3-73" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" tabindex="-1"></a>    training_data <span class="op">=</span> {</span>
<span id="cb3-75"><a href="#cb3-75" tabindex="-1"></a>        img_name: caption_data[img_name] <span class="cf">for</span> img_name <span class="kw">in</span> all_images[:train_size]</span>
<span id="cb3-76"><a href="#cb3-76" tabindex="-1"></a>    }</span>
<span id="cb3-77"><a href="#cb3-77" tabindex="-1"></a>    validation_data <span class="op">=</span> {</span>
<span id="cb3-78"><a href="#cb3-78" tabindex="-1"></a>        img_name: caption_data[img_name] <span class="cf">for</span> img_name <span class="kw">in</span> all_images[train_size:]</span>
<span id="cb3-79"><a href="#cb3-79" tabindex="-1"></a>    }</span>
<span id="cb3-80"><a href="#cb3-80" tabindex="-1"></a></span>
<span id="cb3-81"><a href="#cb3-81" tabindex="-1"></a>    <span class="co"># 4. Return the splits</span></span>
<span id="cb3-82"><a href="#cb3-82" tabindex="-1"></a>    <span class="cf">return</span> training_data, validation_data</span>
<span id="cb3-83"><a href="#cb3-83" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" tabindex="-1"></a></span>
<span id="cb3-85"><a href="#cb3-85" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb3-86"><a href="#cb3-86" tabindex="-1"></a>captions_mapping, text_data <span class="op">=</span> load_captions_data(<span class="st">"Flickr8k.token.txt"</span>)</span>
<span id="cb3-87"><a href="#cb3-87" tabindex="-1"></a></span>
<span id="cb3-88"><a href="#cb3-88" tabindex="-1"></a><span class="co"># Split the dataset into training and validation sets</span></span>
<span id="cb3-89"><a href="#cb3-89" tabindex="-1"></a>train_data, valid_data <span class="op">=</span> train_val_split(captions_mapping)</span>
<span id="cb3-90"><a href="#cb3-90" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of training samples: "</span>, <span class="bu">len</span>(train_data))</span>
<span id="cb3-91"><a href="#cb3-91" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of validation samples: "</span>, <span class="bu">len</span>(valid_data))</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="vectorizing-the-text-data">Vectorizing the text data<a class="anchor" aria-label="anchor" href="#vectorizing-the-text-data"></a>
</h2>
<p>We’ll use the <code>TextVectorization</code> layer to vectorize the
text data, that is to say, to turn the original strings into integer
sequences where each integer represents the index of a word in a
vocabulary. We will use a custom string standardization scheme (strip
punctuation characters except <code>&lt;</code> and <code>&gt;</code>)
and the default splitting scheme (split on whitespace).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> custom_standardization(input_string):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_string)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="cf">return</span> tf.strings.regex_replace(</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        lowercase, <span class="st">"[</span><span class="sc">%s</span><span class="st">]"</span> <span class="op">%</span> re.escape(strip_chars), <span class="st">""</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    )</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>strip_chars <span class="op">=</span> <span class="st">"!</span><span class="ch">\"</span><span class="st">#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~"</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>strip_chars <span class="op">=</span> strip_chars.replace(<span class="st">"&lt;"</span>, <span class="st">""</span>)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>strip_chars <span class="op">=</span> strip_chars.replace(<span class="st">"&gt;"</span>, <span class="st">""</span>)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>vectorization <span class="op">=</span> TextVectorization(</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    max_tokens<span class="op">=</span>VOCAB_SIZE,</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">"int"</span>,</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    output_sequence_length<span class="op">=</span>SEQ_LENGTH,</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    standardize<span class="op">=</span>custom_standardization,</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>)</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>vectorization.adapt(text_data)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="co"># Data augmentation for image data</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>image_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    [</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>        layers.RandomRotation(<span class="fl">0.2</span>),</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>        layers.RandomContrast(<span class="fl">0.3</span>),</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    ]</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-a-tf-data-dataset-pipeline-for-training">Building a <code>tf.data.Dataset</code> pipeline for training<a class="anchor" aria-label="anchor" href="#building-a-tf-data-dataset-pipeline-for-training"></a>
</h2>
<p>We will generate pairs of images and corresponding captions using a
<code>tf.data.Dataset</code> object. The pipeline consists of two
steps:</p>
<ol style="list-style-type: decimal">
<li>Read the image from the disk</li>
<li>Tokenize all the five captions corresponding to the image</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> decode_and_resize(img_path):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    img <span class="op">=</span> tf.io.read_file(img_path)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    img <span class="op">=</span> tf.image.decode_jpeg(img, channels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    img <span class="op">=</span> tf.image.resize(img, IMAGE_SIZE)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    img <span class="op">=</span> tf.image.convert_image_dtype(img, tf.float32)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="cf">return</span> img</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="kw">def</span> process_input(img_path, captions):</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>    <span class="cf">return</span> decode_and_resize(img_path), vectorization(captions)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="kw">def</span> make_dataset(images, captions):</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices((images, captions))</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.shuffle(BATCH_SIZE <span class="op">*</span> <span class="dv">8</span>)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(process_input, num_parallel_calls<span class="op">=</span>AUTOTUNE)</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    <span class="cf">return</span> dataset</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a><span class="co"># Pass the list of images and the list of corresponding captions</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>train_dataset <span class="op">=</span> make_dataset(<span class="bu">list</span>(train_data.keys()), <span class="bu">list</span>(train_data.values()))</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>valid_dataset <span class="op">=</span> make_dataset(<span class="bu">list</span>(valid_data.keys()), <span class="bu">list</span>(valid_data.values()))</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="building-the-model">Building the model<a class="anchor" aria-label="anchor" href="#building-the-model"></a>
</h2>
<p>Our image captioning architecture consists of three models:</p>
<ol style="list-style-type: decimal">
<li>A CNN: used to extract the image features</li>
<li>A TransformerEncoder: The extracted image features are then passed
to a Transformer based encoder that generates a new representation of
the inputs</li>
<li>A TransformerDecoder: This model takes the encoder output and the
text data (sequences) as inputs and tries to learn to generate the
caption.</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> get_cnn_model():</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    base_model <span class="op">=</span> efficientnet.EfficientNetB0(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        input_shape<span class="op">=</span>(<span class="op">*</span>IMAGE_SIZE, <span class="dv">3</span>),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        include_top<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        weights<span class="op">=</span><span class="st">"imagenet"</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    )</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    <span class="co"># We freeze our feature extractor</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    base_model.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    base_model_out <span class="op">=</span> base_model.output</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    base_model_out <span class="op">=</span> layers.Reshape((<span class="op">-</span><span class="dv">1</span>, base_model_out.shape[<span class="op">-</span><span class="dv">1</span>]))(</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        base_model_out</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    )</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    cnn_model <span class="op">=</span> keras.models.Model(base_model.<span class="bu">input</span>, base_model_out)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    <span class="cf">return</span> cnn_model</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a><span class="kw">class</span> TransformerEncoderBlock(layers.Layer):</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, dense_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>        <span class="va">self</span>.dense_dim <span class="op">=</span> dense_dim</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        <span class="va">self</span>.attention_1 <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim, dropout<span class="op">=</span><span class="fl">0.0</span></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        )</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>        <span class="va">self</span>.dense_1 <span class="op">=</span> layers.Dense(embed_dim, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.dense_1(inputs)</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>        attention_output_1 <span class="op">=</span> <span class="va">self</span>.attention_1(</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>            query<span class="op">=</span>inputs,</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>            value<span class="op">=</span>inputs,</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>            key<span class="op">=</span>inputs,</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>            attention_mask<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>            training<span class="op">=</span>training,</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>        )</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>        out_1 <span class="op">=</span> <span class="va">self</span>.layernorm_2(inputs <span class="op">+</span> attention_output_1)</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>        <span class="cf">return</span> out_1</span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a><span class="kw">class</span> PositionalEmbedding(layers.Layer):</span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, sequence_length, vocab_size, embed_dim, <span class="op">**</span>kwargs):</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>        <span class="va">self</span>.token_embeddings <span class="op">=</span> layers.Embedding(</span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a>            input_dim<span class="op">=</span>vocab_size, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb6-50"><a href="#cb6-50" tabindex="-1"></a>        )</span>
<span id="cb6-51"><a href="#cb6-51" tabindex="-1"></a>        <span class="va">self</span>.position_embeddings <span class="op">=</span> layers.Embedding(</span>
<span id="cb6-52"><a href="#cb6-52" tabindex="-1"></a>            input_dim<span class="op">=</span>sequence_length, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb6-53"><a href="#cb6-53" tabindex="-1"></a>        )</span>
<span id="cb6-54"><a href="#cb6-54" tabindex="-1"></a>        <span class="va">self</span>.sequence_length <span class="op">=</span> sequence_length</span>
<span id="cb6-55"><a href="#cb6-55" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb6-56"><a href="#cb6-56" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb6-57"><a href="#cb6-57" tabindex="-1"></a>        <span class="va">self</span>.embed_scale <span class="op">=</span> tf.math.sqrt(tf.cast(embed_dim, tf.float32))</span>
<span id="cb6-58"><a href="#cb6-58" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb6-60"><a href="#cb6-60" tabindex="-1"></a>        length <span class="op">=</span> tf.shape(inputs)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-61"><a href="#cb6-61" tabindex="-1"></a>        positions <span class="op">=</span> tf.<span class="bu">range</span>(start<span class="op">=</span><span class="dv">0</span>, limit<span class="op">=</span>length, delta<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-62"><a href="#cb6-62" tabindex="-1"></a>        embedded_tokens <span class="op">=</span> <span class="va">self</span>.token_embeddings(inputs)</span>
<span id="cb6-63"><a href="#cb6-63" tabindex="-1"></a>        embedded_tokens <span class="op">=</span> embedded_tokens <span class="op">*</span> <span class="va">self</span>.embed_scale</span>
<span id="cb6-64"><a href="#cb6-64" tabindex="-1"></a>        embedded_positions <span class="op">=</span> <span class="va">self</span>.position_embeddings(positions)</span>
<span id="cb6-65"><a href="#cb6-65" tabindex="-1"></a>        <span class="cf">return</span> embedded_tokens <span class="op">+</span> embedded_positions</span>
<span id="cb6-66"><a href="#cb6-66" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" tabindex="-1"></a>    <span class="kw">def</span> compute_mask(<span class="va">self</span>, inputs, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-68"><a href="#cb6-68" tabindex="-1"></a>        <span class="cf">return</span> tf.math.not_equal(inputs, <span class="dv">0</span>)</span>
<span id="cb6-69"><a href="#cb6-69" tabindex="-1"></a></span>
<span id="cb6-70"><a href="#cb6-70" tabindex="-1"></a></span>
<span id="cb6-71"><a href="#cb6-71" tabindex="-1"></a><span class="kw">class</span> TransformerDecoderBlock(layers.Layer):</span>
<span id="cb6-72"><a href="#cb6-72" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, embed_dim, ff_dim, num_heads, <span class="op">**</span>kwargs):</span>
<span id="cb6-73"><a href="#cb6-73" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-74"><a href="#cb6-74" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim</span>
<span id="cb6-75"><a href="#cb6-75" tabindex="-1"></a>        <span class="va">self</span>.ff_dim <span class="op">=</span> ff_dim</span>
<span id="cb6-76"><a href="#cb6-76" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb6-77"><a href="#cb6-77" tabindex="-1"></a>        <span class="va">self</span>.attention_1 <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb6-78"><a href="#cb6-78" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb6-79"><a href="#cb6-79" tabindex="-1"></a>        )</span>
<span id="cb6-80"><a href="#cb6-80" tabindex="-1"></a>        <span class="va">self</span>.attention_2 <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb6-81"><a href="#cb6-81" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>embed_dim, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb6-82"><a href="#cb6-82" tabindex="-1"></a>        )</span>
<span id="cb6-83"><a href="#cb6-83" tabindex="-1"></a>        <span class="va">self</span>.ffn_layer_1 <span class="op">=</span> layers.Dense(ff_dim, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb6-84"><a href="#cb6-84" tabindex="-1"></a>        <span class="va">self</span>.ffn_layer_2 <span class="op">=</span> layers.Dense(embed_dim)</span>
<span id="cb6-85"><a href="#cb6-85" tabindex="-1"></a></span>
<span id="cb6-86"><a href="#cb6-86" tabindex="-1"></a>        <span class="va">self</span>.layernorm_1 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-87"><a href="#cb6-87" tabindex="-1"></a>        <span class="va">self</span>.layernorm_2 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-88"><a href="#cb6-88" tabindex="-1"></a>        <span class="va">self</span>.layernorm_3 <span class="op">=</span> layers.LayerNormalization()</span>
<span id="cb6-89"><a href="#cb6-89" tabindex="-1"></a></span>
<span id="cb6-90"><a href="#cb6-90" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> PositionalEmbedding(</span>
<span id="cb6-91"><a href="#cb6-91" tabindex="-1"></a>            embed_dim<span class="op">=</span>EMBED_DIM,</span>
<span id="cb6-92"><a href="#cb6-92" tabindex="-1"></a>            sequence_length<span class="op">=</span>SEQ_LENGTH,</span>
<span id="cb6-93"><a href="#cb6-93" tabindex="-1"></a>            vocab_size<span class="op">=</span>VOCAB_SIZE,</span>
<span id="cb6-94"><a href="#cb6-94" tabindex="-1"></a>        )</span>
<span id="cb6-95"><a href="#cb6-95" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> layers.Dense(VOCAB_SIZE, activation<span class="op">=</span><span class="st">"softmax"</span>)</span>
<span id="cb6-96"><a href="#cb6-96" tabindex="-1"></a></span>
<span id="cb6-97"><a href="#cb6-97" tabindex="-1"></a>        <span class="va">self</span>.dropout_1 <span class="op">=</span> layers.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb6-98"><a href="#cb6-98" tabindex="-1"></a>        <span class="va">self</span>.dropout_2 <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)</span>
<span id="cb6-99"><a href="#cb6-99" tabindex="-1"></a>        <span class="va">self</span>.supports_masking <span class="op">=</span> <span class="va">True</span></span>
<span id="cb6-100"><a href="#cb6-100" tabindex="-1"></a></span>
<span id="cb6-101"><a href="#cb6-101" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, encoder_outputs, training, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-102"><a href="#cb6-102" tabindex="-1"></a>        inputs <span class="op">=</span> <span class="va">self</span>.embedding(inputs)</span>
<span id="cb6-103"><a href="#cb6-103" tabindex="-1"></a>        causal_mask <span class="op">=</span> <span class="va">self</span>.get_causal_attention_mask(inputs)</span>
<span id="cb6-104"><a href="#cb6-104" tabindex="-1"></a></span>
<span id="cb6-105"><a href="#cb6-105" tabindex="-1"></a>        <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-106"><a href="#cb6-106" tabindex="-1"></a>            padding_mask <span class="op">=</span> tf.cast(mask[:, :, tf.newaxis], dtype<span class="op">=</span>tf.int32)</span>
<span id="cb6-107"><a href="#cb6-107" tabindex="-1"></a>            combined_mask <span class="op">=</span> tf.cast(mask[:, tf.newaxis, :], dtype<span class="op">=</span>tf.int32)</span>
<span id="cb6-108"><a href="#cb6-108" tabindex="-1"></a>            combined_mask <span class="op">=</span> tf.minimum(combined_mask, causal_mask)</span>
<span id="cb6-109"><a href="#cb6-109" tabindex="-1"></a></span>
<span id="cb6-110"><a href="#cb6-110" tabindex="-1"></a>        attention_output_1 <span class="op">=</span> <span class="va">self</span>.attention_1(</span>
<span id="cb6-111"><a href="#cb6-111" tabindex="-1"></a>            query<span class="op">=</span>inputs,</span>
<span id="cb6-112"><a href="#cb6-112" tabindex="-1"></a>            value<span class="op">=</span>inputs,</span>
<span id="cb6-113"><a href="#cb6-113" tabindex="-1"></a>            key<span class="op">=</span>inputs,</span>
<span id="cb6-114"><a href="#cb6-114" tabindex="-1"></a>            attention_mask<span class="op">=</span>combined_mask,</span>
<span id="cb6-115"><a href="#cb6-115" tabindex="-1"></a>            training<span class="op">=</span>training,</span>
<span id="cb6-116"><a href="#cb6-116" tabindex="-1"></a>        )</span>
<span id="cb6-117"><a href="#cb6-117" tabindex="-1"></a>        out_1 <span class="op">=</span> <span class="va">self</span>.layernorm_1(inputs <span class="op">+</span> attention_output_1)</span>
<span id="cb6-118"><a href="#cb6-118" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" tabindex="-1"></a>        attention_output_2 <span class="op">=</span> <span class="va">self</span>.attention_2(</span>
<span id="cb6-120"><a href="#cb6-120" tabindex="-1"></a>            query<span class="op">=</span>out_1,</span>
<span id="cb6-121"><a href="#cb6-121" tabindex="-1"></a>            value<span class="op">=</span>encoder_outputs,</span>
<span id="cb6-122"><a href="#cb6-122" tabindex="-1"></a>            key<span class="op">=</span>encoder_outputs,</span>
<span id="cb6-123"><a href="#cb6-123" tabindex="-1"></a>            attention_mask<span class="op">=</span>padding_mask,</span>
<span id="cb6-124"><a href="#cb6-124" tabindex="-1"></a>            training<span class="op">=</span>training,</span>
<span id="cb6-125"><a href="#cb6-125" tabindex="-1"></a>        )</span>
<span id="cb6-126"><a href="#cb6-126" tabindex="-1"></a>        out_2 <span class="op">=</span> <span class="va">self</span>.layernorm_2(out_1 <span class="op">+</span> attention_output_2)</span>
<span id="cb6-127"><a href="#cb6-127" tabindex="-1"></a></span>
<span id="cb6-128"><a href="#cb6-128" tabindex="-1"></a>        ffn_out <span class="op">=</span> <span class="va">self</span>.ffn_layer_1(out_2)</span>
<span id="cb6-129"><a href="#cb6-129" tabindex="-1"></a>        ffn_out <span class="op">=</span> <span class="va">self</span>.dropout_1(ffn_out, training<span class="op">=</span>training)</span>
<span id="cb6-130"><a href="#cb6-130" tabindex="-1"></a>        ffn_out <span class="op">=</span> <span class="va">self</span>.ffn_layer_2(ffn_out)</span>
<span id="cb6-131"><a href="#cb6-131" tabindex="-1"></a></span>
<span id="cb6-132"><a href="#cb6-132" tabindex="-1"></a>        ffn_out <span class="op">=</span> <span class="va">self</span>.layernorm_3(ffn_out <span class="op">+</span> out_2, training<span class="op">=</span>training)</span>
<span id="cb6-133"><a href="#cb6-133" tabindex="-1"></a>        ffn_out <span class="op">=</span> <span class="va">self</span>.dropout_2(ffn_out, training<span class="op">=</span>training)</span>
<span id="cb6-134"><a href="#cb6-134" tabindex="-1"></a>        preds <span class="op">=</span> <span class="va">self</span>.out(ffn_out)</span>
<span id="cb6-135"><a href="#cb6-135" tabindex="-1"></a>        <span class="cf">return</span> preds</span>
<span id="cb6-136"><a href="#cb6-136" tabindex="-1"></a></span>
<span id="cb6-137"><a href="#cb6-137" tabindex="-1"></a>    <span class="kw">def</span> get_causal_attention_mask(<span class="va">self</span>, inputs):</span>
<span id="cb6-138"><a href="#cb6-138" tabindex="-1"></a>        input_shape <span class="op">=</span> tf.shape(inputs)</span>
<span id="cb6-139"><a href="#cb6-139" tabindex="-1"></a>        batch_size, sequence_length <span class="op">=</span> input_shape[<span class="dv">0</span>], input_shape[<span class="dv">1</span>]</span>
<span id="cb6-140"><a href="#cb6-140" tabindex="-1"></a>        i <span class="op">=</span> tf.<span class="bu">range</span>(sequence_length)[:, tf.newaxis]</span>
<span id="cb6-141"><a href="#cb6-141" tabindex="-1"></a>        j <span class="op">=</span> tf.<span class="bu">range</span>(sequence_length)</span>
<span id="cb6-142"><a href="#cb6-142" tabindex="-1"></a>        mask <span class="op">=</span> tf.cast(i <span class="op">&gt;=</span> j, dtype<span class="op">=</span><span class="st">"int32"</span>)</span>
<span id="cb6-143"><a href="#cb6-143" tabindex="-1"></a>        mask <span class="op">=</span> tf.reshape(mask, (<span class="dv">1</span>, input_shape[<span class="dv">1</span>], input_shape[<span class="dv">1</span>]))</span>
<span id="cb6-144"><a href="#cb6-144" tabindex="-1"></a>        mult <span class="op">=</span> tf.concat(</span>
<span id="cb6-145"><a href="#cb6-145" tabindex="-1"></a>            [</span>
<span id="cb6-146"><a href="#cb6-146" tabindex="-1"></a>                tf.expand_dims(batch_size, <span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb6-147"><a href="#cb6-147" tabindex="-1"></a>                tf.constant([<span class="dv">1</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.int32),</span>
<span id="cb6-148"><a href="#cb6-148" tabindex="-1"></a>            ],</span>
<span id="cb6-149"><a href="#cb6-149" tabindex="-1"></a>            axis<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb6-150"><a href="#cb6-150" tabindex="-1"></a>        )</span>
<span id="cb6-151"><a href="#cb6-151" tabindex="-1"></a>        <span class="cf">return</span> tf.tile(mask, mult)</span>
<span id="cb6-152"><a href="#cb6-152" tabindex="-1"></a></span>
<span id="cb6-153"><a href="#cb6-153" tabindex="-1"></a></span>
<span id="cb6-154"><a href="#cb6-154" tabindex="-1"></a><span class="kw">class</span> ImageCaptioningModel(keras.Model):</span>
<span id="cb6-155"><a href="#cb6-155" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb6-156"><a href="#cb6-156" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb6-157"><a href="#cb6-157" tabindex="-1"></a>        cnn_model,</span>
<span id="cb6-158"><a href="#cb6-158" tabindex="-1"></a>        encoder,</span>
<span id="cb6-159"><a href="#cb6-159" tabindex="-1"></a>        decoder,</span>
<span id="cb6-160"><a href="#cb6-160" tabindex="-1"></a>        num_captions_per_image<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb6-161"><a href="#cb6-161" tabindex="-1"></a>        image_aug<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb6-162"><a href="#cb6-162" tabindex="-1"></a>    ):</span>
<span id="cb6-163"><a href="#cb6-163" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-164"><a href="#cb6-164" tabindex="-1"></a>        <span class="va">self</span>.cnn_model <span class="op">=</span> cnn_model</span>
<span id="cb6-165"><a href="#cb6-165" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb6-166"><a href="#cb6-166" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> decoder</span>
<span id="cb6-167"><a href="#cb6-167" tabindex="-1"></a>        <span class="va">self</span>.loss_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"loss"</span>)</span>
<span id="cb6-168"><a href="#cb6-168" tabindex="-1"></a>        <span class="va">self</span>.acc_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"accuracy"</span>)</span>
<span id="cb6-169"><a href="#cb6-169" tabindex="-1"></a>        <span class="va">self</span>.num_captions_per_image <span class="op">=</span> num_captions_per_image</span>
<span id="cb6-170"><a href="#cb6-170" tabindex="-1"></a>        <span class="va">self</span>.image_aug <span class="op">=</span> image_aug</span>
<span id="cb6-171"><a href="#cb6-171" tabindex="-1"></a></span>
<span id="cb6-172"><a href="#cb6-172" tabindex="-1"></a>    <span class="kw">def</span> calculate_loss(<span class="va">self</span>, y_true, y_pred, mask):</span>
<span id="cb6-173"><a href="#cb6-173" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.loss(y_true, y_pred)</span>
<span id="cb6-174"><a href="#cb6-174" tabindex="-1"></a>        mask <span class="op">=</span> tf.cast(mask, dtype<span class="op">=</span>loss.dtype)</span>
<span id="cb6-175"><a href="#cb6-175" tabindex="-1"></a>        loss <span class="op">*=</span> mask</span>
<span id="cb6-176"><a href="#cb6-176" tabindex="-1"></a>        <span class="cf">return</span> tf.reduce_sum(loss) <span class="op">/</span> tf.reduce_sum(mask)</span>
<span id="cb6-177"><a href="#cb6-177" tabindex="-1"></a></span>
<span id="cb6-178"><a href="#cb6-178" tabindex="-1"></a>    <span class="kw">def</span> calculate_accuracy(<span class="va">self</span>, y_true, y_pred, mask):</span>
<span id="cb6-179"><a href="#cb6-179" tabindex="-1"></a>        accuracy <span class="op">=</span> tf.equal(y_true, tf.argmax(y_pred, axis<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb6-180"><a href="#cb6-180" tabindex="-1"></a>        accuracy <span class="op">=</span> tf.math.logical_and(mask, accuracy)</span>
<span id="cb6-181"><a href="#cb6-181" tabindex="-1"></a>        accuracy <span class="op">=</span> tf.cast(accuracy, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb6-182"><a href="#cb6-182" tabindex="-1"></a>        mask <span class="op">=</span> tf.cast(mask, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb6-183"><a href="#cb6-183" tabindex="-1"></a>        <span class="cf">return</span> tf.reduce_sum(accuracy) <span class="op">/</span> tf.reduce_sum(mask)</span>
<span id="cb6-184"><a href="#cb6-184" tabindex="-1"></a></span>
<span id="cb6-185"><a href="#cb6-185" tabindex="-1"></a>    <span class="kw">def</span> _compute_caption_loss_and_acc(</span>
<span id="cb6-186"><a href="#cb6-186" tabindex="-1"></a>        <span class="va">self</span>, img_embed, batch_seq, training<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-187"><a href="#cb6-187" tabindex="-1"></a>    ):</span>
<span id="cb6-188"><a href="#cb6-188" tabindex="-1"></a>        encoder_out <span class="op">=</span> <span class="va">self</span>.encoder(img_embed, training<span class="op">=</span>training)</span>
<span id="cb6-189"><a href="#cb6-189" tabindex="-1"></a>        batch_seq_inp <span class="op">=</span> batch_seq[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-190"><a href="#cb6-190" tabindex="-1"></a>        batch_seq_true <span class="op">=</span> batch_seq[:, <span class="dv">1</span>:]</span>
<span id="cb6-191"><a href="#cb6-191" tabindex="-1"></a>        mask <span class="op">=</span> tf.math.not_equal(batch_seq_true, <span class="dv">0</span>)</span>
<span id="cb6-192"><a href="#cb6-192" tabindex="-1"></a>        batch_seq_pred <span class="op">=</span> <span class="va">self</span>.decoder(</span>
<span id="cb6-193"><a href="#cb6-193" tabindex="-1"></a>            batch_seq_inp, encoder_out, training<span class="op">=</span>training, mask<span class="op">=</span>mask</span>
<span id="cb6-194"><a href="#cb6-194" tabindex="-1"></a>        )</span>
<span id="cb6-195"><a href="#cb6-195" tabindex="-1"></a>        loss <span class="op">=</span> <span class="va">self</span>.calculate_loss(batch_seq_true, batch_seq_pred, mask)</span>
<span id="cb6-196"><a href="#cb6-196" tabindex="-1"></a>        acc <span class="op">=</span> <span class="va">self</span>.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)</span>
<span id="cb6-197"><a href="#cb6-197" tabindex="-1"></a>        <span class="cf">return</span> loss, acc</span>
<span id="cb6-198"><a href="#cb6-198" tabindex="-1"></a></span>
<span id="cb6-199"><a href="#cb6-199" tabindex="-1"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, batch_data):</span>
<span id="cb6-200"><a href="#cb6-200" tabindex="-1"></a>        batch_img, batch_seq <span class="op">=</span> batch_data</span>
<span id="cb6-201"><a href="#cb6-201" tabindex="-1"></a>        batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-202"><a href="#cb6-202" tabindex="-1"></a>        batch_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-203"><a href="#cb6-203" tabindex="-1"></a></span>
<span id="cb6-204"><a href="#cb6-204" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.image_aug:</span>
<span id="cb6-205"><a href="#cb6-205" tabindex="-1"></a>            batch_img <span class="op">=</span> <span class="va">self</span>.image_aug(batch_img)</span>
<span id="cb6-206"><a href="#cb6-206" tabindex="-1"></a></span>
<span id="cb6-207"><a href="#cb6-207" tabindex="-1"></a>        <span class="co"># 1. Get image embeddings</span></span>
<span id="cb6-208"><a href="#cb6-208" tabindex="-1"></a>        img_embed <span class="op">=</span> <span class="va">self</span>.cnn_model(batch_img)</span>
<span id="cb6-209"><a href="#cb6-209" tabindex="-1"></a></span>
<span id="cb6-210"><a href="#cb6-210" tabindex="-1"></a>        <span class="co"># 2. Pass each of the five captions one by one to the decoder</span></span>
<span id="cb6-211"><a href="#cb6-211" tabindex="-1"></a>        <span class="co"># along with the encoder outputs and compute the loss as well as accuracy</span></span>
<span id="cb6-212"><a href="#cb6-212" tabindex="-1"></a>        <span class="co"># for each caption.</span></span>
<span id="cb6-213"><a href="#cb6-213" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_captions_per_image):</span>
<span id="cb6-214"><a href="#cb6-214" tabindex="-1"></a>            <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb6-215"><a href="#cb6-215" tabindex="-1"></a>                loss, acc <span class="op">=</span> <span class="va">self</span>._compute_caption_loss_and_acc(</span>
<span id="cb6-216"><a href="#cb6-216" tabindex="-1"></a>                    img_embed, batch_seq[:, i, :], training<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-217"><a href="#cb6-217" tabindex="-1"></a>                )</span>
<span id="cb6-218"><a href="#cb6-218" tabindex="-1"></a></span>
<span id="cb6-219"><a href="#cb6-219" tabindex="-1"></a>                <span class="co"># 3. Update loss and accuracy</span></span>
<span id="cb6-220"><a href="#cb6-220" tabindex="-1"></a>                batch_loss <span class="op">+=</span> loss</span>
<span id="cb6-221"><a href="#cb6-221" tabindex="-1"></a>                batch_acc <span class="op">+=</span> acc</span>
<span id="cb6-222"><a href="#cb6-222" tabindex="-1"></a></span>
<span id="cb6-223"><a href="#cb6-223" tabindex="-1"></a>            <span class="co"># 4. Get the list of all the trainable weights</span></span>
<span id="cb6-224"><a href="#cb6-224" tabindex="-1"></a>            train_vars <span class="op">=</span> (</span>
<span id="cb6-225"><a href="#cb6-225" tabindex="-1"></a>                <span class="va">self</span>.encoder.trainable_variables</span>
<span id="cb6-226"><a href="#cb6-226" tabindex="-1"></a>                <span class="op">+</span> <span class="va">self</span>.decoder.trainable_variables</span>
<span id="cb6-227"><a href="#cb6-227" tabindex="-1"></a>            )</span>
<span id="cb6-228"><a href="#cb6-228" tabindex="-1"></a></span>
<span id="cb6-229"><a href="#cb6-229" tabindex="-1"></a>            <span class="co"># 5. Get the gradients</span></span>
<span id="cb6-230"><a href="#cb6-230" tabindex="-1"></a>            grads <span class="op">=</span> tape.gradient(loss, train_vars)</span>
<span id="cb6-231"><a href="#cb6-231" tabindex="-1"></a></span>
<span id="cb6-232"><a href="#cb6-232" tabindex="-1"></a>            <span class="co"># 6. Update the trainable weights</span></span>
<span id="cb6-233"><a href="#cb6-233" tabindex="-1"></a>            <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(grads, train_vars))</span>
<span id="cb6-234"><a href="#cb6-234" tabindex="-1"></a></span>
<span id="cb6-235"><a href="#cb6-235" tabindex="-1"></a>        <span class="co"># 7. Update the trackers</span></span>
<span id="cb6-236"><a href="#cb6-236" tabindex="-1"></a>        batch_acc <span class="op">/=</span> <span class="bu">float</span>(<span class="va">self</span>.num_captions_per_image)</span>
<span id="cb6-237"><a href="#cb6-237" tabindex="-1"></a>        <span class="va">self</span>.loss_tracker.update_state(batch_loss)</span>
<span id="cb6-238"><a href="#cb6-238" tabindex="-1"></a>        <span class="va">self</span>.acc_tracker.update_state(batch_acc)</span>
<span id="cb6-239"><a href="#cb6-239" tabindex="-1"></a></span>
<span id="cb6-240"><a href="#cb6-240" tabindex="-1"></a>        <span class="co"># 8. Return the loss and accuracy values</span></span>
<span id="cb6-241"><a href="#cb6-241" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb6-242"><a href="#cb6-242" tabindex="-1"></a>            <span class="st">"loss"</span>: <span class="va">self</span>.loss_tracker.result(),</span>
<span id="cb6-243"><a href="#cb6-243" tabindex="-1"></a>            <span class="st">"acc"</span>: <span class="va">self</span>.acc_tracker.result(),</span>
<span id="cb6-244"><a href="#cb6-244" tabindex="-1"></a>        }</span>
<span id="cb6-245"><a href="#cb6-245" tabindex="-1"></a></span>
<span id="cb6-246"><a href="#cb6-246" tabindex="-1"></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, batch_data):</span>
<span id="cb6-247"><a href="#cb6-247" tabindex="-1"></a>        batch_img, batch_seq <span class="op">=</span> batch_data</span>
<span id="cb6-248"><a href="#cb6-248" tabindex="-1"></a>        batch_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-249"><a href="#cb6-249" tabindex="-1"></a>        batch_acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-250"><a href="#cb6-250" tabindex="-1"></a></span>
<span id="cb6-251"><a href="#cb6-251" tabindex="-1"></a>        <span class="co"># 1. Get image embeddings</span></span>
<span id="cb6-252"><a href="#cb6-252" tabindex="-1"></a>        img_embed <span class="op">=</span> <span class="va">self</span>.cnn_model(batch_img)</span>
<span id="cb6-253"><a href="#cb6-253" tabindex="-1"></a></span>
<span id="cb6-254"><a href="#cb6-254" tabindex="-1"></a>        <span class="co"># 2. Pass each of the five captions one by one to the decoder</span></span>
<span id="cb6-255"><a href="#cb6-255" tabindex="-1"></a>        <span class="co"># along with the encoder outputs and compute the loss as well as accuracy</span></span>
<span id="cb6-256"><a href="#cb6-256" tabindex="-1"></a>        <span class="co"># for each caption.</span></span>
<span id="cb6-257"><a href="#cb6-257" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_captions_per_image):</span>
<span id="cb6-258"><a href="#cb6-258" tabindex="-1"></a>            loss, acc <span class="op">=</span> <span class="va">self</span>._compute_caption_loss_and_acc(</span>
<span id="cb6-259"><a href="#cb6-259" tabindex="-1"></a>                img_embed, batch_seq[:, i, :], training<span class="op">=</span><span class="va">False</span></span>
<span id="cb6-260"><a href="#cb6-260" tabindex="-1"></a>            )</span>
<span id="cb6-261"><a href="#cb6-261" tabindex="-1"></a></span>
<span id="cb6-262"><a href="#cb6-262" tabindex="-1"></a>            <span class="co"># 3. Update batch loss and batch accuracy</span></span>
<span id="cb6-263"><a href="#cb6-263" tabindex="-1"></a>            batch_loss <span class="op">+=</span> loss</span>
<span id="cb6-264"><a href="#cb6-264" tabindex="-1"></a>            batch_acc <span class="op">+=</span> acc</span>
<span id="cb6-265"><a href="#cb6-265" tabindex="-1"></a></span>
<span id="cb6-266"><a href="#cb6-266" tabindex="-1"></a>        batch_acc <span class="op">/=</span> <span class="bu">float</span>(<span class="va">self</span>.num_captions_per_image)</span>
<span id="cb6-267"><a href="#cb6-267" tabindex="-1"></a></span>
<span id="cb6-268"><a href="#cb6-268" tabindex="-1"></a>        <span class="co"># 4. Update the trackers</span></span>
<span id="cb6-269"><a href="#cb6-269" tabindex="-1"></a>        <span class="va">self</span>.loss_tracker.update_state(batch_loss)</span>
<span id="cb6-270"><a href="#cb6-270" tabindex="-1"></a>        <span class="va">self</span>.acc_tracker.update_state(batch_acc)</span>
<span id="cb6-271"><a href="#cb6-271" tabindex="-1"></a></span>
<span id="cb6-272"><a href="#cb6-272" tabindex="-1"></a>        <span class="co"># 5. Return the loss and accuracy values</span></span>
<span id="cb6-273"><a href="#cb6-273" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb6-274"><a href="#cb6-274" tabindex="-1"></a>            <span class="st">"loss"</span>: <span class="va">self</span>.loss_tracker.result(),</span>
<span id="cb6-275"><a href="#cb6-275" tabindex="-1"></a>            <span class="st">"acc"</span>: <span class="va">self</span>.acc_tracker.result(),</span>
<span id="cb6-276"><a href="#cb6-276" tabindex="-1"></a>        }</span>
<span id="cb6-277"><a href="#cb6-277" tabindex="-1"></a></span>
<span id="cb6-278"><a href="#cb6-278" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb6-279"><a href="#cb6-279" tabindex="-1"></a>    <span class="kw">def</span> metrics(<span class="va">self</span>):</span>
<span id="cb6-280"><a href="#cb6-280" tabindex="-1"></a>        <span class="co"># We need to list our metrics here so the `reset_states()` can be</span></span>
<span id="cb6-281"><a href="#cb6-281" tabindex="-1"></a>        <span class="co"># called automatically.</span></span>
<span id="cb6-282"><a href="#cb6-282" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.loss_tracker, <span class="va">self</span>.acc_tracker]</span>
<span id="cb6-283"><a href="#cb6-283" tabindex="-1"></a></span>
<span id="cb6-284"><a href="#cb6-284" tabindex="-1"></a></span>
<span id="cb6-285"><a href="#cb6-285" tabindex="-1"></a>cnn_model <span class="op">=</span> get_cnn_model()</span>
<span id="cb6-286"><a href="#cb6-286" tabindex="-1"></a>encoder <span class="op">=</span> TransformerEncoderBlock(</span>
<span id="cb6-287"><a href="#cb6-287" tabindex="-1"></a>    embed_dim<span class="op">=</span>EMBED_DIM, dense_dim<span class="op">=</span>FF_DIM, num_heads<span class="op">=</span><span class="dv">1</span></span>
<span id="cb6-288"><a href="#cb6-288" tabindex="-1"></a>)</span>
<span id="cb6-289"><a href="#cb6-289" tabindex="-1"></a>decoder <span class="op">=</span> TransformerDecoderBlock(</span>
<span id="cb6-290"><a href="#cb6-290" tabindex="-1"></a>    embed_dim<span class="op">=</span>EMBED_DIM, ff_dim<span class="op">=</span>FF_DIM, num_heads<span class="op">=</span><span class="dv">2</span></span>
<span id="cb6-291"><a href="#cb6-291" tabindex="-1"></a>)</span>
<span id="cb6-292"><a href="#cb6-292" tabindex="-1"></a>caption_model <span class="op">=</span> ImageCaptioningModel(</span>
<span id="cb6-293"><a href="#cb6-293" tabindex="-1"></a>    cnn_model<span class="op">=</span>cnn_model,</span>
<span id="cb6-294"><a href="#cb6-294" tabindex="-1"></a>    encoder<span class="op">=</span>encoder,</span>
<span id="cb6-295"><a href="#cb6-295" tabindex="-1"></a>    decoder<span class="op">=</span>decoder,</span>
<span id="cb6-296"><a href="#cb6-296" tabindex="-1"></a>    image_aug<span class="op">=</span>image_augmentation,</span>
<span id="cb6-297"><a href="#cb6-297" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-training">Model training<a class="anchor" aria-label="anchor" href="#model-training"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Define the loss function</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>cross_entropy <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    from_logits<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    reduction<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co"># EarlyStopping criteria</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>early_stopping <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">3</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="co"># Learning Rate Scheduler for the optimizer</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="kw">class</span> LRSchedule(</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>    keras.optimizers.schedules.learning_rate_schedule.LearningRateSchedule</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>):</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, post_warmup_learning_rate, warmup_steps):</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>        <span class="va">self</span>.post_warmup_learning_rate <span class="op">=</span> post_warmup_learning_rate</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>        <span class="va">self</span>.warmup_steps <span class="op">=</span> warmup_steps</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, step):</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>        global_step <span class="op">=</span> tf.cast(step, tf.float32)</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>        warmup_steps <span class="op">=</span> tf.cast(<span class="va">self</span>.warmup_steps, tf.float32)</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>        warmup_progress <span class="op">=</span> global_step <span class="op">/</span> warmup_steps</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>        warmup_learning_rate <span class="op">=</span> <span class="va">self</span>.post_warmup_learning_rate <span class="op">*</span> warmup_progress</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>        <span class="cf">return</span> tf.cond(</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>            global_step <span class="op">&lt;</span> warmup_steps,</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>            <span class="kw">lambda</span>: warmup_learning_rate,</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>            <span class="kw">lambda</span>: <span class="va">self</span>.post_warmup_learning_rate,</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>        )</span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a><span class="co"># Create a learning rate schedule</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>num_train_steps <span class="op">=</span> <span class="bu">len</span>(train_dataset) <span class="op">*</span> EPOCHS</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>num_warmup_steps <span class="op">=</span> num_train_steps <span class="op">//</span> <span class="dv">15</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>lr_schedule <span class="op">=</span> LRSchedule(</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>    post_warmup_learning_rate<span class="op">=</span><span class="fl">1e-4</span>, warmup_steps<span class="op">=</span>num_warmup_steps</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>)</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>caption_model.<span class="bu">compile</span>(</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(lr_schedule), loss<span class="op">=</span>cross_entropy</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>)</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>caption_model.fit(</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>    train_dataset,</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>    epochs<span class="op">=</span>EPOCHS,</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>    validation_data<span class="op">=</span>valid_dataset,</span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>    callbacks<span class="op">=</span>[early_stopping],</span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="check-sample-predictions">Check sample predictions<a class="anchor" aria-label="anchor" href="#check-sample-predictions"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>vocab <span class="op">=</span> vectorization.get_vocabulary()</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>index_lookup <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(<span class="bu">range</span>(<span class="bu">len</span>(vocab)), vocab))</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>max_decoded_sentence_length <span class="op">=</span> SEQ_LENGTH <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>valid_images <span class="op">=</span> <span class="bu">list</span>(valid_data.keys())</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="kw">def</span> generate_caption():</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    <span class="co"># Select a random image from the validation dataset</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    sample_img <span class="op">=</span> np.random.choice(valid_images)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    <span class="co"># Read the image from the disk</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    sample_img <span class="op">=</span> decode_and_resize(sample_img)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    img <span class="op">=</span> sample_img.numpy().clip(<span class="dv">0</span>, <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    plt.imshow(img)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>    plt.show()</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    <span class="co"># Pass the image to the CNN</span></span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    img <span class="op">=</span> tf.expand_dims(sample_img, <span class="dv">0</span>)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    img <span class="op">=</span> caption_model.cnn_model(img)</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    <span class="co"># Pass the image features to the Transformer encoder</span></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    encoded_img <span class="op">=</span> caption_model.encoder(img, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>    <span class="co"># Generate the caption using the Transformer decoder</span></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>    decoded_caption <span class="op">=</span> <span class="st">"&lt;start&gt; "</span></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_decoded_sentence_length):</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>        tokenized_caption <span class="op">=</span> vectorization([decoded_caption])[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>        mask <span class="op">=</span> tf.math.not_equal(tokenized_caption, <span class="dv">0</span>)</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>        predictions <span class="op">=</span> caption_model.decoder(</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>            tokenized_caption, encoded_img, training<span class="op">=</span><span class="va">False</span>, mask<span class="op">=</span>mask</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>        )</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>        sampled_token_index <span class="op">=</span> np.argmax(predictions[<span class="dv">0</span>, i, :])</span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>        sampled_token <span class="op">=</span> index_lookup[sampled_token_index]</span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a>        <span class="cf">if</span> sampled_token <span class="op">==</span> <span class="st">"&lt;end&gt;"</span>:</span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a>        decoded_caption <span class="op">+=</span> <span class="st">" "</span> <span class="op">+</span> sampled_token</span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>    decoded_caption <span class="op">=</span> decoded_caption.replace(<span class="st">"&lt;start&gt; "</span>, <span class="st">""</span>)</span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>    decoded_caption <span class="op">=</span> decoded_caption.replace(<span class="st">" &lt;end&gt;"</span>, <span class="st">""</span>).strip()</span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Predicted Caption: "</span>, decoded_caption)</span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a><span class="co"># Check predictions for a few samples</span></span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a>generate_caption()</span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>generate_caption()</span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>generate_caption()</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="end-notes">End Notes<a class="anchor" aria-label="anchor" href="#end-notes"></a>
</h2>
<p>We saw that the model starts to generate reasonable captions after a
few epochs. To keep this example easily runnable, we have trained it
with a few constraints, like a minimal number of attention heads. To
improve the predictions, you can try changing these training settings
and find a good model for your use case.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
