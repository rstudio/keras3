<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Actor Critic Method • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Actor Critic Method">
<meta property="og:description" content="Implement Actor Critic Method in CartPole environment.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Actor Critic Method</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/NandanApoorv" class="external-link">Apoorv Nandan</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/actor_critic_cartpole.Rmd" class="external-link"><code>vignettes/examples/actor_critic_cartpole.Rmd</code></a></small>
      <div class="hidden name"><code>actor_critic_cartpole.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This script shows an implementation of Actor Critic method on
CartPole-V0 environment.</p>
<div class="section level3">
<h3 id="actor-critic-method">Actor Critic Method<a class="anchor" aria-label="anchor" href="#actor-critic-method"></a>
</h3>
<p>As an agent takes actions and moves through an environment, it learns
to map the observed state of the environment to two possible
outputs:</p>
<ol style="list-style-type: decimal">
<li>Recommended action: A probability value for each action in the
action space. The part of the agent responsible for this output is
called the <strong>actor</strong>.</li>
<li>Estimated rewards in the future: Sum of all rewards it expects to
receive in the future. The part of the agent responsible for this output
is the <strong>critic</strong>.</li>
</ol>
<p>Agent and Critic learn to perform their tasks, such that the
recommended actions from the actor maximize the rewards.</p>
</div>
<div class="section level3">
<h3 id="cartpole-v1">CartPole-V1<a class="anchor" aria-label="anchor" href="#cartpole-v1"></a>
</h3>
<p>A pole is attached to a cart placed on a frictionless track. The
agent has to apply force to move the cart. It is rewarded for every time
step the pole remains upright. The agent, therefore, must learn to keep
the pole from falling over.</p>
</div>
<div class="section level3">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ul>
<li><a href="http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf" class="external-link">CartPole</a></li>
<li><a href="https://hal.inria.fr/hal-00840470/document" class="external-link">Actor Critic
Method</a></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Configuration parameters for the whole setup</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">0.99</span>  <span class="co"># Discount factor for past rewards</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>max_steps_per_episode <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>, new_step_api<span class="op">=</span><span class="va">True</span>)  <span class="co"># Create the environment</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>env.reset(seed<span class="op">=</span>seed)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>eps <span class="op">=</span> np.finfo(</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    np.float32</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>).eps.item()  <span class="co"># Smallest number such that 1.0 + eps != 1.0</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-actor-critic-network">Implement Actor Critic network<a class="anchor" aria-label="anchor" href="#implement-actor-critic-network"></a>
</h2>
<p>This network learns two functions:</p>
<ol style="list-style-type: decimal">
<li>Actor: This takes as input the state of our environment and returns
a probability value for each action in its action space.</li>
<li>Critic: This takes as input the state of our environment and returns
an estimate of total rewards in the future.</li>
</ol>
<p>In our implementation, they share the initial layer.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_inputs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>num_actions <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>num_hidden <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>(num_inputs,))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>common <span class="op">=</span> layers.Dense(num_hidden, activation<span class="op">=</span><span class="st">"relu"</span>)(inputs)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>action <span class="op">=</span> layers.Dense(num_actions, activation<span class="op">=</span><span class="st">"softmax"</span>)(common)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>critic <span class="op">=</span> layers.Dense(<span class="dv">1</span>)(common)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>[action, critic])</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train">Train<a class="anchor" aria-label="anchor" href="#train"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>huber_loss <span class="op">=</span> keras.losses.Huber()</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>action_probs_history <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>critic_value_history <span class="op">=</span> []</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>rewards_history <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>running_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>episode_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:  <span class="co"># Run until solved</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    state <span class="op">=</span> env.reset()</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    episode_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>        <span class="cf">for</span> timestep <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, max_steps_per_episode):</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>            <span class="co"># env.render(); Adding this line would show the attempts</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>            <span class="co"># of the agent in a pop up window.</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>            state <span class="op">=</span> tf.convert_to_tensor(state)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>            state <span class="op">=</span> tf.expand_dims(state, <span class="dv">0</span>)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>            <span class="co"># Predict action probabilities and estimated future rewards</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>            <span class="co"># from environment state</span></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>            action_probs, critic_value <span class="op">=</span> model(state)</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>            critic_value_history.append(critic_value[<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>            <span class="co"># Sample action from action probability distribution</span></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>            action <span class="op">=</span> np.random.choice(num_actions, p<span class="op">=</span>np.squeeze(action_probs))</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>            action_probs_history.append(tf.math.log(action_probs[<span class="dv">0</span>, action]))</span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            <span class="co"># Apply the sampled action in our environment</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>            state, reward, done, _, _ <span class="op">=</span> env.step(action)</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>            rewards_history.append(reward)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>            episode_reward <span class="op">+=</span> reward</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>            <span class="cf">if</span> done:</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>        <span class="co"># Update running reward to check condition for solving</span></span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>        running_reward <span class="op">=</span> <span class="fl">0.05</span> <span class="op">*</span> episode_reward <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.05</span>) <span class="op">*</span> running_reward</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a>        <span class="co"># Calculate expected value from rewards</span></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>        <span class="co"># - At each timestep what was the total reward received after that timestep</span></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>        <span class="co"># - Rewards in the past are discounted by multiplying them with gamma</span></span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>        <span class="co"># - These are the labels for our critic</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>        returns <span class="op">=</span> []</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a>        discounted_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> rewards_history[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a>            discounted_sum <span class="op">=</span> r <span class="op">+</span> gamma <span class="op">*</span> discounted_sum</span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a>            returns.insert(<span class="dv">0</span>, discounted_sum)</span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>        returns <span class="op">=</span> np.array(returns)</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>        returns <span class="op">=</span> (returns <span class="op">-</span> np.mean(returns)) <span class="op">/</span> (np.std(returns) <span class="op">+</span> eps)</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>        returns <span class="op">=</span> returns.tolist()</span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a>        <span class="co"># Calculating loss values to update our network</span></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a>        history <span class="op">=</span> <span class="bu">zip</span>(action_probs_history, critic_value_history, returns)</span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a>        actor_losses <span class="op">=</span> []</span>
<span id="cb3-58"><a href="#cb3-58" tabindex="-1"></a>        critic_losses <span class="op">=</span> []</span>
<span id="cb3-59"><a href="#cb3-59" tabindex="-1"></a>        <span class="cf">for</span> log_prob, value, ret <span class="kw">in</span> history:</span>
<span id="cb3-60"><a href="#cb3-60" tabindex="-1"></a>            <span class="co"># At this point in history, the critic estimated that we would get a</span></span>
<span id="cb3-61"><a href="#cb3-61" tabindex="-1"></a>            <span class="co"># total reward = `value` in the future. We took an action with log probability</span></span>
<span id="cb3-62"><a href="#cb3-62" tabindex="-1"></a>            <span class="co"># of `log_prob` and ended up recieving a total reward = `ret`.</span></span>
<span id="cb3-63"><a href="#cb3-63" tabindex="-1"></a>            <span class="co"># The actor must be updated so that it predicts an action that leads to</span></span>
<span id="cb3-64"><a href="#cb3-64" tabindex="-1"></a>            <span class="co"># high rewards (compared to critic's estimate) with high probability.</span></span>
<span id="cb3-65"><a href="#cb3-65" tabindex="-1"></a>            diff <span class="op">=</span> ret <span class="op">-</span> value</span>
<span id="cb3-66"><a href="#cb3-66" tabindex="-1"></a>            actor_losses.append(<span class="op">-</span>log_prob <span class="op">*</span> diff)  <span class="co"># actor loss</span></span>
<span id="cb3-67"><a href="#cb3-67" tabindex="-1"></a></span>
<span id="cb3-68"><a href="#cb3-68" tabindex="-1"></a>            <span class="co"># The critic must be updated so that it predicts a better estimate of</span></span>
<span id="cb3-69"><a href="#cb3-69" tabindex="-1"></a>            <span class="co"># the future rewards.</span></span>
<span id="cb3-70"><a href="#cb3-70" tabindex="-1"></a>            critic_losses.append(</span>
<span id="cb3-71"><a href="#cb3-71" tabindex="-1"></a>                huber_loss(tf.expand_dims(value, <span class="dv">0</span>), tf.expand_dims(ret, <span class="dv">0</span>))</span>
<span id="cb3-72"><a href="#cb3-72" tabindex="-1"></a>            )</span>
<span id="cb3-73"><a href="#cb3-73" tabindex="-1"></a></span>
<span id="cb3-74"><a href="#cb3-74" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb3-75"><a href="#cb3-75" tabindex="-1"></a>        loss_value <span class="op">=</span> <span class="bu">sum</span>(actor_losses) <span class="op">+</span> <span class="bu">sum</span>(critic_losses)</span>
<span id="cb3-76"><a href="#cb3-76" tabindex="-1"></a>        grads <span class="op">=</span> tape.gradient(loss_value, model.trainable_variables)</span>
<span id="cb3-77"><a href="#cb3-77" tabindex="-1"></a>        optimizer.apply_gradients(<span class="bu">zip</span>(grads, model.trainable_variables))</span>
<span id="cb3-78"><a href="#cb3-78" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" tabindex="-1"></a>        <span class="co"># Clear the loss and reward history</span></span>
<span id="cb3-80"><a href="#cb3-80" tabindex="-1"></a>        action_probs_history.clear()</span>
<span id="cb3-81"><a href="#cb3-81" tabindex="-1"></a>        critic_value_history.clear()</span>
<span id="cb3-82"><a href="#cb3-82" tabindex="-1"></a>        rewards_history.clear()</span>
<span id="cb3-83"><a href="#cb3-83" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" tabindex="-1"></a>    <span class="co"># Log details</span></span>
<span id="cb3-85"><a href="#cb3-85" tabindex="-1"></a>    episode_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb3-86"><a href="#cb3-86" tabindex="-1"></a>    <span class="cf">if</span> episode_count <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-87"><a href="#cb3-87" tabindex="-1"></a>        template <span class="op">=</span> <span class="st">"running reward: </span><span class="sc">{:.2f}</span><span class="st"> at episode </span><span class="sc">{}</span><span class="st">"</span></span>
<span id="cb3-88"><a href="#cb3-88" tabindex="-1"></a>        <span class="bu">print</span>(template.<span class="bu">format</span>(running_reward, episode_count))</span>
<span id="cb3-89"><a href="#cb3-89" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" tabindex="-1"></a>    <span class="cf">if</span> running_reward <span class="op">&gt;</span> <span class="dv">195</span>:  <span class="co"># Condition to consider the task solved</span></span>
<span id="cb3-91"><a href="#cb3-91" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Solved at episode </span><span class="sc">{}</span><span class="st">!"</span>.<span class="bu">format</span>(episode_count))</span>
<span id="cb3-92"><a href="#cb3-92" tabindex="-1"></a>        <span class="cf">break</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="visualizations">Visualizations<a class="anchor" aria-label="anchor" href="#visualizations"></a>
</h2>
<p>In early stages of training: <img src="https://i.imgur.com/5gCs5kH.gif" alt="Imgur"></p>
<p>In later stages of training: <img src="https://i.imgur.com/5ziiZUD.gif" alt="Imgur"></p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
