<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Implementation of a self-supervised learning method for computer vision.">
<title>Self-supervised contrastive learning with SimSiam • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.8/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Self-supervised contrastive learning with SimSiam">
<meta property="og:description" content="Implementation of a self-supervised learning method for computer vision.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Self-supervised contrastive learning with SimSiam</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/simsiam.Rmd" class="external-link"><code>vignettes/examples/simsiam.Rmd</code></a></small>
      <div class="d-none name"><code>simsiam.Rmd</code></div>
    </div>

    
    
<p>Self-supervised learning (SSL) is an interesting branch of study in
the field of representation learning. SSL systems try to formulate a
supervised signal from a corpus of unlabeled data points. An example is
we train a deep neural network to predict the next word from a given set
of words. In literature, these tasks are known as <em>pretext tasks</em>
or <em>auxiliary tasks</em>. If we <a href="https://arxiv.org/abs/1801.06146" class="external-link">train such a network</a> on a
huge dataset (such as the <a href="https://www.corpusdata.org/wikipedia.asp" class="external-link">Wikipedia text
corpus</a>) it learns very effective representations that transfer well
to downstream tasks. Language models like <a href="https://arxiv.org/abs/1810.04805" class="external-link">BERT</a>, <a href="https://arxiv.org/abs/2005.14165" class="external-link">GPT-3</a>, <a href="https://allennlp.org/elmo" class="external-link">ELMo</a> all benefit from this.</p>
<p>Much like the language models we can train computer vision models
using similar approaches. To make things work in computer vision, we
need to formulate the learning tasks such that the underlying model (a
deep neural network) is able to make sense of the semantic information
present in vision data. One such task is to a model to <em>contrast</em>
between two different versions of the same image. The hope is that in
this way the model will have learn representations where the similar
images are grouped as together possible while the dissimilar images are
further away.</p>
<p>In this example, we will be implementing one such system called
<strong>SimSiam</strong> proposed in <a href="https://arxiv.org/abs/2011.10566" class="external-link">Exploring Simple Siamese
Representation Learning</a>. It is implemented as the following:</p>
<ol style="list-style-type: decimal">
<li>We create two different versions of the same dataset with a
stochastic data augmentation pipeline. Note that the random
initialization seed needs to be the same during create these
versions.</li>
<li>We take a ResNet without any classification head
(<strong>backbone</strong>) and we add a shallow fully-connected network
(<strong>projection head</strong>) on top of it. Collectively, this is
known as the <strong>encoder</strong>.</li>
<li>We pass the output of the encoder through a
<strong>predictor</strong> which is again a shallow fully-connected
network having an <a href="https://en.wikipedia.org/wiki/Autoencoder" class="external-link">AutoEncoder</a> like
structure.</li>
<li>We then train our encoder to maximize the cosine similarity between
the two different versions of our dataset.</li>
</ol>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>os.environ[<span class="st">'KERAS_BACKEND'</span>] <span class="op">=</span> <span class="st">'tensorflow'</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> regularizers</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="define-hyperparameters">Define hyperparameters<a class="anchor" aria-label="anchor" href="#define-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>AUTO <span class="op">=</span> tf.data.AUTOTUNE</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>EPOCHS <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>CROP_TO <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>SEED <span class="op">=</span> <span class="dv">26</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>PROJECT_DIM <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>LATENT_DIM <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">0.0005</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-cifar-10-dataset">Load the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#load-the-cifar-10-dataset"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total training examples: </span><span class="sc">{</span><span class="bu">len</span>(x_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total test examples: </span><span class="sc">{</span><span class="bu">len</span>(x_test)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="defining-our-data-augmentation-pipeline">Defining our data augmentation pipeline<a class="anchor" aria-label="anchor" href="#defining-our-data-augmentation-pipeline"></a>
</h2>
<p>As studied in <a href="https://arxiv.org/abs/2002.05709" class="external-link">SimCLR</a>
having the right data augmentation pipeline is critical for SSL systems
to work effectively in computer vision. Two particular augmentation
transforms that seem to matter the most are: 1.) Random resized crops
and 2.) Color distortions. Most of the other SSL systems for computer
vision (such as <a href="https://arxiv.org/abs/2006.07733" class="external-link">BYOL</a>, <a href="https://arxiv.org/abs/2003.04297" class="external-link">MoCoV2</a>, <a href="https://arxiv.org/abs/2006.09882" class="external-link">SwAV</a>, etc.) include these in
their training pipelines.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">def</span> flip_random_crop(image):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="co"># With random crops we also apply horizontal flipping.</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    image <span class="op">=</span> tf.image.random_flip_left_right(image)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    image <span class="op">=</span> tf.image.random_crop(image, (CROP_TO, CROP_TO, <span class="dv">3</span>))</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="kw">def</span> color_jitter(x, strength<span class="op">=</span>[<span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.1</span>]):</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    x <span class="op">=</span> tf.image.random_brightness(x, max_delta<span class="op">=</span><span class="fl">0.8</span> <span class="op">*</span> strength[<span class="dv">0</span>])</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    x <span class="op">=</span> tf.image.random_contrast(</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        x, lower<span class="op">=</span><span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">*</span> strength[<span class="dv">1</span>], upper<span class="op">=</span><span class="dv">1</span> <span class="op">+</span> <span class="fl">0.8</span> <span class="op">*</span> strength[<span class="dv">1</span>]</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    )</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    x <span class="op">=</span> tf.image.random_saturation(</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        x, lower<span class="op">=</span><span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">*</span> strength[<span class="dv">2</span>], upper<span class="op">=</span><span class="dv">1</span> <span class="op">+</span> <span class="fl">0.8</span> <span class="op">*</span> strength[<span class="dv">2</span>]</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    )</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    x <span class="op">=</span> tf.image.random_hue(x, max_delta<span class="op">=</span><span class="fl">0.2</span> <span class="op">*</span> strength[<span class="dv">3</span>])</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    <span class="co"># Affine transformations can disturb the natural range of</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="co"># RGB images, hence this is needed.</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    x <span class="op">=</span> tf.clip_by_value(x, <span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a><span class="kw">def</span> color_drop(x):</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    x <span class="op">=</span> tf.image.rgb_to_grayscale(x)</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>    x <span class="op">=</span> tf.tile(x, [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a><span class="kw">def</span> random_apply(func, x, p):</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>    <span class="cf">if</span> tf.random.uniform([], minval<span class="op">=</span><span class="dv">0</span>, maxval<span class="op">=</span><span class="dv">1</span>) <span class="op">&lt;</span> p:</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>        <span class="cf">return</span> func(x)</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a><span class="kw">def</span> custom_augment(image):</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    <span class="co"># As discussed in the SimCLR paper, the series of augmentation</span></span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>    <span class="co"># transformations (except for random crops) need to be applied</span></span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>    <span class="co"># randomly to impose translational invariance.</span></span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    image <span class="op">=</span> flip_random_crop(image)</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>    image <span class="op">=</span> random_apply(color_jitter, image, p<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>    image <span class="op">=</span> random_apply(color_drop, image, p<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>    <span class="cf">return</span> image</span></code></pre></div>
<p>It should be noted that an augmentation pipeline is generally
dependent on various properties of the dataset we are dealing with. For
example, if images in the dataset are heavily object-centric then taking
random crops with a very high probability may hurt the training
performance.</p>
<p>Let’s now apply our augmentation pipeline to our dataset and
visualize a few outputs.</p>
</div>
<div class="section level2">
<h2 id="convert-the-data-into-tensorflow-dataset-objects">Convert the data into TensorFlow <code>Dataset</code> objects<a class="anchor" aria-label="anchor" href="#convert-the-data-into-tensorflow-dataset-objects"></a>
</h2>
<p>Here we create two different versions of our dataset <em>without</em>
any ground-truth labels.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>ssl_ds_one <span class="op">=</span> tf.data.Dataset.from_tensor_slices(x_train)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>ssl_ds_one <span class="op">=</span> (</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    ssl_ds_one.shuffle(<span class="dv">1024</span>, seed<span class="op">=</span>SEED)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    .<span class="bu">map</span>(custom_augment, num_parallel_calls<span class="op">=</span>AUTO)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    .prefetch(AUTO)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>ssl_ds_two <span class="op">=</span> tf.data.Dataset.from_tensor_slices(x_train)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>ssl_ds_two <span class="op">=</span> (</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    ssl_ds_two.shuffle(<span class="dv">1024</span>, seed<span class="op">=</span>SEED)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    .<span class="bu">map</span>(custom_augment, num_parallel_calls<span class="op">=</span>AUTO)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    .prefetch(AUTO)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a><span class="co"># We then zip both of these datasets.</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>ssl_ds <span class="op">=</span> tf.data.Dataset.<span class="bu">zip</span>((ssl_ds_one, ssl_ds_two))</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a><span class="co"># Visualize a few augmented images.</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>sample_images_one <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(ssl_ds_one))</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, n <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>    plt.imshow(sample_images_one[n].numpy().astype(<span class="st">"int"</span>))</span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>plt.show()</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a><span class="co"># Ensure that the different versions of the dataset actually contain</span></span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a><span class="co"># identical images.</span></span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>sample_images_two <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(ssl_ds_two))</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">25</span>):</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>    ax <span class="op">=</span> plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, n <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>    plt.imshow(sample_images_two[n].numpy().astype(<span class="st">"int"</span>))</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>    plt.axis(<span class="st">"off"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>Notice that the images in <code>samples_images_one</code> and
<code>sample_images_two</code> are essentially the same but are
augmented differently.</p>
</div>
<div class="section level2">
<h2 id="defining-the-encoder-and-the-predictor">Defining the encoder and the predictor<a class="anchor" aria-label="anchor" href="#defining-the-encoder-and-the-predictor"></a>
</h2>
<p>We use an implementation of ResNet20 that is specifically configured
for the CIFAR10 dataset. The code is taken from the <a href="https://github.com/GoogleCloudPlatform/keras-idiomatic-programmer/blob/master/zoo/resnet/resnet_cifar10_v2.py" class="external-link">keras-idiomatic-programmer</a>
repository. The hyperparameters of these architectures have been
referred from Section 3 and Appendix A of <a href="https://arxiv.org/abs/2011.10566" class="external-link">the original paper</a>.</p>
<p>wget -q <a href="https://shorturl.at/QS369" class="external-link uri">https://shorturl.at/QS369</a> -O resnet_cifar10_v2.py</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> resnet_cifar10_v2</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>DEPTH <span class="op">=</span> N <span class="op">*</span> <span class="dv">9</span> <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>NUM_BLOCKS <span class="op">=</span> ((DEPTH <span class="op">-</span> <span class="dv">2</span>) <span class="op">//</span> <span class="dv">9</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="kw">def</span> get_encoder():</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    <span class="co"># Input and backbone.</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input((CROP_TO, CROP_TO, <span class="dv">3</span>))</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    x <span class="op">=</span> layers.Rescaling(scale<span class="op">=</span><span class="fl">1.0</span> <span class="op">/</span> <span class="fl">127.5</span>, offset<span class="op">=-</span><span class="dv">1</span>)(inputs)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    x <span class="op">=</span> resnet_cifar10_v2.stem(x)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    x <span class="op">=</span> resnet_cifar10_v2.learner(x, NUM_BLOCKS)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    x <span class="op">=</span> layers.GlobalAveragePooling2D(name<span class="op">=</span><span class="st">"backbone_pool"</span>)(x)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    <span class="co"># Projection head.</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        PROJECT_DIM,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>regularizers.l2(WEIGHT_DECAY),</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>    )(x)</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    x <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>    x <span class="op">=</span> layers.ReLU()(x)</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        PROJECT_DIM,</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>regularizers.l2(WEIGHT_DECAY),</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>    )(x)</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>    outputs <span class="op">=</span> layers.BatchNormalization()(x)</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs, name<span class="op">=</span><span class="st">"encoder"</span>)</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a><span class="kw">def</span> get_predictor():</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>    model <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>        [</span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>            <span class="co"># Note the AutoEncoder-like structure.</span></span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>            layers.Input((PROJECT_DIM,)),</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>            layers.Dense(</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>                LATENT_DIM,</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>                use_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a>                kernel_regularizer<span class="op">=</span>regularizers.l2(WEIGHT_DECAY),</span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>            ),</span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a>            layers.ReLU(),</span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a>            layers.BatchNormalization(),</span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a>            layers.Dense(PROJECT_DIM),</span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a>        ],</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"predictor"</span>,</span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>    )</span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="defining-the-pre-training-loop">Defining the (pre-)training loop<a class="anchor" aria-label="anchor" href="#defining-the-pre-training-loop"></a>
</h2>
<p>One of the main reasons behind training networks with these kinds of
approaches is to utilize the learned representations for downstream
tasks like classification. This is why this particular training phase is
also referred to as <em>pre-training</em>.</p>
<p>We start by defining the loss function.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> compute_loss(p, z):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="co"># The authors of SimSiam emphasize the impact of</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    <span class="co"># the `stop_gradient` operator in the paper as it</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    <span class="co"># has an important role in the overall optimization.</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    z <span class="op">=</span> tf.stop_gradient(z)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    p <span class="op">=</span> tf.math.l2_normalize(p, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    z <span class="op">=</span> tf.math.l2_normalize(z, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="co"># Negative cosine similarity (minimizing this is</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>    <span class="co"># equivalent to maximizing the similarity).</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>tf.reduce_mean(tf.reduce_sum((p <span class="op">*</span> z), axis<span class="op">=</span><span class="dv">1</span>))</span></code></pre></div>
<p>We then define our training loop by overriding the
<code>train_step()</code> function of the <code>keras.Model</code>
class.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> SimSiam(keras.Model):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, encoder, predictor):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> encoder</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        <span class="va">self</span>.predictor <span class="op">=</span> predictor</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="va">self</span>.loss_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"loss"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    <span class="kw">def</span> metrics(<span class="va">self</span>):</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.loss_tracker]</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, data):</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>        <span class="co"># Unpack the data.</span></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        ds_one, ds_two <span class="op">=</span> data</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>        <span class="co"># Forward pass through the encoder and predictor.</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>            z1, z2 <span class="op">=</span> <span class="va">self</span>.encoder(ds_one), <span class="va">self</span>.encoder(ds_two)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>            p1, p2 <span class="op">=</span> <span class="va">self</span>.predictor(z1), <span class="va">self</span>.predictor(z2)</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>            <span class="co"># Note that here we are enforcing the network to match</span></span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>            <span class="co"># the representations of two differently augmented batches</span></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>            <span class="co"># of data.</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>            loss <span class="op">=</span> compute_loss(p1, z2) <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> compute_loss(p2, z1) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>        <span class="co"># Compute gradients and update the parameters.</span></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>        learnable_params <span class="op">=</span> (</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>            <span class="va">self</span>.encoder.trainable_variables</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>            <span class="op">+</span> <span class="va">self</span>.predictor.trainable_variables</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>        )</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>        gradients <span class="op">=</span> tape.gradient(loss, learnable_params)</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(gradients, learnable_params))</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>        <span class="co"># Monitor loss.</span></span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a>        <span class="va">self</span>.loss_tracker.update_state(loss)</span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>        <span class="cf">return</span> {<span class="st">"loss"</span>: <span class="va">self</span>.loss_tracker.result()}</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="pre-training-our-networks">Pre-training our networks<a class="anchor" aria-label="anchor" href="#pre-training-our-networks"></a>
</h2>
<p>In the interest of this example, we will train the model for only 5
epochs. In reality, this should at least be 100 epochs.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Create a cosine decay learning scheduler.</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>num_training_samples <span class="op">=</span> <span class="bu">len</span>(x_train)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>steps <span class="op">=</span> EPOCHS <span class="op">*</span> (num_training_samples <span class="op">//</span> BATCH_SIZE)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>lr_decayed_fn <span class="op">=</span> keras.optimizers.schedules.CosineDecay(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    initial_learning_rate<span class="op">=</span><span class="fl">0.03</span>, decay_steps<span class="op">=</span>steps</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co"># Create an early stopping callback.</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>early_stopping <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    monitor<span class="op">=</span><span class="st">"loss"</span>, patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a><span class="co"># Compile model and start training.</span></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>simsiam <span class="op">=</span> SimSiam(get_encoder(), get_predictor())</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>simsiam.<span class="bu">compile</span>(optimizer<span class="op">=</span>keras.optimizers.SGD(lr_decayed_fn, momentum<span class="op">=</span><span class="fl">0.6</span>))</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>history <span class="op">=</span> simsiam.fit(ssl_ds, epochs<span class="op">=</span>EPOCHS, callbacks<span class="op">=</span>[early_stopping])</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a><span class="co"># Visualize the training progress of the model.</span></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>])</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>plt.grid()</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>plt.title(<span class="st">"Negative Cosine Similairty"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>If your solution gets very close to -1 (minimum value of our loss)
very quickly with a different dataset and a different backbone
architecture that is likely because of <em>representation collapse</em>.
It is a phenomenon where the encoder yields similar output for all the
images. In that case additional hyperparameter tuning is required
especially in the following areas:</p>
<ul>
<li>Strength of the color distortions and their probabilities.</li>
<li>Learning rate and its schedule.</li>
<li>Architecture of both the backbone and their projection head.</li>
</ul>
</div>
<div class="section level2">
<h2 id="evaluating-our-ssl-method">Evaluating our SSL method<a class="anchor" aria-label="anchor" href="#evaluating-our-ssl-method"></a>
</h2>
<p>The most popularly used method to evaluate a SSL method in computer
vision (or any other pre-training method as such) is to learn a linear
classifier on the frozen features of the trained backbone model (in this
case it is ResNet20) and evaluate the classifier on unseen images. Other
methods include <a href="https://keras.io/guides/transfer_learning/" class="external-link">fine-tuning</a> on the
source dataset or even a target dataset with 5% or 10% labels present.
Practically, we can use the backbone model for any downstream task such
as semantic segmentation, object detection, and so on where the backbone
models are usually pre-trained with <em>pure supervised
learning</em>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># We first create labeled `Dataset` objects.</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>train_ds <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_train, y_train))</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>test_ds <span class="op">=</span> tf.data.Dataset.from_tensor_slices((x_test, y_test))</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co"># Then we shuffle, batch, and prefetch this dataset for performance. We</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co"># also apply random resized crops as an augmentation but only to the</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># training set.</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>train_ds <span class="op">=</span> (</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    train_ds.shuffle(<span class="dv">1024</span>)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    .<span class="bu">map</span>(<span class="kw">lambda</span> x, y: (flip_random_crop(x), y), num_parallel_calls<span class="op">=</span>AUTO)</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    .batch(BATCH_SIZE)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    .prefetch(AUTO)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>test_ds <span class="op">=</span> test_ds.batch(BATCH_SIZE).prefetch(AUTO)</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a><span class="co"># Extract the backbone ResNet20.</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>backbone <span class="op">=</span> keras.Model(</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    simsiam.encoder.<span class="bu">input</span>, simsiam.encoder.get_layer(<span class="st">"backbone_pool"</span>).output</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>)</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a><span class="co"># We then create our linear classifier and train it.</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>backbone.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>inputs <span class="op">=</span> layers.Input((CROP_TO, CROP_TO, <span class="dv">3</span>))</span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>x <span class="op">=</span> backbone(inputs, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>linear_model <span class="op">=</span> keras.Model(inputs, outputs, name<span class="op">=</span><span class="st">"linear_model"</span>)</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a><span class="co"># Compile model and start training.</span></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>linear_model.<span class="bu">compile</span>(</span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.SGD(lr_decayed_fn, momentum<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a>)</span>
<span id="cb10-34"><a href="#cb10-34" tabindex="-1"></a>history <span class="op">=</span> linear_model.fit(</span>
<span id="cb10-35"><a href="#cb10-35" tabindex="-1"></a>    train_ds, validation_data<span class="op">=</span>test_ds, epochs<span class="op">=</span>EPOCHS, callbacks<span class="op">=</span>[early_stopping]</span>
<span id="cb10-36"><a href="#cb10-36" tabindex="-1"></a>)</span>
<span id="cb10-37"><a href="#cb10-37" tabindex="-1"></a>_, test_acc <span class="op">=</span> linear_model.evaluate(test_ds)</span>
<span id="cb10-38"><a href="#cb10-38" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy: </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(test_acc <span class="op">*</span> <span class="dv">100</span>))</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h2>
<ul>
<li>More data and longer pre-training schedule benefit SSL in
general.</li>
<li>SSL is particularly very helpful when you do not have access to very
limited <em>labeled</em> training data but you can manage to build a
large corpus of unlabeled data. Recently, using an SSL method called <a href="https://arxiv.org/abs/2006.09882" class="external-link">SwAV</a>, a group of researchers
at Facebook trained a <a href="https://arxiv.org/abs/2006.09882" class="external-link">RegNet</a> on 2 Billion images.
They were able to achieve downstream performance very close to those
achieved by pure supervised pre-training. For some downstream tasks,
their method even outperformed the supervised counterparts. You can
check out <a href="https://arxiv.org/pdf/2103.01988.pdf" class="external-link">their paper</a>
to know the details.</li>
<li>If you are interested to understand why contrastive SSL helps
networks learn meaningful representations, you can check out the
following resources:
<ul>
<li><a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/" class="external-link">Self-supervised
learning: The dark matter of intelligence</a></li>
<li><a href="https://sslneuips20.github.io/files/CameraReadys%203-77/64/CameraReady/Understanding_self_supervised_learning.pdf" class="external-link">Understanding
self-supervised learning using controlled datasets with known
structure</a></li>
</ul>
</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
