<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Image classification with Perceiver • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Image classification with Perceiver">
<meta property="og:description" content="Implementing the Perceiver model for image classification.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Guides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Image classification with Perceiver</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/khalid-salama-24403144/" class="external-link">Khalid
Salama</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/perceiver_image_classification.Rmd" class="external-link"><code>vignettes/examples/perceiver_image_classification.Rmd</code></a></small>
      <div class="hidden name"><code>perceiver_image_classification.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example implements the <a href="https://arxiv.org/abs/2103.03206" class="external-link">Perceiver: General Perception
with Iterative Attention</a> model by Andrew Jaegle et al. for image
classification, and demonstrates it on the CIFAR-100 dataset.</p>
<p>The Perceiver model leverages an asymmetric attention mechanism to
iteratively distill inputs into a tight latent bottleneck, allowing it
to scale to handle very large inputs.</p>
<p>In other words: let’s assume that your input data array (e.g. image)
has <code>M</code> elements (i.e. patches), where <code>M</code> is
large. In a standard Transformer model, a self-attention operation is
performed for the <code>M</code> elements. The complexity of this
operation is <code>O(M^2)</code>. However, the Perceiver model creates a
latent array of size <code>N</code> elements, where
<code>N &lt;&lt; M</code>, and performs two operations iteratively:</p>
<ol style="list-style-type: decimal">
<li>Cross-attention Transformer between the latent array and the data
array - The complexity of this operation is <code>O(M.N)</code>.</li>
<li>Self-attention Transformer on the latent array - The complexity of
this operation is <code>O(N^2)</code>.</li>
</ol>
<p>This example requires TensorFlow 2.4 or higher, as well as <a href="https://www.tensorflow.org/addons/overview" class="external-link">TensorFlow Addons</a>,
which can be installed using the following command:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>pip install <span class="op">-</span>U tensorflow<span class="op">-</span>addons</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar100.load_data()</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configure-the-hyperparameters">Configure the hyperparameters<a class="anchor" aria-label="anchor" href="#configure-the-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>dropout_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">64</span>  <span class="co"># We'll resize input images to this size.</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Size of the patches to be extract from the input images.</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>num_patches <span class="op">=</span> (image_size <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span>  <span class="co"># Size of the data array.</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>latent_dim <span class="op">=</span> <span class="dv">256</span>  <span class="co"># Size of the latent array.</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>projection_dim <span class="op">=</span> (</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="dv">256</span>  <span class="co"># Embedding size of each element in the data and latent arrays.</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Number of Transformer heads.</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>ffn_units <span class="op">=</span> [</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    projection_dim,</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    projection_dim,</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>]  <span class="co"># Size of the Transformer Feedforward network.</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>num_transformer_blocks <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>num_iterations <span class="op">=</span> (</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    <span class="dv">2</span>  <span class="co"># Repetitions of the cross-attention and Transformer modules.</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>)</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>classifier_units <span class="op">=</span> [</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    projection_dim,</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    num_classes,</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>]  <span class="co"># Size of the Feedforward network of the final classifier.</span></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image size: </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>image_size<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>image_size <span class="op">**</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch size: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_size <span class="op">**</span> <span class="dv">2</span><span class="sc">}</span><span class="ss"> "</span>)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patches per image: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Elements per patch (3 channels): </span><span class="sc">{</span>(patch_size <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> <span class="dv">3</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Latent array shape: </span><span class="sc">{</span>latent_dim<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>projection_dim<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data array shape: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>projection_dim<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
<p>Note that, in order to use each pixel as an individual input in the
data array, set <code>patch_size</code> to 1.</p>
</div>
<div class="section level2">
<h2 id="use-data-augmentation">Use data augmentation<a class="anchor" aria-label="anchor" href="#use-data-augmentation"></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    [</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        layers.Normalization(),</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        layers.Resizing(image_size, image_size),</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        layers.RandomZoom(height_factor<span class="op">=</span><span class="fl">0.2</span>, width_factor<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    ],</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co"># Compute the mean and the variance of the training data for normalization.</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>data_augmentation.layers[<span class="dv">0</span>].adapt(x_train)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-feedforward-network-ffn">Implement Feedforward network (FFN)<a class="anchor" aria-label="anchor" href="#implement-feedforward-network-ffn"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> create_ffn(hidden_units, dropout_rate):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    ffn_layers <span class="op">=</span> []</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="cf">for</span> units <span class="kw">in</span> hidden_units[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>        ffn_layers.append(layers.Dense(units, activation<span class="op">=</span>tf.nn.gelu))</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    ffn_layers.append(layers.Dense(units<span class="op">=</span>hidden_units[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    ffn_layers.append(layers.Dropout(dropout_rate))</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    ffn <span class="op">=</span> keras.Sequential(ffn_layers)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    <span class="cf">return</span> ffn</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-patch-creation-as-a-layer">Implement patch creation as a layer<a class="anchor" aria-label="anchor" href="#implement-patch-creation-as-a-layer"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> Patches(layers.Layer):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, images):</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        batch_size <span class="op">=</span> tf.shape(images)[<span class="dv">0</span>]</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        patches <span class="op">=</span> tf.image.extract_patches(</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>            images<span class="op">=</span>images,</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>            sizes<span class="op">=</span>[<span class="dv">1</span>, <span class="va">self</span>.patch_size, <span class="va">self</span>.patch_size, <span class="dv">1</span>],</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>            strides<span class="op">=</span>[<span class="dv">1</span>, <span class="va">self</span>.patch_size, <span class="va">self</span>.patch_size, <span class="dv">1</span>],</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>            rates<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            padding<span class="op">=</span><span class="st">"VALID"</span>,</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>        )</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>        patch_dims <span class="op">=</span> patches.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>        patches <span class="op">=</span> tf.reshape(patches, [batch_size, <span class="op">-</span><span class="dv">1</span>, patch_dims])</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>        <span class="cf">return</span> patches</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-patch-encoding-layer">Implement the patch encoding layer<a class="anchor" aria-label="anchor" href="#implement-the-patch-encoding-layer"></a>
</h2>
<p>The <code>PatchEncoder</code> layer will linearly transform a patch
by projecting it into a vector of size <code>latent_dim</code>. In
addition, it adds a learnable position embedding to the projected
vector.</p>
<p>Note that the orginal Perceiver paper uses the Fourier feature
positional encodings.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> PatchEncoder(layers.Layer):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patches, projection_dim):</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        <span class="va">self</span>.num_patches <span class="op">=</span> num_patches</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> layers.Dense(units<span class="op">=</span>projection_dim)</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> layers.Embedding(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>            input_dim<span class="op">=</span>num_patches, output_dim<span class="op">=</span>projection_dim</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>        )</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, patches):</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>        positions <span class="op">=</span> tf.<span class="bu">range</span>(start<span class="op">=</span><span class="dv">0</span>, limit<span class="op">=</span><span class="va">self</span>.num_patches, delta<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>        encoded <span class="op">=</span> <span class="va">self</span>.projection(patches) <span class="op">+</span> <span class="va">self</span>.position_embedding(positions)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>        <span class="cf">return</span> encoded</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-the-perceiver-model">Build the Perceiver model<a class="anchor" aria-label="anchor" href="#build-the-perceiver-model"></a>
</h2>
<p>The Perceiver consists of two modules: a cross-attention module and a
standard Transformer with self-attention.</p>
<div class="section level3">
<h3 id="cross-attention-module">Cross-attention module<a class="anchor" aria-label="anchor" href="#cross-attention-module"></a>
</h3>
<p>The cross-attention expects a
<code>(latent_dim, projection_dim)</code> latent array, and the
<code>(data_dim,  projection_dim)</code> data array as inputs, to
produce a <code>(latent_dim, projection_dim)</code> latent array as an
output. To apply cross-attention, the <code>query</code> vectors are
generated from the latent array, while the <code>key</code> and
<code>value</code> vectors are generated from the encoded image.</p>
<p>Note that the data array in this example is the image, where the
<code>data_dim</code> is set to the <code>num_patches</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> create_cross_attention_module(</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    latent_dim, data_dim, projection_dim, ffn_units, dropout_rate</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>):</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    inputs <span class="op">=</span> {</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        <span class="co"># Recieve the latent array as an input of shape [1, latent_dim, projection_dim].</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        <span class="st">"latent_array"</span>: layers.Input(</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>            shape<span class="op">=</span>(latent_dim, projection_dim), name<span class="op">=</span><span class="st">"latent_array"</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>        ),</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        <span class="co"># Recieve the data_array (encoded image) as an input of shape [batch_size, data_dim, projection_dim].</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>        <span class="st">"data_array"</span>: layers.Input(</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>            shape<span class="op">=</span>(data_dim, projection_dim), name<span class="op">=</span><span class="st">"data_array"</span></span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        ),</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>    }</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    <span class="co"># Apply layer norm to the inputs</span></span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>    latent_array <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>        inputs[<span class="st">"latent_array"</span>]</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>    )</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>    data_array <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(inputs[<span class="st">"data_array"</span>])</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>    <span class="co"># Create query tensor: [1, latent_dim, projection_dim].</span></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>    query <span class="op">=</span> layers.Dense(units<span class="op">=</span>projection_dim)(latent_array)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>    <span class="co"># Create key tensor: [batch_size, data_dim, projection_dim].</span></span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>    key <span class="op">=</span> layers.Dense(units<span class="op">=</span>projection_dim)(data_array)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>    <span class="co"># Create value tensor: [batch_size, data_dim, projection_dim].</span></span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>    value <span class="op">=</span> layers.Dense(units<span class="op">=</span>projection_dim)(data_array)</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>    <span class="co"># Generate cross-attention outputs: [batch_size, latent_dim, projection_dim].</span></span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>    attention_output <span class="op">=</span> layers.Attention(use_scale<span class="op">=</span><span class="va">True</span>, dropout<span class="op">=</span><span class="fl">0.1</span>)(</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>        [query, key, value], return_attention_scores<span class="op">=</span><span class="va">False</span></span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>    )</span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>    <span class="co"># Skip connection 1.</span></span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>    attention_output <span class="op">=</span> layers.Add()([attention_output, latent_array])</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>    <span class="co"># Apply layer norm.</span></span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>    attention_output <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(attention_output)</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>    <span class="co"># Apply Feedforward network.</span></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>    ffn <span class="op">=</span> create_ffn(hidden_units<span class="op">=</span>ffn_units, dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>    outputs <span class="op">=</span> ffn(attention_output)</span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>    <span class="co"># Skip connection 2.</span></span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Add()([outputs, attention_output])</span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="transformer-module">Transformer module<a class="anchor" aria-label="anchor" href="#transformer-module"></a>
</h3>
<p>The Transformer expects the output latent vector from the
cross-attention module as an input, applies multi-head self-attention to
its <code>latent_dim</code> elements, followed by feedforward network,
to produce another <code>(latent_dim, projection_dim)</code> latent
array.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> create_transformer_module(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    latent_dim,</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    projection_dim,</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    num_heads,</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    num_transformer_blocks,</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    ffn_units,</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    dropout_rate,</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>):</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>    <span class="co"># input_shape: [1, latent_dim, projection_dim]</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>(latent_dim, projection_dim))</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    x0 <span class="op">=</span> inputs</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    <span class="co"># Create multiple layers of the Transformer block.</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_transformer_blocks):</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>        <span class="co"># Apply layer normalization 1.</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>        x1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(x0)</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>        <span class="co"># Create a multi-head self-attention layer.</span></span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>        attention_output <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads, key_dim<span class="op">=</span>projection_dim, dropout<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>        )(x1, x1)</span>
<span id="cb10-21"><a href="#cb10-21" tabindex="-1"></a>        <span class="co"># Skip connection 1.</span></span>
<span id="cb10-22"><a href="#cb10-22" tabindex="-1"></a>        x2 <span class="op">=</span> layers.Add()([attention_output, x0])</span>
<span id="cb10-23"><a href="#cb10-23" tabindex="-1"></a>        <span class="co"># Apply layer normalization 2.</span></span>
<span id="cb10-24"><a href="#cb10-24" tabindex="-1"></a>        x3 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-6</span>)(x2)</span>
<span id="cb10-25"><a href="#cb10-25" tabindex="-1"></a>        <span class="co"># Apply Feedforward network.</span></span>
<span id="cb10-26"><a href="#cb10-26" tabindex="-1"></a>        ffn <span class="op">=</span> create_ffn(hidden_units<span class="op">=</span>ffn_units, dropout_rate<span class="op">=</span>dropout_rate)</span>
<span id="cb10-27"><a href="#cb10-27" tabindex="-1"></a>        x3 <span class="op">=</span> ffn(x3)</span>
<span id="cb10-28"><a href="#cb10-28" tabindex="-1"></a>        <span class="co"># Skip connection 2.</span></span>
<span id="cb10-29"><a href="#cb10-29" tabindex="-1"></a>        x0 <span class="op">=</span> layers.Add()([x3, x2])</span>
<span id="cb10-30"><a href="#cb10-30" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" tabindex="-1"></a>    <span class="co"># Create the Keras model.</span></span>
<span id="cb10-32"><a href="#cb10-32" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>x0)</span>
<span id="cb10-33"><a href="#cb10-33" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="perceiver-model">Perceiver model<a class="anchor" aria-label="anchor" href="#perceiver-model"></a>
</h3>
<p>The Perceiver model repeats the cross-attention and Transformer
modules <code>num_iterations</code> times—with shared weights and skip
connections—to allow the latent array to iteratively extract information
from the input image as it is needed.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">class</span> Perceiver(keras.Model):</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>        patch_size,</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        data_dim,</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        latent_dim,</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        projection_dim,</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        num_heads,</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>        num_transformer_blocks,</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>        ffn_units,</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>        dropout_rate,</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>        num_iterations,</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>        classifier_units,</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>    ):</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a>        <span class="va">self</span>.latent_dim <span class="op">=</span> latent_dim</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>        <span class="va">self</span>.data_dim <span class="op">=</span> data_dim</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>        <span class="va">self</span>.projection_dim <span class="op">=</span> projection_dim</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>        <span class="va">self</span>.num_transformer_blocks <span class="op">=</span> num_transformer_blocks</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>        <span class="va">self</span>.ffn_units <span class="op">=</span> ffn_units</span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>        <span class="va">self</span>.dropout_rate <span class="op">=</span> dropout_rate</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>        <span class="va">self</span>.num_iterations <span class="op">=</span> num_iterations</span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a>        <span class="va">self</span>.classifier_units <span class="op">=</span> classifier_units</span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a>        <span class="co"># Create latent array.</span></span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a>        <span class="va">self</span>.latent_array <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.latent_dim, <span class="va">self</span>.projection_dim),</span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a>        )</span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a></span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>        <span class="co"># Create patching module.</span></span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a>        <span class="va">self</span>.patcher <span class="op">=</span> Patches(<span class="va">self</span>.patch_size)</span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a></span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a>        <span class="co"># Create patch encoder.</span></span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a>        <span class="va">self</span>.patch_encoder <span class="op">=</span> PatchEncoder(<span class="va">self</span>.data_dim, <span class="va">self</span>.projection_dim)</span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" tabindex="-1"></a>        <span class="co"># Create cross-attenion module.</span></span>
<span id="cb11-43"><a href="#cb11-43" tabindex="-1"></a>        <span class="va">self</span>.cross_attention <span class="op">=</span> create_cross_attention_module(</span>
<span id="cb11-44"><a href="#cb11-44" tabindex="-1"></a>            <span class="va">self</span>.latent_dim,</span>
<span id="cb11-45"><a href="#cb11-45" tabindex="-1"></a>            <span class="va">self</span>.data_dim,</span>
<span id="cb11-46"><a href="#cb11-46" tabindex="-1"></a>            <span class="va">self</span>.projection_dim,</span>
<span id="cb11-47"><a href="#cb11-47" tabindex="-1"></a>            <span class="va">self</span>.ffn_units,</span>
<span id="cb11-48"><a href="#cb11-48" tabindex="-1"></a>            <span class="va">self</span>.dropout_rate,</span>
<span id="cb11-49"><a href="#cb11-49" tabindex="-1"></a>        )</span>
<span id="cb11-50"><a href="#cb11-50" tabindex="-1"></a></span>
<span id="cb11-51"><a href="#cb11-51" tabindex="-1"></a>        <span class="co"># Create Transformer module.</span></span>
<span id="cb11-52"><a href="#cb11-52" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> create_transformer_module(</span>
<span id="cb11-53"><a href="#cb11-53" tabindex="-1"></a>            <span class="va">self</span>.latent_dim,</span>
<span id="cb11-54"><a href="#cb11-54" tabindex="-1"></a>            <span class="va">self</span>.projection_dim,</span>
<span id="cb11-55"><a href="#cb11-55" tabindex="-1"></a>            <span class="va">self</span>.num_heads,</span>
<span id="cb11-56"><a href="#cb11-56" tabindex="-1"></a>            <span class="va">self</span>.num_transformer_blocks,</span>
<span id="cb11-57"><a href="#cb11-57" tabindex="-1"></a>            <span class="va">self</span>.ffn_units,</span>
<span id="cb11-58"><a href="#cb11-58" tabindex="-1"></a>            <span class="va">self</span>.dropout_rate,</span>
<span id="cb11-59"><a href="#cb11-59" tabindex="-1"></a>        )</span>
<span id="cb11-60"><a href="#cb11-60" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" tabindex="-1"></a>        <span class="co"># Create global average pooling layer.</span></span>
<span id="cb11-62"><a href="#cb11-62" tabindex="-1"></a>        <span class="va">self</span>.global_average_pooling <span class="op">=</span> layers.GlobalAveragePooling1D()</span>
<span id="cb11-63"><a href="#cb11-63" tabindex="-1"></a></span>
<span id="cb11-64"><a href="#cb11-64" tabindex="-1"></a>        <span class="co"># Create a classification head.</span></span>
<span id="cb11-65"><a href="#cb11-65" tabindex="-1"></a>        <span class="va">self</span>.classification_head <span class="op">=</span> create_ffn(</span>
<span id="cb11-66"><a href="#cb11-66" tabindex="-1"></a>            hidden_units<span class="op">=</span><span class="va">self</span>.classifier_units, dropout_rate<span class="op">=</span><span class="va">self</span>.dropout_rate</span>
<span id="cb11-67"><a href="#cb11-67" tabindex="-1"></a>        )</span>
<span id="cb11-68"><a href="#cb11-68" tabindex="-1"></a></span>
<span id="cb11-69"><a href="#cb11-69" tabindex="-1"></a>        <span class="bu">super</span>().build(input_shape)</span>
<span id="cb11-70"><a href="#cb11-70" tabindex="-1"></a></span>
<span id="cb11-71"><a href="#cb11-71" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb11-72"><a href="#cb11-72" tabindex="-1"></a>        <span class="co"># Augment data.</span></span>
<span id="cb11-73"><a href="#cb11-73" tabindex="-1"></a>        augmented <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb11-74"><a href="#cb11-74" tabindex="-1"></a>        <span class="co"># Create patches.</span></span>
<span id="cb11-75"><a href="#cb11-75" tabindex="-1"></a>        patches <span class="op">=</span> <span class="va">self</span>.patcher(augmented)</span>
<span id="cb11-76"><a href="#cb11-76" tabindex="-1"></a>        <span class="co"># Encode patches.</span></span>
<span id="cb11-77"><a href="#cb11-77" tabindex="-1"></a>        encoded_patches <span class="op">=</span> <span class="va">self</span>.patch_encoder(patches)</span>
<span id="cb11-78"><a href="#cb11-78" tabindex="-1"></a>        <span class="co"># Prepare cross-attention inputs.</span></span>
<span id="cb11-79"><a href="#cb11-79" tabindex="-1"></a>        cross_attention_inputs <span class="op">=</span> {</span>
<span id="cb11-80"><a href="#cb11-80" tabindex="-1"></a>            <span class="st">"latent_array"</span>: tf.expand_dims(<span class="va">self</span>.latent_array, <span class="dv">0</span>),</span>
<span id="cb11-81"><a href="#cb11-81" tabindex="-1"></a>            <span class="st">"data_array"</span>: encoded_patches,</span>
<span id="cb11-82"><a href="#cb11-82" tabindex="-1"></a>        }</span>
<span id="cb11-83"><a href="#cb11-83" tabindex="-1"></a>        <span class="co"># Apply the cross-attention and the Transformer modules iteratively.</span></span>
<span id="cb11-84"><a href="#cb11-84" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_iterations):</span>
<span id="cb11-85"><a href="#cb11-85" tabindex="-1"></a>            <span class="co"># Apply cross-attention from the latent array to the data array.</span></span>
<span id="cb11-86"><a href="#cb11-86" tabindex="-1"></a>            latent_array <span class="op">=</span> <span class="va">self</span>.cross_attention(cross_attention_inputs)</span>
<span id="cb11-87"><a href="#cb11-87" tabindex="-1"></a>            <span class="co"># Apply self-attention Transformer to the latent array.</span></span>
<span id="cb11-88"><a href="#cb11-88" tabindex="-1"></a>            latent_array <span class="op">=</span> <span class="va">self</span>.transformer(latent_array)</span>
<span id="cb11-89"><a href="#cb11-89" tabindex="-1"></a>            <span class="co"># Set the latent array of the next iteration.</span></span>
<span id="cb11-90"><a href="#cb11-90" tabindex="-1"></a>            cross_attention_inputs[<span class="st">"latent_array"</span>] <span class="op">=</span> latent_array</span>
<span id="cb11-91"><a href="#cb11-91" tabindex="-1"></a></span>
<span id="cb11-92"><a href="#cb11-92" tabindex="-1"></a>        <span class="co"># Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.</span></span>
<span id="cb11-93"><a href="#cb11-93" tabindex="-1"></a>        representation <span class="op">=</span> <span class="va">self</span>.global_average_pooling(latent_array)</span>
<span id="cb11-94"><a href="#cb11-94" tabindex="-1"></a>        <span class="co"># Generate logits.</span></span>
<span id="cb11-95"><a href="#cb11-95" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classification_head(representation)</span>
<span id="cb11-96"><a href="#cb11-96" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="compile-train-and-evaluate-the-mode">Compile, train, and evaluate the mode<a class="anchor" aria-label="anchor" href="#compile-train-and-evaluate-the-mode"></a>
</h2>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    <span class="co"># Create Adam optimizer with weight decay.</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.Adam(</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>        weight_decay<span class="op">=</span>weight_decay,</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    )</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>    <span class="co"># Compile the model.</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>        loss<span class="op">=</span>keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>            keras.metrics.SparseCategoricalAccuracy(name<span class="op">=</span><span class="st">"acc"</span>),</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>            keras.metrics.SparseTopKCategoricalAccuracy(<span class="dv">5</span>, name<span class="op">=</span><span class="st">"top5-acc"</span>),</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>        ],</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>    )</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>    <span class="co"># Create a learning rate scheduler callback.</span></span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>    reduce_lr <span class="op">=</span> keras.callbacks.ReduceLROnPlateau(</span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>, factor<span class="op">=</span><span class="fl">0.2</span>, patience<span class="op">=</span><span class="dv">3</span></span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>    )</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a>    <span class="co"># Create an early stopping callback.</span></span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a>    early_stopping <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb12-25"><a href="#cb12-25" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">15</span>, restore_best_weights<span class="op">=</span><span class="va">True</span></span>
<span id="cb12-26"><a href="#cb12-26" tabindex="-1"></a>    )</span>
<span id="cb12-27"><a href="#cb12-27" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" tabindex="-1"></a>    <span class="co"># Fit the model.</span></span>
<span id="cb12-29"><a href="#cb12-29" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb12-30"><a href="#cb12-30" tabindex="-1"></a>        x<span class="op">=</span>x_train,</span>
<span id="cb12-31"><a href="#cb12-31" tabindex="-1"></a>        y<span class="op">=</span>y_train,</span>
<span id="cb12-32"><a href="#cb12-32" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size,</span>
<span id="cb12-33"><a href="#cb12-33" tabindex="-1"></a>        epochs<span class="op">=</span>num_epochs,</span>
<span id="cb12-34"><a href="#cb12-34" tabindex="-1"></a>        validation_split<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb12-35"><a href="#cb12-35" tabindex="-1"></a>        callbacks<span class="op">=</span>[early_stopping, reduce_lr],</span>
<span id="cb12-36"><a href="#cb12-36" tabindex="-1"></a>    )</span>
<span id="cb12-37"><a href="#cb12-37" tabindex="-1"></a></span>
<span id="cb12-38"><a href="#cb12-38" tabindex="-1"></a>    _, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb12-39"><a href="#cb12-39" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb12-40"><a href="#cb12-40" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb12-41"><a href="#cb12-41" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" tabindex="-1"></a>    <span class="co"># Return history to plot learning curves.</span></span>
<span id="cb12-43"><a href="#cb12-43" tabindex="-1"></a>    <span class="cf">return</span> history</span></code></pre></div>
<p>Note that training the perceiver model with the current settings on a
V100 GPUs takes around 200 seconds.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>perceiver_classifier <span class="op">=</span> Perceiver(</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    patch_size,</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    num_patches,</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    latent_dim,</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    projection_dim,</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>    num_heads,</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    num_transformer_blocks,</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    ffn_units,</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    dropout_rate,</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>    num_iterations,</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>    classifier_units,</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>history <span class="op">=</span> run_experiment(perceiver_classifier)</span></code></pre></div>
<p>After 40 epochs, the Perceiver model achieves around 53% accuracy and
81% top-5 accuracy on the test data.</p>
<p>As mentioned in the ablations of the <a href="https://arxiv.org/abs/2103.03206" class="external-link">Perceiver</a> paper, you can
obtain better results by increasing the latent array size, increasing
the (projection) dimensions of the latent array and data array elements,
increasing the number of blocks in the Transformer module, and
increasing the number of iterations of applying the cross-attention and
the latent Transformer modules. You may also try to increase the size
the input images and use different patch sizes.</p>
<p>The Perceiver benefits from inceasing the model size. However, larger
models needs bigger accelerators to fit in and train efficiently. This
is why in the Perceiver paper they used 32 TPU cores to run the
experiments.</p>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
