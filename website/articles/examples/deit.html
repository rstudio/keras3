<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Distillation of Vision Transformers through attention.">
<title>Distilling Vision Transformers • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Distilling Vision Transformers">
<meta property="og:description" content="Distillation of Vision Transformers through attention.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Distilling Vision Transformers</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/deit.Rmd" class="external-link"><code>vignettes/examples/deit.Rmd</code></a></small>
      <div class="d-none name"><code>deit.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>In the original <em>Vision Transformers</em> (ViT) paper (<a href="https://arxiv.org/abs/2010.11929" class="external-link">Dosovitskiy et al.</a>), the
authors concluded that to perform on par with Convolutional Neural
Networks (CNNs), ViTs need to be pre-trained on larger datasets. The
larger the better. This is mainly due to the lack of inductive biases in
the ViT architecture – unlike CNNs, they don’t have layers that exploit
locality. In a follow-up paper (<a href="https://arxiv.org/abs/2106.10270" class="external-link">Steiner et al.</a>), the authors
show that it is possible to substantially improve the performance of
ViTs with stronger regularization and longer training.</p>
<p>Many groups have proposed different ways to deal with the problem of
data-intensiveness of ViT training. One such way was shown in the
<em>Data-efficient image Transformers</em>, (DeiT) paper (<a href="https://arxiv.org/abs/2012.12877" class="external-link">Touvron et al.</a>). The authors
introduced a distillation technique that is specific to
transformer-based vision models. DeiT is among the first works to show
that it’s possible to train ViTs well without using larger datasets.</p>
<p>In this example, we implement the distillation recipe proposed in
DeiT. This requires us to slightly tweak the original ViT architecture
and write a custom training loop to implement the distillation
recipe.</p>
<p>To comfortably navigate through this example, you’ll be expected to
know how a ViT and knowledge distillation work. The following are good
resources in case you needed a refresher:</p>
<ul>
<li><a href="https://keras.io/examples/vision/image_classification_with_vision_transformer" class="external-link">ViT
on keras.io</a></li>
<li><a href="https://keras.io/examples/vision/knowledge_distillation/" class="external-link">Knowledge
distillation on keras.io</a></li>
</ul>
</div>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>tfds.disable_progress_bar()</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>keras.utils.set_random_seed(<span class="dv">42</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="constants">Constants<a class="anchor" aria-label="anchor" href="#constants"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>MODEL_TYPE <span class="op">=</span> <span class="st">"deit_distilled_tiny_patch16_224"</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>RESOLUTION <span class="op">=</span> <span class="dv">224</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>PATCH_SIZE <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>NUM_PATCHES <span class="op">=</span> (RESOLUTION <span class="op">//</span> PATCH_SIZE) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>LAYER_NORM_EPS <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>PROJECTION_DIM <span class="op">=</span> <span class="dv">192</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>NUM_HEADS <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>NUM_LAYERS <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>MLP_UNITS <span class="op">=</span> [</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    PROJECTION_DIM <span class="op">*</span> <span class="dv">4</span>,</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>    PROJECTION_DIM,</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>]</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>DROPOUT_RATE <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>DROP_PATH_RATE <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>BASE_LR <span class="op">=</span> <span class="fl">0.0005</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>WEIGHT_DECAY <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>AUTO <span class="op">=</span> tf.data.AUTOTUNE</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>NUM_CLASSES <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
<p>You probably noticed that <code>DROPOUT_RATE</code> has been set 0.0.
Dropout has been used in the implementation to keep it complete. For
smaller models (like the one used in this example), you don’t need it,
but for bigger models, using dropout helps.</p>
</div>
<div class="section level2">
<h2 id="load-the-tf_flowers-dataset-and-prepare-preprocessing-utilities">Load the <code>tf_flowers</code> dataset and prepare preprocessing
utilities<a class="anchor" aria-label="anchor" href="#load-the-tf_flowers-dataset-and-prepare-preprocessing-utilities"></a>
</h2>
<p>The authors use an array of different augmentation techniques,
including MixUp (<a href="https://arxiv.org/abs/1710.09412" class="external-link">Zhang et
al.</a>), RandAugment (<a href="https://arxiv.org/abs/1909.13719" class="external-link">Cubuk
et al.</a>), and so on. However, to keep the example simple to work
through, we’ll discard them.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> preprocess_dataset(is_training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    <span class="kw">def</span> fn(image, label):</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>        <span class="cf">if</span> is_training:</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>            <span class="co"># Resize to a bigger spatial resolution and take the random</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>            <span class="co"># crops.</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>            image <span class="op">=</span> tf.image.resize(image, (RESOLUTION <span class="op">+</span> <span class="dv">20</span>, RESOLUTION <span class="op">+</span> <span class="dv">20</span>))</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>            image <span class="op">=</span> tf.image.random_crop(image, (RESOLUTION, RESOLUTION, <span class="dv">3</span>))</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>            image <span class="op">=</span> tf.image.random_flip_left_right(image)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>            image <span class="op">=</span> tf.image.resize(image, (RESOLUTION, RESOLUTION))</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        label <span class="op">=</span> tf.one_hot(label, depth<span class="op">=</span>NUM_CLASSES)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>        <span class="cf">return</span> image, label</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    <span class="cf">return</span> fn</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="kw">def</span> prepare_dataset(dataset, is_training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    <span class="cf">if</span> is_training:</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.shuffle(BATCH_SIZE <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>        preprocess_dataset(is_training), num_parallel_calls<span class="op">=</span>AUTO</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    )</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    <span class="cf">return</span> dataset.batch(BATCH_SIZE).prefetch(AUTO)</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> tfds.load(</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>    <span class="st">"tf_flowers"</span>, split<span class="op">=</span>[<span class="st">"train[:90%]"</span>, <span class="st">"train[90%:]"</span>], as_supervised<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>)</span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>num_train <span class="op">=</span> train_dataset.cardinality()</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>num_val <span class="op">=</span> val_dataset.cardinality()</span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of training examples: </span><span class="sc">{</span>num_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of validation examples: </span><span class="sc">{</span>num_val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>train_dataset <span class="op">=</span> prepare_dataset(train_dataset, is_training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>val_dataset <span class="op">=</span> prepare_dataset(val_dataset, is_training<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implementing-the-deit-variants-of-vit">Implementing the DeiT variants of ViT<a class="anchor" aria-label="anchor" href="#implementing-the-deit-variants-of-vit"></a>
</h2>
<p>Since DeiT is an extension of ViT it’d make sense to first implement
ViT and then extend it to support DeiT’s components.</p>
<p>First, we’ll implement a layer for Stochastic Depth (<a href="https://arxiv.org/abs/1603.09382" class="external-link">Huang et al.</a>) which is used
in DeiT for regularization.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Referred from: github.com:rwightman/pytorch-image-models.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="kw">class</span> StochasticDepth(layers.Layer):</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, drop_prop, <span class="op">**</span>kwargs):</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        <span class="va">self</span>.drop_prob <span class="op">=</span> drop_prop</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x, training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        <span class="cf">if</span> training:</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>            keep_prob <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.drop_prob</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>            shape <span class="op">=</span> (tf.shape(x)[<span class="dv">0</span>],) <span class="op">+</span> (<span class="dv">1</span>,) <span class="op">*</span> (<span class="bu">len</span>(x.shape) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>            random_tensor <span class="op">=</span> keep_prob <span class="op">+</span> tf.random.uniform(shape, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>            random_tensor <span class="op">=</span> tf.floor(random_tensor)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>            <span class="cf">return</span> (x <span class="op">/</span> keep_prob) <span class="op">*</span> random_tensor</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
<p>Now, we’ll implement the MLP and Transformer blocks.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> mlp(x, dropout_rate: <span class="bu">float</span>, hidden_units):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="co">"""FFN for a Transformer block."""</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    <span class="co"># Iterate over the hidden units and</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    <span class="co"># add Dense =&gt; Dropout.</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="cf">for</span> idx, units <span class="kw">in</span> <span class="bu">enumerate</span>(hidden_units):</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        x <span class="op">=</span> layers.Dense(</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>            units,</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>            activation<span class="op">=</span>tf.nn.gelu <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        )(x)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        x <span class="op">=</span> layers.Dropout(dropout_rate)(x)</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="kw">def</span> transformer(drop_prob: <span class="bu">float</span>, name: <span class="bu">str</span>) <span class="op">-&gt;</span> keras.Model:</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    <span class="co">"""Transformer block with pre-norm."""</span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    num_patches <span class="op">=</span> (</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        NUM_PATCHES <span class="op">+</span> <span class="dv">2</span> <span class="cf">if</span> <span class="st">"distilled"</span> <span class="kw">in</span> MODEL_TYPE <span class="cf">else</span> NUM_PATCHES <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>    )</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    encoded_patches <span class="op">=</span> layers.Input((num_patches, PROJECTION_DIM))</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    <span class="co"># Layer normalization 1.</span></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>    x1 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(encoded_patches)</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>    <span class="co"># Multi Head Self Attention layer 1.</span></span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>    attention_output <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>        num_heads<span class="op">=</span>NUM_HEADS,</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>        key_dim<span class="op">=</span>PROJECTION_DIM,</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>        dropout<span class="op">=</span>DROPOUT_RATE,</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>    )(x1, x1)</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>    attention_output <span class="op">=</span> (</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>        StochasticDepth(drop_prob)(attention_output)</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>        <span class="cf">if</span> drop_prob</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>        <span class="cf">else</span> attention_output</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>    )</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>    <span class="co"># Skip connection 1.</span></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>    x2 <span class="op">=</span> layers.Add()([attention_output, encoded_patches])</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>    <span class="co"># Layer normalization 2.</span></span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>    x3 <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)(x2)</span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a>    <span class="co"># MLP layer 1.</span></span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a>    x4 <span class="op">=</span> mlp(x3, hidden_units<span class="op">=</span>MLP_UNITS, dropout_rate<span class="op">=</span>DROPOUT_RATE)</span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>    x4 <span class="op">=</span> StochasticDepth(drop_prob)(x4) <span class="cf">if</span> drop_prob <span class="cf">else</span> x4</span>
<span id="cb5-45"><a href="#cb5-45" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" tabindex="-1"></a>    <span class="co"># Skip connection 2.</span></span>
<span id="cb5-47"><a href="#cb5-47" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Add()([x2, x4])</span>
<span id="cb5-48"><a href="#cb5-48" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(encoded_patches, outputs, name<span class="op">=</span>name)</span></code></pre></div>
<p>We’ll now implement a <code>ViTClassifier</code> class building on
top of the components we just developed. Here we’ll be following the
original pooling strategy used in the ViT paper – use a class token and
use the feature representations corresponding to it for
classification.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">class</span> ViTClassifier(keras.Model):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="co">"""Vision Transformer base class."""</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">**</span>kwargs):</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        <span class="co"># Patchify + linear projection + reshaping.</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> keras.Sequential(</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>            [</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>                layers.Conv2D(</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>                    filters<span class="op">=</span>PROJECTION_DIM,</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>                    kernel_size<span class="op">=</span>(PATCH_SIZE, PATCH_SIZE),</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>                    strides<span class="op">=</span>(PATCH_SIZE, PATCH_SIZE),</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>                    padding<span class="op">=</span><span class="st">"VALID"</span>,</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>                    name<span class="op">=</span><span class="st">"conv_projection"</span>,</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>                ),</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>                layers.Reshape(</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>                    target_shape<span class="op">=</span>(NUM_PATCHES, PROJECTION_DIM),</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>                    name<span class="op">=</span><span class="st">"flatten_projection"</span>,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>                ),</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>            ],</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"projection"</span>,</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>        )</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        <span class="co"># Positional embedding.</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        init_shape <span class="op">=</span> (</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>            <span class="dv">1</span>,</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>            NUM_PATCHES <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>            PROJECTION_DIM,</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>        )</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> tf.Variable(</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>            tf.zeros(init_shape), name<span class="op">=</span><span class="st">"pos_embedding"</span></span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>        )</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>        <span class="co"># Transformer blocks.</span></span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>        dpr <span class="op">=</span> [x <span class="cf">for</span> x <span class="kw">in</span> tf.linspace(<span class="fl">0.0</span>, DROP_PATH_RATE, NUM_LAYERS)]</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>        <span class="va">self</span>.transformer_blocks <span class="op">=</span> [</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>            transformer(drop_prob<span class="op">=</span>dpr[i], name<span class="op">=</span><span class="ss">f"transformer_block_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-39"><a href="#cb6-39" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(NUM_LAYERS)</span>
<span id="cb6-40"><a href="#cb6-40" tabindex="-1"></a>        ]</span>
<span id="cb6-41"><a href="#cb6-41" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" tabindex="-1"></a>        <span class="co"># CLS token.</span></span>
<span id="cb6-43"><a href="#cb6-43" tabindex="-1"></a>        initial_value <span class="op">=</span> tf.zeros((<span class="dv">1</span>, <span class="dv">1</span>, PROJECTION_DIM))</span>
<span id="cb6-44"><a href="#cb6-44" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> tf.Variable(</span>
<span id="cb6-45"><a href="#cb6-45" tabindex="-1"></a>            initial_value<span class="op">=</span>initial_value, trainable<span class="op">=</span><span class="va">True</span>, name<span class="op">=</span><span class="st">"cls"</span></span>
<span id="cb6-46"><a href="#cb6-46" tabindex="-1"></a>        )</span>
<span id="cb6-47"><a href="#cb6-47" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" tabindex="-1"></a>        <span class="co"># Other layers.</span></span>
<span id="cb6-49"><a href="#cb6-49" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> layers.Dropout(DROPOUT_RATE)</span>
<span id="cb6-50"><a href="#cb6-50" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span>LAYER_NORM_EPS)</span>
<span id="cb6-51"><a href="#cb6-51" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> layers.Dense(</span>
<span id="cb6-52"><a href="#cb6-52" tabindex="-1"></a>            NUM_CLASSES,</span>
<span id="cb6-53"><a href="#cb6-53" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"classification_head"</span>,</span>
<span id="cb6-54"><a href="#cb6-54" tabindex="-1"></a>        )</span>
<span id="cb6-55"><a href="#cb6-55" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb6-57"><a href="#cb6-57" tabindex="-1"></a>        n <span class="op">=</span> tf.shape(inputs)[<span class="dv">0</span>]</span>
<span id="cb6-58"><a href="#cb6-58" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" tabindex="-1"></a>        <span class="co"># Create patches and project the patches.</span></span>
<span id="cb6-60"><a href="#cb6-60" tabindex="-1"></a>        projected_patches <span class="op">=</span> <span class="va">self</span>.projection(inputs)</span>
<span id="cb6-61"><a href="#cb6-61" tabindex="-1"></a></span>
<span id="cb6-62"><a href="#cb6-62" tabindex="-1"></a>        <span class="co"># Append class token if needed.</span></span>
<span id="cb6-63"><a href="#cb6-63" tabindex="-1"></a>        cls_token <span class="op">=</span> tf.tile(<span class="va">self</span>.cls_token, (n, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb6-64"><a href="#cb6-64" tabindex="-1"></a>        cls_token <span class="op">=</span> tf.cast(cls_token, projected_patches.dtype)</span>
<span id="cb6-65"><a href="#cb6-65" tabindex="-1"></a>        projected_patches <span class="op">=</span> tf.concat([cls_token, projected_patches], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-66"><a href="#cb6-66" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" tabindex="-1"></a>        <span class="co"># Add positional embeddings to the projected patches.</span></span>
<span id="cb6-68"><a href="#cb6-68" tabindex="-1"></a>        encoded_patches <span class="op">=</span> (</span>
<span id="cb6-69"><a href="#cb6-69" tabindex="-1"></a>            <span class="va">self</span>.positional_embedding <span class="op">+</span> projected_patches</span>
<span id="cb6-70"><a href="#cb6-70" tabindex="-1"></a>        )  <span class="co"># (B, number_patches, projection_dim)</span></span>
<span id="cb6-71"><a href="#cb6-71" tabindex="-1"></a>        encoded_patches <span class="op">=</span> <span class="va">self</span>.dropout(encoded_patches)</span>
<span id="cb6-72"><a href="#cb6-72" tabindex="-1"></a></span>
<span id="cb6-73"><a href="#cb6-73" tabindex="-1"></a>        <span class="co"># Iterate over the number of layers and stack up blocks of</span></span>
<span id="cb6-74"><a href="#cb6-74" tabindex="-1"></a>        <span class="co"># Transformer.</span></span>
<span id="cb6-75"><a href="#cb6-75" tabindex="-1"></a>        <span class="cf">for</span> transformer_module <span class="kw">in</span> <span class="va">self</span>.transformer_blocks:</span>
<span id="cb6-76"><a href="#cb6-76" tabindex="-1"></a>            <span class="co"># Add a Transformer block.</span></span>
<span id="cb6-77"><a href="#cb6-77" tabindex="-1"></a>            encoded_patches <span class="op">=</span> transformer_module(encoded_patches)</span>
<span id="cb6-78"><a href="#cb6-78" tabindex="-1"></a></span>
<span id="cb6-79"><a href="#cb6-79" tabindex="-1"></a>        <span class="co"># Final layer normalization.</span></span>
<span id="cb6-80"><a href="#cb6-80" tabindex="-1"></a>        representation <span class="op">=</span> <span class="va">self</span>.layer_norm(encoded_patches)</span>
<span id="cb6-81"><a href="#cb6-81" tabindex="-1"></a></span>
<span id="cb6-82"><a href="#cb6-82" tabindex="-1"></a>        <span class="co"># Pool representation.</span></span>
<span id="cb6-83"><a href="#cb6-83" tabindex="-1"></a>        encoded_patches <span class="op">=</span> representation[:, <span class="dv">0</span>]</span>
<span id="cb6-84"><a href="#cb6-84" tabindex="-1"></a></span>
<span id="cb6-85"><a href="#cb6-85" tabindex="-1"></a>        <span class="co"># Classification head.</span></span>
<span id="cb6-86"><a href="#cb6-86" tabindex="-1"></a>        output <span class="op">=</span> <span class="va">self</span>.head(encoded_patches)</span>
<span id="cb6-87"><a href="#cb6-87" tabindex="-1"></a>        <span class="cf">return</span> output</span></code></pre></div>
<p>This class can be used standalone as ViT and is end-to-end trainable.
Just remove the <code>distilled</code> phrase in <code>MODEL_TYPE</code>
and it should work with <code>vit_tiny = ViTClassifier()</code>. Let’s
now extend it to DeiT. The following figure presents the schematic of
DeiT (taken from the DeiT paper):</p>
<p><img src="https://i.imgur.com/5lmg2Xs.png"></p>
<p>Apart from the class token, DeiT has another token for distillation.
During distillation, the logits corresponding to the class token are
compared to the true labels, and the logits corresponding to the
distillation token are compared to the teacher’s predictions.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">class</span> ViTDistilled(ViTClassifier):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, regular_training<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>kwargs):</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>        <span class="va">self</span>.num_tokens <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>        <span class="va">self</span>.regular_training <span class="op">=</span> regular_training</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        <span class="co"># CLS and distillation tokens, positional embedding.</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        init_value <span class="op">=</span> tf.zeros((<span class="dv">1</span>, <span class="dv">1</span>, PROJECTION_DIM))</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="va">self</span>.dist_token <span class="op">=</span> tf.Variable(init_value, name<span class="op">=</span><span class="st">"dist_token"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> tf.Variable(</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>            tf.zeros(</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>                (</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>                    <span class="dv">1</span>,</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>                    NUM_PATCHES <span class="op">+</span> <span class="va">self</span>.num_tokens,</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>                    PROJECTION_DIM,</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>                )</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>            ),</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"pos_embedding"</span>,</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>        )</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>        <span class="co"># Head layers.</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> layers.Dense(</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>            NUM_CLASSES,</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"classification_head"</span>,</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>        )</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>        <span class="va">self</span>.head_dist <span class="op">=</span> layers.Dense(</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>            NUM_CLASSES,</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"distillation_head"</span>,</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>        )</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs, training<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a>        n <span class="op">=</span> tf.shape(inputs)[<span class="dv">0</span>]</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>        <span class="co"># Create patches and project the patches.</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>        projected_patches <span class="op">=</span> <span class="va">self</span>.projection(inputs)</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>        <span class="co"># Append the tokens.</span></span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>        cls_token <span class="op">=</span> tf.tile(<span class="va">self</span>.cls_token, (n, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>        dist_token <span class="op">=</span> tf.tile(<span class="va">self</span>.dist_token, (n, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>        cls_token <span class="op">=</span> tf.cast(cls_token, projected_patches.dtype)</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>        dist_token <span class="op">=</span> tf.cast(dist_token, projected_patches.dtype)</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>        projected_patches <span class="op">=</span> tf.concat(</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>            [cls_token, dist_token, projected_patches], axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>        )</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>        <span class="co"># Add positional embeddings to the projected patches.</span></span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>        encoded_patches <span class="op">=</span> (</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>            <span class="va">self</span>.positional_embedding <span class="op">+</span> projected_patches</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>        )  <span class="co"># (B, number_patches, projection_dim)</span></span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a>        encoded_patches <span class="op">=</span> <span class="va">self</span>.dropout(encoded_patches)</span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a>        <span class="co"># Iterate over the number of layers and stack up blocks of</span></span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>        <span class="co"># Transformer.</span></span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a>        <span class="cf">for</span> transformer_module <span class="kw">in</span> <span class="va">self</span>.transformer_blocks:</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>            <span class="co"># Add a Transformer block.</span></span>
<span id="cb7-56"><a href="#cb7-56" tabindex="-1"></a>            encoded_patches <span class="op">=</span> transformer_module(encoded_patches)</span>
<span id="cb7-57"><a href="#cb7-57" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" tabindex="-1"></a>        <span class="co"># Final layer normalization.</span></span>
<span id="cb7-59"><a href="#cb7-59" tabindex="-1"></a>        representation <span class="op">=</span> <span class="va">self</span>.layer_norm(encoded_patches)</span>
<span id="cb7-60"><a href="#cb7-60" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" tabindex="-1"></a>        <span class="co"># Classification heads.</span></span>
<span id="cb7-62"><a href="#cb7-62" tabindex="-1"></a>        x, x_dist <span class="op">=</span> (</span>
<span id="cb7-63"><a href="#cb7-63" tabindex="-1"></a>            <span class="va">self</span>.head(representation[:, <span class="dv">0</span>]),</span>
<span id="cb7-64"><a href="#cb7-64" tabindex="-1"></a>            <span class="va">self</span>.head_dist(representation[:, <span class="dv">1</span>]),</span>
<span id="cb7-65"><a href="#cb7-65" tabindex="-1"></a>        )</span>
<span id="cb7-66"><a href="#cb7-66" tabindex="-1"></a></span>
<span id="cb7-67"><a href="#cb7-67" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> training <span class="kw">or</span> <span class="va">self</span>.regular_training:</span>
<span id="cb7-68"><a href="#cb7-68" tabindex="-1"></a>            <span class="co"># During standard train / finetune, inference average the classifier</span></span>
<span id="cb7-69"><a href="#cb7-69" tabindex="-1"></a>            <span class="co"># predictions.</span></span>
<span id="cb7-70"><a href="#cb7-70" tabindex="-1"></a>            <span class="cf">return</span> (x <span class="op">+</span> x_dist) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb7-71"><a href="#cb7-71" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" tabindex="-1"></a>        <span class="cf">elif</span> training:</span>
<span id="cb7-73"><a href="#cb7-73" tabindex="-1"></a>            <span class="co"># Only return separate classification predictions when training in distilled</span></span>
<span id="cb7-74"><a href="#cb7-74" tabindex="-1"></a>            <span class="co"># mode.</span></span>
<span id="cb7-75"><a href="#cb7-75" tabindex="-1"></a>            <span class="cf">return</span> x, x_dist</span></code></pre></div>
<p>Let’s verify if the <code>ViTDistilled</code> class can be
initialized and called as expected.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>deit_tiny_distilled <span class="op">=</span> ViTDistilled()</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>dummy_inputs <span class="op">=</span> tf.ones((<span class="dv">2</span>, <span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>))</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>outputs <span class="op">=</span> deit_tiny_distilled(dummy_inputs, training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"output_shape: </span><span class="sc">{</span>outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implementing-the-trainer">Implementing the trainer<a class="anchor" aria-label="anchor" href="#implementing-the-trainer"></a>
</h2>
<p>Unlike what happens in standard knowledge distillation (<a href="https://arxiv.org/abs/1503.02531" class="external-link">Hinton et al.</a>), where a
temperature-scaled softmax is used as well as KL divergence, DeiT
authors use the following loss function:</p>
<p><img src="https://i.imgur.com/bXdxsBq.png"></p>
<p>Here,</p>
<ul>
<li>CE is cross-entropy</li>
<li>
<code>psi</code> is the softmax function</li>
<li>Z_s denotes student predictions</li>
<li>y denotes true labels</li>
<li>y_t denotes teacher predictions</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">class</span> DeiT(keras.Model):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    <span class="co"># Reference:</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    <span class="co"># https://keras.io/examples/vision/knowledge_distillation/</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, student, teacher, <span class="op">**</span>kwargs):</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        <span class="va">self</span>.student <span class="op">=</span> student</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        <span class="va">self</span>.teacher <span class="op">=</span> teacher</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>        <span class="va">self</span>.student_loss_tracker <span class="op">=</span> keras.metrics.Mean(name<span class="op">=</span><span class="st">"student_loss"</span>)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        <span class="va">self</span>.distillation_loss_tracker <span class="op">=</span> keras.metrics.Mean(</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"distillation_loss"</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        )</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        <span class="va">self</span>.accuracy <span class="op">=</span> keras.metrics.CategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    <span class="kw">def</span> metrics(<span class="va">self</span>):</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>        <span class="cf">return</span> [</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>            <span class="va">self</span>.accuracy,</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>            <span class="va">self</span>.student_loss_tracker,</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>            <span class="va">self</span>.distillation_loss_tracker,</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        ]</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>    <span class="kw">def</span> <span class="bu">compile</span>(</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>        optimizer,</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>        student_loss_fn,</span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>        distillation_loss_fn,</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>        run_eagerly<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>        jit_compile<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a>    ):</span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>        <span class="bu">super</span>().<span class="bu">compile</span>(</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>            optimizer<span class="op">=</span>optimizer,</span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>            run_eagerly<span class="op">=</span>run_eagerly,</span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>            jit_compile<span class="op">=</span>jit_compile,</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a>        )</span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>        <span class="va">self</span>.student_loss_fn <span class="op">=</span> student_loss_fn</span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>        <span class="va">self</span>.distillation_loss_fn <span class="op">=</span> distillation_loss_fn</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>    <span class="kw">def</span> train_step(<span class="va">self</span>, data):</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a>        <span class="co"># Unpack data.</span></span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a>        x, y <span class="op">=</span> data</span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a>        <span class="co"># Forward pass of teacher</span></span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>        teacher_predictions <span class="op">=</span> <span class="va">self</span>.teacher(x)[<span class="st">"dense"</span>]</span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>        teacher_predictions <span class="op">=</span> tf.nn.softmax(teacher_predictions, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" tabindex="-1"></a>        <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb9-47"><a href="#cb9-47" tabindex="-1"></a>            <span class="co"># Forward pass of student.</span></span>
<span id="cb9-48"><a href="#cb9-48" tabindex="-1"></a>            cls_predictions, dist_predictions <span class="op">=</span> <span class="va">self</span>.student(</span>
<span id="cb9-49"><a href="#cb9-49" tabindex="-1"></a>                x <span class="op">/</span> <span class="fl">255.0</span>, training<span class="op">=</span><span class="va">True</span></span>
<span id="cb9-50"><a href="#cb9-50" tabindex="-1"></a>            )</span>
<span id="cb9-51"><a href="#cb9-51" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" tabindex="-1"></a>            <span class="co"># Compute losses.</span></span>
<span id="cb9-53"><a href="#cb9-53" tabindex="-1"></a>            student_loss <span class="op">=</span> <span class="va">self</span>.student_loss_fn(y, cls_predictions)</span>
<span id="cb9-54"><a href="#cb9-54" tabindex="-1"></a>            distillation_loss <span class="op">=</span> <span class="va">self</span>.distillation_loss_fn(</span>
<span id="cb9-55"><a href="#cb9-55" tabindex="-1"></a>                teacher_predictions, dist_predictions</span>
<span id="cb9-56"><a href="#cb9-56" tabindex="-1"></a>            )</span>
<span id="cb9-57"><a href="#cb9-57" tabindex="-1"></a>            loss <span class="op">=</span> (student_loss <span class="op">+</span> distillation_loss) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb9-58"><a href="#cb9-58" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" tabindex="-1"></a>        <span class="co"># Compute gradients.</span></span>
<span id="cb9-60"><a href="#cb9-60" tabindex="-1"></a>        trainable_vars <span class="op">=</span> <span class="va">self</span>.student.trainable_variables</span>
<span id="cb9-61"><a href="#cb9-61" tabindex="-1"></a>        gradients <span class="op">=</span> tape.gradient(loss, trainable_vars)</span>
<span id="cb9-62"><a href="#cb9-62" tabindex="-1"></a></span>
<span id="cb9-63"><a href="#cb9-63" tabindex="-1"></a>        <span class="co"># Update weights.</span></span>
<span id="cb9-64"><a href="#cb9-64" tabindex="-1"></a>        <span class="va">self</span>.optimizer.apply_gradients(<span class="bu">zip</span>(gradients, trainable_vars))</span>
<span id="cb9-65"><a href="#cb9-65" tabindex="-1"></a></span>
<span id="cb9-66"><a href="#cb9-66" tabindex="-1"></a>        <span class="co"># Update the metrics configured in `compile()`.</span></span>
<span id="cb9-67"><a href="#cb9-67" tabindex="-1"></a>        student_predictions <span class="op">=</span> (cls_predictions <span class="op">+</span> dist_predictions) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb9-68"><a href="#cb9-68" tabindex="-1"></a>        <span class="va">self</span>.student_loss_tracker.update_state(student_loss)</span>
<span id="cb9-69"><a href="#cb9-69" tabindex="-1"></a>        <span class="va">self</span>.distillation_loss_tracker.update_state(distillation_loss)</span>
<span id="cb9-70"><a href="#cb9-70" tabindex="-1"></a>        <span class="va">self</span>.accuracy.update_state(y, student_predictions)</span>
<span id="cb9-71"><a href="#cb9-71" tabindex="-1"></a></span>
<span id="cb9-72"><a href="#cb9-72" tabindex="-1"></a>        <span class="co"># Return a dict of performance.</span></span>
<span id="cb9-73"><a href="#cb9-73" tabindex="-1"></a>        <span class="cf">return</span> {m.name: m.result() <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics}</span>
<span id="cb9-74"><a href="#cb9-74" tabindex="-1"></a></span>
<span id="cb9-75"><a href="#cb9-75" tabindex="-1"></a>    <span class="kw">def</span> test_step(<span class="va">self</span>, data):</span>
<span id="cb9-76"><a href="#cb9-76" tabindex="-1"></a>        <span class="co"># Unpack the data.</span></span>
<span id="cb9-77"><a href="#cb9-77" tabindex="-1"></a>        x, y <span class="op">=</span> data</span>
<span id="cb9-78"><a href="#cb9-78" tabindex="-1"></a></span>
<span id="cb9-79"><a href="#cb9-79" tabindex="-1"></a>        <span class="co"># Compute predictions.</span></span>
<span id="cb9-80"><a href="#cb9-80" tabindex="-1"></a>        y_prediction <span class="op">=</span> <span class="va">self</span>.student(x <span class="op">/</span> <span class="fl">255.0</span>)</span>
<span id="cb9-81"><a href="#cb9-81" tabindex="-1"></a></span>
<span id="cb9-82"><a href="#cb9-82" tabindex="-1"></a>        <span class="co"># Calculate the loss.</span></span>
<span id="cb9-83"><a href="#cb9-83" tabindex="-1"></a>        student_loss <span class="op">=</span> <span class="va">self</span>.student_loss_fn(y, y_prediction)</span>
<span id="cb9-84"><a href="#cb9-84" tabindex="-1"></a></span>
<span id="cb9-85"><a href="#cb9-85" tabindex="-1"></a>        <span class="co"># Update the metrics.</span></span>
<span id="cb9-86"><a href="#cb9-86" tabindex="-1"></a>        <span class="va">self</span>.student_loss_tracker.update_state(student_loss)</span>
<span id="cb9-87"><a href="#cb9-87" tabindex="-1"></a>        <span class="va">self</span>.accuracy.update_state(y, y_prediction)</span>
<span id="cb9-88"><a href="#cb9-88" tabindex="-1"></a></span>
<span id="cb9-89"><a href="#cb9-89" tabindex="-1"></a>        <span class="co"># Return a dict of performance.</span></span>
<span id="cb9-90"><a href="#cb9-90" tabindex="-1"></a>        results <span class="op">=</span> {m.name: m.result() <span class="cf">for</span> m <span class="kw">in</span> <span class="va">self</span>.metrics}</span>
<span id="cb9-91"><a href="#cb9-91" tabindex="-1"></a>        <span class="cf">return</span> results</span>
<span id="cb9-92"><a href="#cb9-92" tabindex="-1"></a></span>
<span id="cb9-93"><a href="#cb9-93" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb9-94"><a href="#cb9-94" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.student(inputs <span class="op">/</span> <span class="fl">255.0</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-teacher-model">Load the teacher model<a class="anchor" aria-label="anchor" href="#load-the-teacher-model"></a>
</h2>
<p>This model is based on the BiT family of ResNets (<a href="https://arxiv.org/abs/1912.11370" class="external-link">Kolesnikov et al.</a>)
fine-tuned on the <code>tf_flowers</code> dataset. You can refer to <a href="https://github.com/sayakpaul/deit-tf/blob/main/notebooks/bit-teacher.ipynb" class="external-link">this
notebook</a> to know how the training was performed. The teacher model
has about 212 Million parameters which is about <strong>40x
more</strong> than the student.</p>
<p>wget -q <a href="https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip" class="external-link uri">https://github.com/sayakpaul/deit-tf/releases/download/v0.1.0/bit_teacher_flowers.zip</a>
unzip -q bit_teacher_flowers.zip</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>bit_teacher_flowers <span class="op">=</span> keras.layers.TFSMLayer(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    filepath<span class="op">=</span><span class="st">"bit_teacher_flowers"</span>,</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    call_endpoint<span class="op">=</span><span class="st">"serving_default"</span>,</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="training-through-distillation">Training through distillation<a class="anchor" aria-label="anchor" href="#training-through-distillation"></a>
</h2>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>deit_tiny <span class="op">=</span> ViTDistilled()</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>deit_distiller <span class="op">=</span> DeiT(student<span class="op">=</span>deit_tiny, teacher<span class="op">=</span>bit_teacher_flowers)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>lr_scaled <span class="op">=</span> (BASE_LR <span class="op">/</span> <span class="dv">512</span>) <span class="op">*</span> BATCH_SIZE</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>deit_distiller.<span class="bu">compile</span>(</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.AdamW(</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        weight_decay<span class="op">=</span>WEIGHT_DECAY, learning_rate<span class="op">=</span>lr_scaled</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    ),</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>    student_loss_fn<span class="op">=</span>keras.losses.CategoricalCrossentropy(</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>        from_logits<span class="op">=</span><span class="va">True</span>, label_smoothing<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    ),</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>    distillation_loss_fn<span class="op">=</span>keras.losses.CategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>)</span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>_ <span class="op">=</span> deit_distiller.fit(</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>    train_dataset, validation_data<span class="op">=</span>val_dataset, epochs<span class="op">=</span>NUM_EPOCHS</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>)</span></code></pre></div>
<p>If we had trained the same model (the <code>ViTClassifier</code>)
from scratch with the exact same hyperparameters, the model would have
scored about 59% accuracy. You can adapt the following code to reproduce
this result:</p>
<pre><code>vit_tiny = ViTClassifier()

inputs = keras.Input((RESOLUTION, RESOLUTION, 3))
x = keras.layers.Rescaling(scale=1./255)(inputs)
outputs = deit_tiny(x)
model = keras.Model(inputs, outputs)

model.compile(...)
model.fit(...)</code></pre>
</div>
<div class="section level2">
<h2 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h2>
<ul>
<li>Through the use of distillation, we’re effectively transferring the
inductive biases of a CNN-based teacher model.</li>
<li>Interestingly enough, this distillation strategy works better with a
CNN as the teacher model rather than a Transformer as shown in the
paper.</li>
<li>The use of regularization to train DeiT models is very
important.</li>
<li>ViT models are initialized with a combination of different
initializers including truncated normal, random normal, Glorot uniform,
etc. If you’re looking for end-to-end reproduction of the original
results, don’t forget to initialize the ViTs well.</li>
<li>If you want to explore the pre-trained DeiT models in TensorFlow and
Keras with code for fine-tuning, <a href="https://tfhub.dev/sayakpaul/collections/deit/1" class="external-link">check out these
models on TF-Hub</a>.</li>
</ul>
</div>
<div class="section level2">
<h2 id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<ul>
<li>Ross Wightman for keeping <a href="https://github.com/rwightman/pytorch-image-models" class="external-link"><code>timm</code></a>
updated with readable implementations. I referred to the implementations
of ViT and DeiT a lot during implementing them in TensorFlow.</li>
<li>
<a href="https://github.com/ariG23498" class="external-link">Aritra Roy Gosthipaty</a> who
implemented some portions of the <code>ViTClassifier</code> in another
project.</li>
<li>
<a href="https://developers.google.com/programs/experts/" class="external-link">Google
Developers Experts</a> program for supporting me with GCP credits which
were used to run experiments for this example.</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
