<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>lstm_seq2seq • keras</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="lstm_seq2seq">
<meta property="og:description" content="">
<meta property="og:image" content="/logo.png">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">2.1.6.9003</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Getting Started</li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Guide to the Sequential Model</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Guide to the Functional API</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li class="divider">
    <li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
<li>
  <a href="https://github.com/rstudio/keras">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>lstm_seq2seq</h1>
            
      
      

    </div>

    
    
<div class="contents">
<div class="source-ref">
<p><span class="caption">Source: </span><a href="https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.R" class="uri">https://github.com/rstudio/keras/blob/master/vignettes/examples/lstm_seq2seq.R</a></p>
</div>
<p>Sequence to sequence example in Keras (character-level).</p>
<p>This script demonstrates how to implement a basic character-level sequence-to-sequence model. We apply it to translating short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level machine translation, as word-level models are more common in this domain.</p>
<p><strong>Algorithm</strong></p>
<ul>
<li>We start with input sequences from a domain (e.g. English sentences) and correspding target sequences from another domain (e.g. French sentences).</li>
<li>An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).</li>
<li>A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called “teacher forcing” in this context. Is uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate <code>targets[t+1...]</code> given <code>targets[...t]</code>, conditioned on the input sequence.</li>
<li>In inference mode, when we want to decode unknown input sequences, we:
<ul>
<li>Encode the input sequence into state vectors</li>
<li>Start with a target sequence of size 1 (just the start-of-sequence character)</li>
<li>Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character</li>
<li>Sample the next character using these predictions (we simply use argmax).</li>
<li>Append the sampled character to the target sequence</li>
<li>Repeat until we generate the end-of-sequence character or we hit the character limit.</li>
</ul>
</li>
</ul>
<p><strong>Data download</strong></p>
<p>English to French sentence pairs. <a href="http://www.manythings.org/anki/fra-eng.zip" class="uri">http://www.manythings.org/anki/fra-eng.zip</a></p>
<p>Lots of neat sentence pairs datasets can be found at: <a href="http://www.manythings.org/anki/" class="uri">http://www.manythings.org/anki/</a></p>
<p><strong>References</strong></p>
<ul>
<li>Sequence to Sequence Learning with Neural Networks <a href="https://arxiv.org/abs/1409.3215" class="uri">https://arxiv.org/abs/1409.3215</a>
</li>
<li>Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation <a href="https://arxiv.org/abs/1406.1078" class="uri">https://arxiv.org/abs/1406.1078</a>
</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">batch_size =<span class="st"> </span><span class="dv">64</span>  <span class="co"># Batch size for training.</span></a>
<a class="sourceLine" id="cb1-5" data-line-number="5">epochs =<span class="st"> </span><span class="dv">100</span>  <span class="co"># Number of epochs to train for.</span></a>
<a class="sourceLine" id="cb1-6" data-line-number="6">latent_dim =<span class="st"> </span><span class="dv">256</span>  <span class="co"># Latent dimensionality of the encoding space.</span></a>
<a class="sourceLine" id="cb1-7" data-line-number="7">num_samples =<span class="st"> </span><span class="dv">10000</span>  <span class="co"># Number of samples to train on.</span></a>
<a class="sourceLine" id="cb1-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1-9" data-line-number="9">## Path to the data txt file on disk.</a>
<a class="sourceLine" id="cb1-10" data-line-number="10">data_path =<span class="st"> 'fra.txt'</span></a>
<a class="sourceLine" id="cb1-11" data-line-number="11">text &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/data.table/topics/fread">fread</a></span>(data_path, <span class="dt">sep=</span><span class="st">"</span><span class="ch">\t</span><span class="st">"</span>, <span class="dt">header=</span><span class="ot">FALSE</span>, <span class="dt">nrows=</span>num_samples)</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1-13" data-line-number="13">## Vectorize the data.</a>
<a class="sourceLine" id="cb1-14" data-line-number="14">input_texts  &lt;-<span class="st"> </span>text[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb1-15" data-line-number="15">target_texts &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>,text[[<span class="dv">2</span>]],<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-16" data-line-number="16">input_texts  &lt;-<span class="st"> </span><span class="kw">lapply</span>( input_texts, <span class="cf">function</span>(s) <span class="kw"><a href="http://www.rdocumentation.org/packages/data.table/topics/tstrsplit">strsplit</a></span>(s, <span class="dt">split=</span><span class="st">""</span>)[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1-17" data-line-number="17">target_texts &lt;-<span class="st"> </span><span class="kw">lapply</span>( target_texts, <span class="cf">function</span>(s) <span class="kw"><a href="http://www.rdocumentation.org/packages/data.table/topics/tstrsplit">strsplit</a></span>(s, <span class="dt">split=</span><span class="st">""</span>)[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1-18" data-line-number="18"></a>
<a class="sourceLine" id="cb1-19" data-line-number="19">input_characters  &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/data.table/topics/duplicated">unique</a></span>(<span class="kw">unlist</span>(input_texts)))</a>
<a class="sourceLine" id="cb1-20" data-line-number="20">target_characters &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/data.table/topics/duplicated">unique</a></span>(<span class="kw">unlist</span>(target_texts)))</a>
<a class="sourceLine" id="cb1-21" data-line-number="21">num_encoder_tokens &lt;-<span class="st"> </span><span class="kw">length</span>(input_characters)</a>
<a class="sourceLine" id="cb1-22" data-line-number="22">num_decoder_tokens &lt;-<span class="st"> </span><span class="kw">length</span>(target_characters)</a>
<a class="sourceLine" id="cb1-23" data-line-number="23">max_encoder_seq_length &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">sapply</span>(input_texts,length))</a>
<a class="sourceLine" id="cb1-24" data-line-number="24">max_decoder_seq_length &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">sapply</span>(target_texts,length))</a>
<a class="sourceLine" id="cb1-25" data-line-number="25"></a>
<a class="sourceLine" id="cb1-26" data-line-number="26"><span class="kw">cat</span>(<span class="st">'Number of samples:'</span>, <span class="kw">length</span>(input_texts),<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-27" data-line-number="27"><span class="kw">cat</span>(<span class="st">'Number of unique input tokens:'</span>, num_encoder_tokens,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-28" data-line-number="28"><span class="kw">cat</span>(<span class="st">'Number of unique output tokens:'</span>, num_decoder_tokens,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-29" data-line-number="29"><span class="kw">cat</span>(<span class="st">'Max sequence length for inputs:'</span>, max_encoder_seq_length,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-30" data-line-number="30"><span class="kw">cat</span>(<span class="st">'Max sequence length for outputs:'</span>, max_decoder_seq_length,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb1-31" data-line-number="31"></a>
<a class="sourceLine" id="cb1-32" data-line-number="32">input_token_index  &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(input_characters)</a>
<a class="sourceLine" id="cb1-33" data-line-number="33"><span class="kw">names</span>(input_token_index) &lt;-<span class="st"> </span>input_characters</a>
<a class="sourceLine" id="cb1-34" data-line-number="34">target_token_index &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(target_characters)</a>
<a class="sourceLine" id="cb1-35" data-line-number="35"><span class="kw">names</span>(target_token_index) &lt;-<span class="st"> </span>target_characters</a>
<a class="sourceLine" id="cb1-36" data-line-number="36">encoder_input_data &lt;-<span class="st"> </span><span class="kw">array</span>(</a>
<a class="sourceLine" id="cb1-37" data-line-number="37">  <span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">length</span>(input_texts), max_encoder_seq_length, num_encoder_tokens))</a>
<a class="sourceLine" id="cb1-38" data-line-number="38">decoder_input_data &lt;-<span class="st"> </span><span class="kw">array</span>(</a>
<a class="sourceLine" id="cb1-39" data-line-number="39">  <span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">length</span>(input_texts), max_decoder_seq_length, num_decoder_tokens))</a>
<a class="sourceLine" id="cb1-40" data-line-number="40">decoder_target_data &lt;-<span class="st"> </span><span class="kw">array</span>(</a>
<a class="sourceLine" id="cb1-41" data-line-number="41">  <span class="dv">0</span>, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">length</span>(input_texts), max_decoder_seq_length, num_decoder_tokens))</a>
<a class="sourceLine" id="cb1-42" data-line-number="42"></a>
<a class="sourceLine" id="cb1-43" data-line-number="43"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(input_texts)) {</a>
<a class="sourceLine" id="cb1-44" data-line-number="44">  d1 &lt;-<span class="st"> </span><span class="kw">sapply</span>( input_characters, <span class="cf">function</span>(x) { <span class="kw">as.integer</span>(x <span class="op">==</span><span class="st"> </span>input_texts[[i]]) })</a>
<a class="sourceLine" id="cb1-45" data-line-number="45">  encoder_input_data[i,<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(d1),] &lt;-<span class="st"> </span>d1</a>
<a class="sourceLine" id="cb1-46" data-line-number="46">  d2 &lt;-<span class="st"> </span><span class="kw">sapply</span>( target_characters, <span class="cf">function</span>(x) { <span class="kw">as.integer</span>(x <span class="op">==</span><span class="st"> </span>target_texts[[i]]) })</a>
<a class="sourceLine" id="cb1-47" data-line-number="47">  decoder_input_data[i,<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(d2),] &lt;-<span class="st"> </span>d2</a>
<a class="sourceLine" id="cb1-48" data-line-number="48">  d3 &lt;-<span class="st"> </span><span class="kw">sapply</span>( target_characters, <span class="cf">function</span>(x) { <span class="kw">as.integer</span>(x <span class="op">==</span><span class="st"> </span>target_texts[[i]][<span class="op">-</span><span class="dv">1</span>]) })</a>
<a class="sourceLine" id="cb1-49" data-line-number="49">  decoder_target_data[i,<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(d3),] &lt;-<span class="st"> </span>d3</a>
<a class="sourceLine" id="cb1-50" data-line-number="50">}</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1">## Create the model</a></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1">## Define an input sequence and process it.</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">encoder_inputs  &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape=</span><span class="kw">list</span>(<span class="ot">NULL</span>,num_encoder_tokens))</a>
<a class="sourceLine" id="cb3-3" data-line-number="3">encoder         &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_lstm.html">layer_lstm</a></span>(<span class="dt">units=</span>latent_dim, <span class="dt">return_state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb3-4" data-line-number="4">encoder_results &lt;-<span class="st"> </span>encoder_inputs <span class="op">%&gt;%</span><span class="st"> </span>encoder</a>
<a class="sourceLine" id="cb3-5" data-line-number="5">## We discard `encoder_outputs` and only keep the states.</a>
<a class="sourceLine" id="cb3-6" data-line-number="6">encoder_states  &lt;-<span class="st"> </span>encoder_results[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]</a>
<a class="sourceLine" id="cb3-7" data-line-number="7"></a>
<a class="sourceLine" id="cb3-8" data-line-number="8">## Set up the decoder, using `encoder_states` as initial state.</a>
<a class="sourceLine" id="cb3-9" data-line-number="9">decoder_inputs  &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape=</span><span class="kw">list</span>(<span class="ot">NULL</span>, num_decoder_tokens))</a>
<a class="sourceLine" id="cb3-10" data-line-number="10">## We set up our decoder to return full output sequences,</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">## and to return internal states as well. We don't use the</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">## return states in the training model, but we will use them in inference.</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">decoder_lstm    &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_lstm.html">layer_lstm</a></span>(<span class="dt">units=</span>latent_dim, <span class="dt">return_sequences=</span><span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb3-14" data-line-number="14">                              <span class="dt">return_state=</span><span class="ot">TRUE</span>, <span class="dt">stateful=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">decoder_results &lt;-<span class="st"> </span><span class="kw">decoder_lstm</span>(decoder_inputs, <span class="dt">initial_state=</span>encoder_states)</a>
<a class="sourceLine" id="cb3-16" data-line-number="16">decoder_dense   &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_dense.html">layer_dense</a></span>(<span class="dt">units=</span>num_decoder_tokens, <span class="dt">activation=</span><span class="st">'softmax'</span>)</a>
<a class="sourceLine" id="cb3-17" data-line-number="17">decoder_outputs &lt;-<span class="st"> </span><span class="kw">decoder_dense</span>(decoder_results[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb3-18" data-line-number="18"></a>
<a class="sourceLine" id="cb3-19" data-line-number="19">## Define the model that will turn</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">## `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</a>
<a class="sourceLine" id="cb3-21" data-line-number="21">model &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/keras_model.html">keras_model</a></span>( <span class="dt">inputs =</span> <span class="kw">list</span>(encoder_inputs, decoder_inputs),</a>
<a class="sourceLine" id="cb3-22" data-line-number="22">                      <span class="dt">outputs =</span> decoder_outputs )</a>
<a class="sourceLine" id="cb3-23" data-line-number="23"></a>
<a class="sourceLine" id="cb3-24" data-line-number="24">## Compile model</a>
<a class="sourceLine" id="cb3-25" data-line-number="25">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../reference/compile.html">compile</a></span>(<span class="dt">optimizer=</span><span class="st">'rmsprop'</span>, <span class="dt">loss=</span><span class="st">'categorical_crossentropy'</span>)</a>
<a class="sourceLine" id="cb3-26" data-line-number="26"></a>
<a class="sourceLine" id="cb3-27" data-line-number="27">## Run model</a>
<a class="sourceLine" id="cb3-28" data-line-number="28">model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="../../reference/fit.html">fit</a></span>( <span class="kw">list</span>(encoder_input_data, decoder_input_data), decoder_target_data,</a>
<a class="sourceLine" id="cb3-29" data-line-number="29">               <span class="dt">batch_size=</span>batch_size,</a>
<a class="sourceLine" id="cb3-30" data-line-number="30">               <span class="dt">epochs=</span>epochs,</a>
<a class="sourceLine" id="cb3-31" data-line-number="31">               <span class="dt">validation_split=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb3-32" data-line-number="32"></a>
<a class="sourceLine" id="cb3-33" data-line-number="33">## Save model</a>
<a class="sourceLine" id="cb3-34" data-line-number="34"><span class="kw"><a href="../../reference/save_model_hdf5.html">save_model_hdf5</a></span>(model,<span class="st">'s2s.h5'</span>)</a>
<a class="sourceLine" id="cb3-35" data-line-number="35"><span class="kw"><a href="../../reference/save_model_weights_hdf5.html">save_model_weights_hdf5</a></span>(model,<span class="st">'s2s-wt.h5'</span>)</a>
<a class="sourceLine" id="cb3-36" data-line-number="36"></a>
<a class="sourceLine" id="cb3-37" data-line-number="37">##model &lt;- load_model_hdf5('s2s.h5')</a>
<a class="sourceLine" id="cb3-38" data-line-number="38">##load_model_weights_hdf5(model,'s2s-wt.h5')</a></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1">## Next: inference mode (sampling).</a></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">## Here's the drill:</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">## 1) encode input and retrieve initial decoder state</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">## 2) run one step of decoder with this initial state</a>
<a class="sourceLine" id="cb5-4" data-line-number="4">## and a "start of sequence" token as target.</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">## Output will be the next target token</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">## 3) Repeat with the current target token and current states</a>
<a class="sourceLine" id="cb5-7" data-line-number="7"></a>
<a class="sourceLine" id="cb5-8" data-line-number="8">## Define sampling models</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">encoder_model &lt;-<span class="st">  </span><span class="kw"><a href="../../reference/keras_model.html">keras_model</a></span>(encoder_inputs, encoder_states)</a>
<a class="sourceLine" id="cb5-10" data-line-number="10">decoder_state_input_h &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape=</span>latent_dim)</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">decoder_state_input_c &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/layer_input.html">layer_input</a></span>(<span class="dt">shape=</span>latent_dim)</a>
<a class="sourceLine" id="cb5-12" data-line-number="12">decoder_states_inputs &lt;-<span class="st"> </span><span class="kw">c</span>(decoder_state_input_h, decoder_state_input_c)</a>
<a class="sourceLine" id="cb5-13" data-line-number="13">decoder_results &lt;-<span class="st"> </span><span class="kw">decoder_lstm</span>(decoder_inputs, <span class="dt">initial_state=</span>decoder_states_inputs)</a>
<a class="sourceLine" id="cb5-14" data-line-number="14">decoder_states  &lt;-<span class="st"> </span>decoder_results[<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>]</a>
<a class="sourceLine" id="cb5-15" data-line-number="15">decoder_outputs &lt;-<span class="st"> </span><span class="kw">decoder_dense</span>(decoder_results[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb5-16" data-line-number="16">decoder_model   &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/keras_model.html">keras_model</a></span>(</a>
<a class="sourceLine" id="cb5-17" data-line-number="17">  <span class="dt">inputs  =</span> <span class="kw">c</span>(decoder_inputs, decoder_states_inputs),</a>
<a class="sourceLine" id="cb5-18" data-line-number="18">  <span class="dt">outputs =</span> <span class="kw">c</span>(decoder_outputs, decoder_states))</a>
<a class="sourceLine" id="cb5-19" data-line-number="19"></a>
<a class="sourceLine" id="cb5-20" data-line-number="20">## Reverse-lookup token index to decode sequences back to</a>
<a class="sourceLine" id="cb5-21" data-line-number="21">## something readable.</a>
<a class="sourceLine" id="cb5-22" data-line-number="22">reverse_input_char_index  &lt;-<span class="st"> </span><span class="kw">as.character</span>(input_characters)</a>
<a class="sourceLine" id="cb5-23" data-line-number="23">reverse_target_char_index &lt;-<span class="st"> </span><span class="kw">as.character</span>(target_characters)</a>
<a class="sourceLine" id="cb5-24" data-line-number="24"></a>
<a class="sourceLine" id="cb5-25" data-line-number="25">decode_sequence &lt;-<span class="st"> </span><span class="cf">function</span>(input_seq) {</a>
<a class="sourceLine" id="cb5-26" data-line-number="26">  ## Encode the input as state vectors.</a>
<a class="sourceLine" id="cb5-27" data-line-number="27">  states_value &lt;-<span class="st"> </span><span class="kw">predict</span>(encoder_model, input_seq)</a>
<a class="sourceLine" id="cb5-28" data-line-number="28">  </a>
<a class="sourceLine" id="cb5-29" data-line-number="29">  ## Generate empty target sequence of length 1.</a>
<a class="sourceLine" id="cb5-30" data-line-number="30">  target_seq &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="dt">dim=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, num_decoder_tokens))</a>
<a class="sourceLine" id="cb5-31" data-line-number="31">  ## Populate the first character of target sequence with the start character.</a>
<a class="sourceLine" id="cb5-32" data-line-number="32">  target_seq[<span class="dv">1</span>, <span class="dv">1</span>, target_token_index[<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>]] &lt;-<span class="st"> </span><span class="fl">1.</span></a>
<a class="sourceLine" id="cb5-33" data-line-number="33">  </a>
<a class="sourceLine" id="cb5-34" data-line-number="34">  ## Sampling loop for a batch of sequences</a>
<a class="sourceLine" id="cb5-35" data-line-number="35">  ## (to simplify, here we assume a batch of size 1).</a>
<a class="sourceLine" id="cb5-36" data-line-number="36">  stop_condition =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb5-37" data-line-number="37">  decoded_sentence =<span class="st"> ''</span></a>
<a class="sourceLine" id="cb5-38" data-line-number="38">  maxiter =<span class="st"> </span>max_decoder_seq_length</a>
<a class="sourceLine" id="cb5-39" data-line-number="39">  niter =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb5-40" data-line-number="40">  <span class="cf">while</span> (<span class="op">!</span>stop_condition <span class="op">&amp;&amp;</span><span class="st"> </span>niter <span class="op">&lt;</span><span class="st"> </span>maxiter) {</a>
<a class="sourceLine" id="cb5-41" data-line-number="41">    </a>
<a class="sourceLine" id="cb5-42" data-line-number="42">    ## output_tokens, h, c = decoder_model.predict([target_seq] + states_value)</a>
<a class="sourceLine" id="cb5-43" data-line-number="43">    decoder_predict &lt;-<span class="st"> </span><span class="kw">predict</span>(decoder_model, <span class="kw">c</span>(<span class="kw">list</span>(target_seq), states_value))</a>
<a class="sourceLine" id="cb5-44" data-line-number="44">    output_tokens &lt;-<span class="st"> </span>decoder_predict[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb5-45" data-line-number="45">    </a>
<a class="sourceLine" id="cb5-46" data-line-number="46">    ## Sample a token</a>
<a class="sourceLine" id="cb5-47" data-line-number="47">    sampled_token_index &lt;-<span class="st"> </span><span class="kw">which.max</span>(output_tokens[<span class="dv">1</span>, <span class="dv">1</span>, ])</a>
<a class="sourceLine" id="cb5-48" data-line-number="48">    sampled_char &lt;-<span class="st"> </span>reverse_target_char_index[sampled_token_index]</a>
<a class="sourceLine" id="cb5-49" data-line-number="49">    decoded_sentence &lt;-<span class="st">  </span><span class="kw">paste0</span>(decoded_sentence, sampled_char)</a>
<a class="sourceLine" id="cb5-50" data-line-number="50">    decoded_sentence</a>
<a class="sourceLine" id="cb5-51" data-line-number="51">    </a>
<a class="sourceLine" id="cb5-52" data-line-number="52">    ## Exit condition: either hit max length</a>
<a class="sourceLine" id="cb5-53" data-line-number="53">    ## or find stop character.</a>
<a class="sourceLine" id="cb5-54" data-line-number="54">    <span class="cf">if</span> (sampled_char <span class="op">==</span><span class="st"> '</span><span class="ch">\n</span><span class="st">'</span> <span class="op">||</span></a>
<a class="sourceLine" id="cb5-55" data-line-number="55"><span class="st">        </span><span class="kw">length</span>(decoded_sentence) <span class="op">&gt;</span><span class="st"> </span>max_decoder_seq_length) {</a>
<a class="sourceLine" id="cb5-56" data-line-number="56">      stop_condition =<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb5-57" data-line-number="57">    }</a>
<a class="sourceLine" id="cb5-58" data-line-number="58">    </a>
<a class="sourceLine" id="cb5-59" data-line-number="59">    ## Update the target sequence (of length 1).</a>
<a class="sourceLine" id="cb5-60" data-line-number="60">    ## target_seq = np.zeros((1, 1, num_decoder_tokens))</a>
<a class="sourceLine" id="cb5-61" data-line-number="61">    target_seq[<span class="dv">1</span>, <span class="dv">1</span>, ] &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb5-62" data-line-number="62">    target_seq[<span class="dv">1</span>, <span class="dv">1</span>, sampled_token_index] &lt;-<span class="st"> </span><span class="fl">1.</span></a>
<a class="sourceLine" id="cb5-63" data-line-number="63">    </a>
<a class="sourceLine" id="cb5-64" data-line-number="64">    ## Update states</a>
<a class="sourceLine" id="cb5-65" data-line-number="65">    h &lt;-<span class="st"> </span>decoder_predict[[<span class="dv">2</span>]]</a>
<a class="sourceLine" id="cb5-66" data-line-number="66">    c &lt;-<span class="st"> </span>decoder_predict[[<span class="dv">3</span>]]</a>
<a class="sourceLine" id="cb5-67" data-line-number="67">    states_value =<span class="st"> </span><span class="kw">list</span>(h, c)</a>
<a class="sourceLine" id="cb5-68" data-line-number="68">    niter &lt;-<span class="st"> </span>niter <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb5-69" data-line-number="69">  }    </a>
<a class="sourceLine" id="cb5-70" data-line-number="70">  <span class="kw">return</span>(decoded_sentence)</a>
<a class="sourceLine" id="cb5-71" data-line-number="71">}</a>
<a class="sourceLine" id="cb5-72" data-line-number="72"></a>
<a class="sourceLine" id="cb5-73" data-line-number="73"><span class="cf">for</span> (seq_index <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb5-74" data-line-number="74">  ## Take one sequence (part of the training test)</a>
<a class="sourceLine" id="cb5-75" data-line-number="75">  ## for trying out decoding.</a>
<a class="sourceLine" id="cb5-76" data-line-number="76">  input_seq =<span class="st"> </span>encoder_input_data[seq_index,,,drop=<span class="ot">FALSE</span>]</a>
<a class="sourceLine" id="cb5-77" data-line-number="77">  decoded_sentence =<span class="st"> </span><span class="kw">decode_sequence</span>(input_seq)</a>
<a class="sourceLine" id="cb5-78" data-line-number="78">  target_sentence &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">"</span><span class="ch">\t</span><span class="st">|</span><span class="ch">\n</span><span class="st">"</span>,<span class="st">""</span>,<span class="kw">paste</span>(target_texts[[seq_index]],<span class="dt">collapse=</span><span class="st">''</span>))</a>
<a class="sourceLine" id="cb5-79" data-line-number="79">  input_sentence  &lt;-<span class="st"> </span><span class="kw">paste</span>(input_texts[[seq_index]],<span class="dt">collapse=</span><span class="st">''</span>)</a>
<a class="sourceLine" id="cb5-80" data-line-number="80">  <span class="kw">cat</span>(<span class="st">'-</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb5-81" data-line-number="81">  <span class="kw">cat</span>(<span class="st">'Input sentence  : '</span>, input_sentence,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb5-82" data-line-number="82">  <span class="kw">cat</span>(<span class="st">'Target sentence : '</span>, target_sentence,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb5-83" data-line-number="83">  <span class="kw">cat</span>(<span class="st">'Decoded sentence: '</span>, decoded_sentence,<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</a>
<a class="sourceLine" id="cb5-84" data-line-number="84">}</a></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by JJ Allaire, François Chollet,  RStudio,  Google.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
