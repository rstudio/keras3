<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="An all-convolutional network applied to patches of images.">
<title>Image classification with ConvMixer • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Image classification with ConvMixer">
<meta property="og:description" content="An all-convolutional network applied to patches of images.">
<meta property="og:image" content="https://keras.posit.co/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Image classification with ConvMixer</h1>
                        <h4 data-toc-skip class="author"><a href="https://twitter.com/RisingSayak" class="external-link">Sayak Paul</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/convmixer.Rmd" class="external-link"><code>vignettes/examples/convmixer.Rmd</code></a></small>
      <div class="d-none name"><code>convmixer.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Vision Transformers (ViT; <a href="https://arxiv.org/abs/1612.00593" class="external-link">Dosovitskiy et al.</a>) extract
small patches from the input images, linearly project them, and then
apply the Transformer (<a href="https://arxiv.org/abs/1706.03762" class="external-link">Vaswani et al.</a>) blocks. The
application of ViTs to image recognition tasks is quickly becoming a
promising area of research, because ViTs eliminate the need to have
strong inductive biases (such as convolutions) for modeling locality.
This presents them as a general computation primititive capable of
learning just from the training data with as minimal inductive priors as
possible. ViTs yield great downstream performance when trained with
proper regularization, data augmentation, and relatively large
datasets.</p>
<p>In the <a href="https://openreview.net/pdf?id=TVHS5Y4dNvM" class="external-link">Patches
Are All You Need</a> paper (note: at the time of writing, it is a
submission to the ICLR 2022 conference), the authors extend the idea of
using patches to train an all-convolutional network and demonstrate
competitive results. Their architecture namely
<strong>ConvMixer</strong> uses recipes from the recent isotrophic
architectures like ViT, MLP-Mixer (<a href="https://arxiv.org/abs/2105.01601" class="external-link">Tolstikhin et al.</a>), such as
using the same depth and resolution across different layers in the
network, residual connections, and so on.</p>
<p>In this example, we will implement the ConvMixer model and
demonstrate its performance on the CIFAR-10 dataset.</p>
</div>
<div class="section level2">
<h2 id="imports">Imports<a class="anchor" aria-label="anchor" href="#imports"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"jax"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="hyperparameters">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h2>
<p>To keep run time short, we will train the model for only 10 epochs.
To focus on the core ideas of ConvMixer, we will not use other
training-specific elements like RandAugment (<a href="https://arxiv.org/abs/1909.13719" class="external-link">Cubuk et al.</a>). If you are
interested in learning more about those details, please refer to the <a href="https://openreview.net/pdf?id=TVHS5Y4dNvM" class="external-link">original paper</a>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="load-the-cifar-10-dataset">Load the CIFAR-10 dataset<a class="anchor" aria-label="anchor" href="#load-the-cifar-10-dataset"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar10.load_data()</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>val_split <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>val_indices <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(x_train) <span class="op">*</span> val_split)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>new_x_train, new_y_train <span class="op">=</span> x_train[val_indices:], y_train[val_indices:]</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>x_val, y_val <span class="op">=</span> x_train[:val_indices], y_train[:val_indices]</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training data samples: </span><span class="sc">{</span><span class="bu">len</span>(new_x_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation data samples: </span><span class="sc">{</span><span class="bu">len</span>(x_val)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test data samples: </span><span class="sc">{</span><span class="bu">len</span>(x_test)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-tf-data-dataset-objects">Prepare <code>tf.data.Dataset</code> objects<a class="anchor" aria-label="anchor" href="#prepare-tf-data-dataset-objects"></a>
</h2>
<p>Our data augmentation pipeline is different from what the authors
used for the CIFAR-10 dataset, which is fine for the purpose of the
example. Note that, it’s ok to use <strong>TF APIs for data I/O and
preprocessing</strong> with other backends (jax, torch) as it is
feature-complete framework when it comes to data preprocessing.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>auto <span class="op">=</span> tf.data.AUTOTUNE</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>augmentation_layers <span class="op">=</span> [</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    keras.layers.RandomCrop(image_size, image_size),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    keras.layers.RandomFlip(<span class="st">"horizontal"</span>)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>]</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="kw">def</span> augment_images(images):</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> augmentation_layers:</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        images <span class="op">=</span> layer(images, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    <span class="cf">return</span> images</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="kw">def</span> make_datasets(images, labels, is_train<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>    dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices((images, labels))</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="cf">if</span> is_train:</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.shuffle(batch_size <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(batch_size)</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>    <span class="cf">if</span> is_train:</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>        dataset <span class="op">=</span> dataset.<span class="bu">map</span>(</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>            <span class="kw">lambda</span> x, y: (augment_images(x), y), num_parallel_calls<span class="op">=</span>auto</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>        )</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>    <span class="cf">return</span> dataset.prefetch(auto)</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>train_dataset <span class="op">=</span> make_datasets(new_x_train, new_y_train, is_train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>val_dataset <span class="op">=</span> make_datasets(x_val, y_val)</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>test_dataset <span class="op">=</span> make_datasets(x_test, y_test)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="convmixer-utilities">ConvMixer utilities<a class="anchor" aria-label="anchor" href="#convmixer-utilities"></a>
</h2>
<p>The following figure (taken from the original paper) depicts the
ConvMixer model:</p>
<p><img src="https://i.imgur.com/yF8actg.png"></p>
<p>ConvMixer is very similar to the MLP-Mixer, model with the following
key differences:</p>
<ul>
<li>Instead of using fully-connected layers, it uses standard
convolution layers.</li>
<li>Instead of LayerNorm (which is typical for ViTs and MLP-Mixers), it
uses BatchNorm.</li>
</ul>
<p>Two types of convolution layers are used in ConvMixer.
<strong>(1)</strong>: Depthwise convolutions, for mixing spatial
locations of the images, <strong>(2)</strong>: Pointwise convolutions
(which follow the depthwise convolutions), for mixing channel-wise
information across the patches. Another keypoint is the use of
<em>larger kernel sizes</em> to allow a larger receptive field.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> activation_block(x):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    x <span class="op">=</span> layers.Activation(<span class="st">"gelu"</span>)(x)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    <span class="cf">return</span> layers.BatchNormalization()(x)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="kw">def</span> conv_stem(x, filters: <span class="bu">int</span>, patch_size: <span class="bu">int</span>):</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(filters, kernel_size<span class="op">=</span>patch_size, strides<span class="op">=</span>patch_size)(x)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    <span class="cf">return</span> activation_block(x)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="kw">def</span> conv_mixer_block(x, filters: <span class="bu">int</span>, kernel_size: <span class="bu">int</span>):</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    <span class="co"># Depthwise convolution.</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    x0 <span class="op">=</span> x</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    x <span class="op">=</span> layers.DepthwiseConv2D(kernel_size<span class="op">=</span>kernel_size, padding<span class="op">=</span><span class="st">"same"</span>)(x)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    x <span class="op">=</span> layers.Add()([activation_block(x), x0])  <span class="co"># Residual.</span></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    <span class="co"># Pointwise convolution.</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>    x <span class="op">=</span> layers.Conv2D(filters, kernel_size<span class="op">=</span><span class="dv">1</span>)(x)</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>    x <span class="op">=</span> activation_block(x)</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="kw">def</span> get_conv_mixer_256_8(</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>    image_size<span class="op">=</span><span class="dv">32</span>, filters<span class="op">=</span><span class="dv">256</span>, depth<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, patch_size<span class="op">=</span><span class="dv">2</span>, num_classes<span class="op">=</span><span class="dv">10</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>):</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>    <span class="co">"""ConvMixer-256/8: https://openreview.net/pdf?id=TVHS5Y4dNvM.</span></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a><span class="co">    The hyperparameter values are taken from the paper.</span></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>    inputs <span class="op">=</span> keras.Input((image_size, image_size, <span class="dv">3</span>))</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>    x <span class="op">=</span> layers.Rescaling(scale<span class="op">=</span><span class="fl">1.0</span> <span class="op">/</span> <span class="dv">255</span>)(inputs)</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>    <span class="co"># Extract patch embeddings.</span></span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>    x <span class="op">=</span> conv_stem(x, filters, patch_size)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>    <span class="co"># ConvMixer blocks.</span></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(depth):</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>        x <span class="op">=</span> conv_mixer_block(x, filters, kernel_size)</span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>    <span class="co"># Classification block.</span></span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a>    x <span class="op">=</span> layers.GlobalAvgPool2D()(x)</span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(num_classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, outputs)</span></code></pre></div>
<p>The model used in this experiment is termed as
<strong>ConvMixer-256/8</strong> where 256 denotes the number of
channels and 8 denotes the depth. The resulting model only has 0.8
million parameters.</p>
</div>
<div class="section level2">
<h2 id="model-training-and-evaluation-utility">Model training and evaluation utility<a class="anchor" aria-label="anchor" href="#model-training-and-evaluation-utility"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Code reference:</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="co"># https://keras.io/examples/vision/image_classification_with_vision_transformer/.</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="kw">def</span> run_experiment(model):</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    optimizer <span class="op">=</span> keras.optimizers.AdamW(</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>weight_decay</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    )</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        optimizer<span class="op">=</span>optimizer,</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    )</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    checkpoint_filepath <span class="op">=</span> <span class="st">"/tmp/checkpoint.keras"</span></span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    checkpoint_callback <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        checkpoint_filepath,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_accuracy"</span>,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>        save_weights_only<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    )</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>    history <span class="op">=</span> model.fit(</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        train_dataset,</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>        validation_data<span class="op">=</span>val_dataset,</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>        epochs<span class="op">=</span>num_epochs,</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>        callbacks<span class="op">=</span>[checkpoint_callback],</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>    )</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>    model.load_weights(checkpoint_filepath)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>    _, accuracy <span class="op">=</span> model.evaluate(test_dataset)</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>    <span class="cf">return</span> history, model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-and-evaluate-model">Train and evaluate model<a class="anchor" aria-label="anchor" href="#train-and-evaluate-model"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>conv_mixer_model <span class="op">=</span> get_conv_mixer_256_8()</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>history, conv_mixer_model <span class="op">=</span> run_experiment(conv_mixer_model)</span></code></pre></div>
<p>The gap in training and validation performance can be mitigated by
using additional regularization techniques. Nevertheless, being able to
get to ~83% accuracy within 10 epochs with 0.8 million parameters is a
strong result.</p>
</div>
<div class="section level2">
<h2 id="visualizing-the-internals-of-convmixer">Visualizing the internals of ConvMixer<a class="anchor" aria-label="anchor" href="#visualizing-the-internals-of-convmixer"></a>
</h2>
<p>We can visualize the patch embeddings and the learned convolution
filters. Recall that each patch embedding and intermediate feature map
have the same number of channels (256 in this case). This will make our
visualization utility easier to implement.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Code reference: https://bit.ly/3awIRbP.</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="kw">def</span> visualization_plot(weights, idx<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    <span class="co"># First, apply min-max normalization to the</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="co"># given weights to avoid isotrophic scaling.</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    p_min, p_max <span class="op">=</span> weights.<span class="bu">min</span>(), weights.<span class="bu">max</span>()</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    weights <span class="op">=</span> (weights <span class="op">-</span> p_min) <span class="op">/</span> (p_max <span class="op">-</span> p_min)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    <span class="co"># Visualize all the filters.</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    num_filters <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_filters):</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>        current_weight <span class="op">=</span> weights[:, :, :, i]</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>        <span class="cf">if</span> current_weight.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>            current_weight <span class="op">=</span> current_weight.squeeze()</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>        ax <span class="op">=</span> plt.subplot(<span class="dv">16</span>, <span class="dv">16</span>, idx)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>        ax.set_xticks([])</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>        ax.set_yticks([])</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>        plt.imshow(current_weight)</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>        idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a><span class="co"># We first visualize the learned patch embeddings.</span></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>patch_embeddings <span class="op">=</span> conv_mixer_model.layers[<span class="dv">2</span>].get_weights()[<span class="dv">0</span>]</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>visualization_plot(patch_embeddings)</span></code></pre></div>
<p>Even though we did not train the network to convergence, we can
notice that different patches show different patterns. Some share
similarity with others while some are very different. These
visualizations are more salient with larger image sizes.</p>
<p>Similarly, we can visualize the raw convolution kernels. This can
help us understand the patterns to which a given kernel is
receptive.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># First, print the indices of the convolution layers that are not</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co"># pointwise convolutions.</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(conv_mixer_model.layers):</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(layer, layers.DepthwiseConv2D):</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        <span class="cf">if</span> layer.get_config()[<span class="st">"kernel_size"</span>] <span class="op">==</span> (<span class="dv">5</span>, <span class="dv">5</span>):</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>            <span class="bu">print</span>(i, layer)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">26</span>  <span class="co"># Taking a kernel from the middle of the network.</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>kernel <span class="op">=</span> conv_mixer_model.layers[idx].get_weights()[<span class="dv">0</span>]</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>kernel <span class="op">=</span> np.expand_dims(kernel.squeeze(), axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>visualization_plot(kernel)</span></code></pre></div>
<p>We see that different filters in the kernel have different locality
spans, and this pattern is likely to evolve with more training.</p>
</div>
<div class="section level2">
<h2 id="final-notes">Final notes<a class="anchor" aria-label="anchor" href="#final-notes"></a>
</h2>
<p>There’s been a recent trend on fusing convolutions with other
data-agnostic operations like self-attention. Following works are along
this line of research:</p>
<ul>
<li>ConViT (<a href="https://arxiv.org/abs/2103.10697" class="external-link">d’Ascoli et
al.</a>)</li>
<li>CCT (<a href="https://arxiv.org/abs/2104.05704" class="external-link">Hassani et
al.</a>)</li>
<li>CoAtNet (<a href="https://arxiv.org/abs/2106.04803" class="external-link">Dai et
al.</a>)</li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
