<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Low-light image enhancement using MIRNet • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Low-light image enhancement using MIRNet">
<meta property="og:description" content="Implementing the MIRNet architecture for low-light image enhancement.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Low-light image enhancement using MIRNet</h1>
                        <h4 data-toc-skip class="author"><a href="http://github.com/soumik12345" class="external-link">Soumik Rakshit</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/mirnet.Rmd" class="external-link"><code>vignettes/examples/mirnet.Rmd</code></a></small>
      <div class="hidden name"><code>mirnet.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>With the goal of recovering high-quality image content from its
degraded version, image restoration enjoys numerous applications, such
as in photography, security, medical imaging, and remote sensing. In
this example, we implement the <strong>MIRNet</strong> model for
low-light image enhancement, a fully-convolutional architecture that
learns an enriched set of features that combines contextual information
from multiple scales, while simultaneously preserving the
high-resolution spatial details.</p>
<div class="section level3">
<h3 id="references">References:<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ul>
<li><a href="https://arxiv.org/abs/2003.06792" class="external-link">Learning Enriched
Features for Real Image Restoration and Enhancement</a></li>
<li><a href="http://www.cnbc.cmu.edu/~tai/cp_papers/E.Land_Retinex_Theory_ScientifcAmerican.pdf" class="external-link">The
Retinex Theory of Color Vision</a></li>
<li><a href="https://ieeexplore.ieee.org/document/413553" class="external-link">Two
deterministic half-quadratic regularization algorithms for computed
imaging</a></li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="downloading-loldataset">Downloading LOLDataset<a class="anchor" aria-label="anchor" href="#downloading-loldataset"></a>
</h2>
<p>The <strong>LoL Dataset</strong> has been created for low-light image
enhancement. It provides 485 images for training and 15 for testing.
Each image pair in the dataset consists of a low-light input image and
its corresponding well-exposed reference image.</p>
<p>pip install -q git+<a href="https://github.com/keras-team/keras" class="external-link uri">https://github.com/keras-team/keras</a></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageOps</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span></code></pre></div>
<p>wget <a href="https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip" class="external-link uri">https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip</a>
unzip -q lol_dataset.zip &amp;&amp; rm lol_dataset.zip</p>
</div>
<div class="section level2">
<h2 id="creating-a-tensorflow-dataset">Creating a TensorFlow Dataset<a class="anchor" aria-label="anchor" href="#creating-a-tensorflow-dataset"></a>
</h2>
<p>We use 300 image pairs from the LoL Dataset’s training set for
training, and we use the remaining 185 image pairs for validation. We
generate random crops of size <code>128 x 128</code> from the image
pairs to be used for both training and validation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>random.seed(<span class="dv">10</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>IMAGE_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>MAX_TRAIN_IMAGES <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="kw">def</span> read_image(image_path):</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    image <span class="op">=</span> tf.io.read_file(image_path)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    image <span class="op">=</span> tf.image.decode_png(image, channels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    image.set_shape([<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>])</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>    image <span class="op">=</span> tf.cast(image, dtype<span class="op">=</span>tf.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="kw">def</span> random_crop(low_image, enhanced_image):</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>    low_image_shape <span class="op">=</span> tf.shape(low_image)[:<span class="dv">2</span>]</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>    low_w <span class="op">=</span> tf.random.uniform(</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>        shape<span class="op">=</span>(), maxval<span class="op">=</span>low_image_shape[<span class="dv">1</span>] <span class="op">-</span> IMAGE_SIZE <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>tf.int32</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>    )</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    low_h <span class="op">=</span> tf.random.uniform(</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>        shape<span class="op">=</span>(), maxval<span class="op">=</span>low_image_shape[<span class="dv">0</span>] <span class="op">-</span> IMAGE_SIZE <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>tf.int32</span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    )</span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>    low_image_cropped <span class="op">=</span> low_image[</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>        low_h : low_h <span class="op">+</span> IMAGE_SIZE, low_w : low_w <span class="op">+</span> IMAGE_SIZE</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>    ]</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>    enhanced_image_cropped <span class="op">=</span> enhanced_image[</span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>        low_h : low_h <span class="op">+</span> IMAGE_SIZE, low_w : low_w <span class="op">+</span> IMAGE_SIZE</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>    ]</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>    <span class="co"># in order to avoid `NONE` during shape inference</span></span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>    low_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, <span class="dv">3</span>])</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>    enhanced_image_cropped.set_shape([IMAGE_SIZE, IMAGE_SIZE, <span class="dv">3</span>])</span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a>    <span class="cf">return</span> low_image_cropped, enhanced_image_cropped</span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a><span class="kw">def</span> load_data(low_light_image_path, enhanced_image_path):</span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>    low_light_image <span class="op">=</span> read_image(low_light_image_path)</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>    enhanced_image <span class="op">=</span> read_image(enhanced_image_path)</span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    low_light_image, enhanced_image <span class="op">=</span> random_crop(</span>
<span id="cb2-40"><a href="#cb2-40" tabindex="-1"></a>        low_light_image, enhanced_image</span>
<span id="cb2-41"><a href="#cb2-41" tabindex="-1"></a>    )</span>
<span id="cb2-42"><a href="#cb2-42" tabindex="-1"></a>    <span class="cf">return</span> low_light_image, enhanced_image</span>
<span id="cb2-43"><a href="#cb2-43" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" tabindex="-1"></a><span class="kw">def</span> get_dataset(low_light_images, enhanced_images):</span>
<span id="cb2-46"><a href="#cb2-46" tabindex="-1"></a>    dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices(</span>
<span id="cb2-47"><a href="#cb2-47" tabindex="-1"></a>        (low_light_images, enhanced_images)</span>
<span id="cb2-48"><a href="#cb2-48" tabindex="-1"></a>    )</span>
<span id="cb2-49"><a href="#cb2-49" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.<span class="bu">map</span>(load_data, num_parallel_calls<span class="op">=</span>tf.data.AUTOTUNE)</span>
<span id="cb2-50"><a href="#cb2-50" tabindex="-1"></a>    dataset <span class="op">=</span> dataset.batch(BATCH_SIZE, drop_remainder<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-51"><a href="#cb2-51" tabindex="-1"></a>    <span class="cf">return</span> dataset</span>
<span id="cb2-52"><a href="#cb2-52" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" tabindex="-1"></a>train_low_light_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/our485/low/*"</span>))[</span>
<span id="cb2-55"><a href="#cb2-55" tabindex="-1"></a>    :MAX_TRAIN_IMAGES</span>
<span id="cb2-56"><a href="#cb2-56" tabindex="-1"></a>]</span>
<span id="cb2-57"><a href="#cb2-57" tabindex="-1"></a>train_enhanced_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/our485/high/*"</span>))[</span>
<span id="cb2-58"><a href="#cb2-58" tabindex="-1"></a>    :MAX_TRAIN_IMAGES</span>
<span id="cb2-59"><a href="#cb2-59" tabindex="-1"></a>]</span>
<span id="cb2-60"><a href="#cb2-60" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" tabindex="-1"></a>val_low_light_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/our485/low/*"</span>))[</span>
<span id="cb2-62"><a href="#cb2-62" tabindex="-1"></a>    MAX_TRAIN_IMAGES:</span>
<span id="cb2-63"><a href="#cb2-63" tabindex="-1"></a>]</span>
<span id="cb2-64"><a href="#cb2-64" tabindex="-1"></a>val_enhanced_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/our485/high/*"</span>))[</span>
<span id="cb2-65"><a href="#cb2-65" tabindex="-1"></a>    MAX_TRAIN_IMAGES:</span>
<span id="cb2-66"><a href="#cb2-66" tabindex="-1"></a>]</span>
<span id="cb2-67"><a href="#cb2-67" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" tabindex="-1"></a>test_low_light_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/eval15/low/*"</span>))</span>
<span id="cb2-69"><a href="#cb2-69" tabindex="-1"></a>test_enhanced_images <span class="op">=</span> <span class="bu">sorted</span>(glob(<span class="st">"./lol_dataset/eval15/high/*"</span>))</span>
<span id="cb2-70"><a href="#cb2-70" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" tabindex="-1"></a>train_dataset <span class="op">=</span> get_dataset(train_low_light_images, train_enhanced_images)</span>
<span id="cb2-73"><a href="#cb2-73" tabindex="-1"></a>val_dataset <span class="op">=</span> get_dataset(val_low_light_images, val_enhanced_images)</span>
<span id="cb2-74"><a href="#cb2-74" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train Dataset:"</span>, train_dataset.element_spec)</span>
<span id="cb2-77"><a href="#cb2-77" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Val Dataset:"</span>, val_dataset.element_spec)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="mirnet-model">MIRNet Model<a class="anchor" aria-label="anchor" href="#mirnet-model"></a>
</h2>
<p>Here are the main features of the MIRNet model:</p>
<ul>
<li>A feature extraction model that computes a complementary set of
features across multiple spatial scales, while maintaining the original
high-resolution features to preserve precise spatial details.</li>
<li>A regularly repeated mechanism for information exchange, where the
features across multi-resolution branches are progressively fused
together for improved representation learning.</li>
<li>A new approach to fuse multi-scale features using a selective kernel
network that dynamically combines variable receptive fields and
faithfully preserves the original feature information at each spatial
resolution.</li>
<li>A recursive residual design that progressively breaks down the input
signal in order to simplify the overall learning process, and allows the
construction of very deep networks.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/soumik12345/MIRNet/master/assets/mirnet_architecture.png"></p>
<div class="section level3">
<h3 id="selective-kernel-feature-fusion">Selective Kernel Feature Fusion<a class="anchor" aria-label="anchor" href="#selective-kernel-feature-fusion"></a>
</h3>
<p>The Selective Kernel Feature Fusion or SKFF module performs dynamic
adjustment of receptive fields via two operations: <strong>Fuse</strong>
and <strong>Select</strong>. The Fuse operator generates global feature
descriptors by combining the information from multi-resolution streams.
The Select operator uses these descriptors to recalibrate the feature
maps (of different streams) followed by their aggregation.</p>
<p><strong>Fuse</strong>: The SKFF receives inputs from three parallel
convolution streams carrying different scales of information. We first
combine these multi-scale features using an element-wise sum, on which
we apply Global Average Pooling (GAP) across the spatial dimension.
Next, we apply a channel- downscaling convolution layer to generate a
compact feature representation which passes through three parallel
channel-upscaling convolution layers (one for each resolution stream)
and provides us with three feature descriptors.</p>
<p><strong>Select</strong>: This operator applies the softmax function
to the feature descriptors to obtain the corresponding activations that
are used to adaptively recalibrate multi-scale feature maps. The
aggregated features are defined as the sum of product of the
corresponding multi-scale feature and the feature descriptor.</p>
<p><img src="https://i.imgur.com/7U6ixF6.png"></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> selective_kernel_feature_fusion(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>):</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    channels <span class="op">=</span> <span class="bu">list</span>(multi_scale_feature_1.shape)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    combined_feature <span class="op">=</span> layers.Add()(</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>        [multi_scale_feature_1, multi_scale_feature_2, multi_scale_feature_3]</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    )</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    gap <span class="op">=</span> layers.GlobalAveragePooling2D()(combined_feature)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    channel_wise_statistics <span class="op">=</span> layers.Reshape((<span class="dv">1</span>, <span class="dv">1</span>, channels))(gap)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    compact_feature_representation <span class="op">=</span> layers.Conv2D(</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>        filters<span class="op">=</span>channels <span class="op">//</span> <span class="dv">8</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    )(channel_wise_statistics)</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>    feature_descriptor_1 <span class="op">=</span> layers.Conv2D(</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"softmax"</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>    )(compact_feature_representation)</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>    feature_descriptor_2 <span class="op">=</span> layers.Conv2D(</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"softmax"</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>    )(compact_feature_representation)</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>    feature_descriptor_3 <span class="op">=</span> layers.Conv2D(</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"softmax"</span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>    )(compact_feature_representation)</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>    feature_1 <span class="op">=</span> multi_scale_feature_1 <span class="op">*</span> feature_descriptor_1</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>    feature_2 <span class="op">=</span> multi_scale_feature_2 <span class="op">*</span> feature_descriptor_2</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>    feature_3 <span class="op">=</span> multi_scale_feature_3 <span class="op">*</span> feature_descriptor_3</span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>    aggregated_feature <span class="op">=</span> layers.Add()([feature_1, feature_2, feature_3])</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>    <span class="cf">return</span> aggregated_feature</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="dual-attention-unit">Dual Attention Unit<a class="anchor" aria-label="anchor" href="#dual-attention-unit"></a>
</h3>
<p>The Dual Attention Unit or DAU is used to extract features in the
convolutional streams. While the SKFF block fuses information across
multi-resolution branches, we also need a mechanism to share information
within a feature tensor, both along the spatial and the channel
dimensions which is done by the DAU block. The DAU suppresses less
useful features and only allows more informative ones to pass further.
This feature recalibration is achieved by using <strong>Channel
Attention</strong> and <strong>Spatial Attention</strong>
mechanisms.</p>
<p>The <strong>Channel Attention</strong> branch exploits the
inter-channel relationships of the convolutional feature maps by
applying squeeze and excitation operations. Given a feature map, the
squeeze operation applies Global Average Pooling across spatial
dimensions to encode global context, thus yielding a feature descriptor.
The excitation operator passes this feature descriptor through two
convolutional layers followed by the sigmoid gating and generates
activations. Finally, the output of Channel Attention branch is obtained
by rescaling the input feature map with the output activations.</p>
<p>The <strong>Spatial Attention</strong> branch is designed to exploit
the inter-spatial dependencies of convolutional features. The goal of
Spatial Attention is to generate a spatial attention map and use it to
recalibrate the incoming features. To generate the spatial attention
map, the Spatial Attention branch first independently applies Global
Average Pooling and Max Pooling operations on input features along the
channel dimensions and concatenates the outputs to form a resultant
feature map which is then passed through a convolution and sigmoid
activation to obtain the spatial attention map. This spatial attention
map is then used to rescale the input feature map.</p>
<p><img src="https://i.imgur.com/Dl0IwQs.png"></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">class</span> ChannelPooling(layers.Layer):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, axis<span class="op">=-</span><span class="dv">1</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        <span class="va">self</span>.axis <span class="op">=</span> axis</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        <span class="va">self</span>.concat <span class="op">=</span> layers.Concatenate(axis<span class="op">=</span><span class="va">self</span>.axis)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        average_pooling <span class="op">=</span> tf.expand_dims(</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>            tf.reduce_mean(inputs, axis<span class="op">=-</span><span class="dv">1</span>), axis<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>        )</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        max_pooling <span class="op">=</span> tf.expand_dims(tf.reduce_max(inputs, axis<span class="op">=-</span><span class="dv">1</span>), axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.concat([average_pooling, max_pooling])</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="kw">def</span> get_config(<span class="va">self</span>):</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>        config <span class="op">=</span> <span class="bu">super</span>().get_config()</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>        config.update({<span class="st">"axis"</span>: <span class="va">self</span>.axis})</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a><span class="kw">def</span> spatial_attention_block(input_tensor):</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>    compressed_feature_map <span class="op">=</span> ChannelPooling(axis<span class="op">=-</span><span class="dv">1</span>)(input_tensor)</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>    feature_map <span class="op">=</span> layers.Conv2D(<span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(compressed_feature_map)</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    feature_map <span class="op">=</span> keras.activations.sigmoid(feature_map)</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    <span class="cf">return</span> input_tensor <span class="op">*</span> feature_map</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a><span class="kw">def</span> channel_attention_block(input_tensor):</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>    channels <span class="op">=</span> <span class="bu">list</span>(input_tensor.shape)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>    average_pooling <span class="op">=</span> layers.GlobalAveragePooling2D()(input_tensor)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>    feature_descriptor <span class="op">=</span> layers.Reshape((<span class="dv">1</span>, <span class="dv">1</span>, channels))(average_pooling)</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>    feature_activations <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>        filters<span class="op">=</span>channels <span class="op">//</span> <span class="dv">8</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>    )(feature_descriptor)</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>    feature_activations <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>        filters<span class="op">=</span>channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"sigmoid"</span></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>    )(feature_activations)</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>    <span class="cf">return</span> input_tensor <span class="op">*</span> feature_activations</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a><span class="kw">def</span> dual_attention_unit_block(input_tensor):</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    channels <span class="op">=</span> <span class="bu">list</span>(input_tensor.shape)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>    feature_map <span class="op">=</span> layers.Conv2D(</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>    )(input_tensor)</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>    feature_map <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>        feature_map</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>    )</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>    channel_attention <span class="op">=</span> channel_attention_block(feature_map)</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>    spatial_attention <span class="op">=</span> spatial_attention_block(feature_map)</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>    concatenation <span class="op">=</span> layers.Concatenate(axis<span class="op">=-</span><span class="dv">1</span>)(</span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>        [channel_attention, spatial_attention]</span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a>    )</span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a>    concatenation <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(concatenation)</span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([input_tensor, concatenation])</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="multi-scale-residual-block">Multi-Scale Residual Block<a class="anchor" aria-label="anchor" href="#multi-scale-residual-block"></a>
</h3>
<p>The Multi-Scale Residual Block is capable of generating a
spatially-precise output by maintaining high-resolution representations,
while receiving rich contextual information from low-resolutions. The
MRB consists of multiple (three in this paper) fully-convolutional
streams connected in parallel. It allows information exchange across
parallel streams in order to consolidate the high-resolution features
with the help of low-resolution features, and vice versa. The MIRNet
employs a recursive residual design (with skip connections) to ease the
flow of information during the learning process. In order to maintain
the residual nature of our architecture, residual resizing modules are
used to perform downsampling and upsampling operations that are used in
the Multi-scale Residual Block.</p>
<p><img src="https://i.imgur.com/wzZKV57.png"></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Recursive Residual Modules</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="kw">def</span> down_sampling_module(input_tensor):</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    channels <span class="op">=</span> <span class="bu">list</span>(input_tensor.shape)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>    )(input_tensor)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    )(main_branch)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.MaxPooling2D()(main_branch)</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(channels <span class="op">*</span> <span class="dv">2</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(main_branch)</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    skip_branch <span class="op">=</span> layers.MaxPooling2D()(input_tensor)</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    skip_branch <span class="op">=</span> layers.Conv2D(channels <span class="op">*</span> <span class="dv">2</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(skip_branch)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([skip_branch, main_branch])</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a><span class="kw">def</span> up_sampling_module(input_tensor):</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>    channels <span class="op">=</span> <span class="bu">list</span>(input_tensor.shape)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>), activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>    )(input_tensor)</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a>        channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>    )(main_branch)</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.UpSampling2D()(main_branch)</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>    main_branch <span class="op">=</span> layers.Conv2D(channels <span class="op">//</span> <span class="dv">2</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(main_branch)</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>    skip_branch <span class="op">=</span> layers.UpSampling2D()(input_tensor)</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>    skip_branch <span class="op">=</span> layers.Conv2D(channels <span class="op">//</span> <span class="dv">2</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))(skip_branch)</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([skip_branch, main_branch])</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a><span class="co"># MRB Block</span></span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a><span class="kw">def</span> multi_scale_residual_block(input_tensor, channels):</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>    <span class="co"># features</span></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>    level1 <span class="op">=</span> input_tensor</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>    level2 <span class="op">=</span> down_sampling_module(input_tensor)</span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>    level3 <span class="op">=</span> down_sampling_module(level2)</span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>    <span class="co"># DAU</span></span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a>    level1_dau <span class="op">=</span> dual_attention_unit_block(level1)</span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a>    level2_dau <span class="op">=</span> dual_attention_unit_block(level2)</span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a>    level3_dau <span class="op">=</span> dual_attention_unit_block(level3)</span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>    <span class="co"># SKFF</span></span>
<span id="cb5-45"><a href="#cb5-45" tabindex="-1"></a>    level1_skff <span class="op">=</span> selective_kernel_feature_fusion(</span>
<span id="cb5-46"><a href="#cb5-46" tabindex="-1"></a>        level1_dau,</span>
<span id="cb5-47"><a href="#cb5-47" tabindex="-1"></a>        up_sampling_module(level2_dau),</span>
<span id="cb5-48"><a href="#cb5-48" tabindex="-1"></a>        up_sampling_module(up_sampling_module(level3_dau)),</span>
<span id="cb5-49"><a href="#cb5-49" tabindex="-1"></a>    )</span>
<span id="cb5-50"><a href="#cb5-50" tabindex="-1"></a>    level2_skff <span class="op">=</span> selective_kernel_feature_fusion(</span>
<span id="cb5-51"><a href="#cb5-51" tabindex="-1"></a>        down_sampling_module(level1_dau),</span>
<span id="cb5-52"><a href="#cb5-52" tabindex="-1"></a>        level2_dau,</span>
<span id="cb5-53"><a href="#cb5-53" tabindex="-1"></a>        up_sampling_module(level3_dau),</span>
<span id="cb5-54"><a href="#cb5-54" tabindex="-1"></a>    )</span>
<span id="cb5-55"><a href="#cb5-55" tabindex="-1"></a>    level3_skff <span class="op">=</span> selective_kernel_feature_fusion(</span>
<span id="cb5-56"><a href="#cb5-56" tabindex="-1"></a>        down_sampling_module(down_sampling_module(level1_dau)),</span>
<span id="cb5-57"><a href="#cb5-57" tabindex="-1"></a>        down_sampling_module(level2_dau),</span>
<span id="cb5-58"><a href="#cb5-58" tabindex="-1"></a>        level3_dau,</span>
<span id="cb5-59"><a href="#cb5-59" tabindex="-1"></a>    )</span>
<span id="cb5-60"><a href="#cb5-60" tabindex="-1"></a>    <span class="co"># DAU 2</span></span>
<span id="cb5-61"><a href="#cb5-61" tabindex="-1"></a>    level1_dau_2 <span class="op">=</span> dual_attention_unit_block(level1_skff)</span>
<span id="cb5-62"><a href="#cb5-62" tabindex="-1"></a>    level2_dau_2 <span class="op">=</span> up_sampling_module((dual_attention_unit_block(level2_skff)))</span>
<span id="cb5-63"><a href="#cb5-63" tabindex="-1"></a>    level3_dau_2 <span class="op">=</span> up_sampling_module(</span>
<span id="cb5-64"><a href="#cb5-64" tabindex="-1"></a>        up_sampling_module(dual_attention_unit_block(level3_skff))</span>
<span id="cb5-65"><a href="#cb5-65" tabindex="-1"></a>    )</span>
<span id="cb5-66"><a href="#cb5-66" tabindex="-1"></a>    <span class="co"># SKFF 2</span></span>
<span id="cb5-67"><a href="#cb5-67" tabindex="-1"></a>    skff_ <span class="op">=</span> selective_kernel_feature_fusion(</span>
<span id="cb5-68"><a href="#cb5-68" tabindex="-1"></a>        level1_dau_2, level2_dau_2, level3_dau_2</span>
<span id="cb5-69"><a href="#cb5-69" tabindex="-1"></a>    )</span>
<span id="cb5-70"><a href="#cb5-70" tabindex="-1"></a>    conv <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(skff_)</span>
<span id="cb5-71"><a href="#cb5-71" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([input_tensor, conv])</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="mirnet-model-1">MIRNet Model<a class="anchor" aria-label="anchor" href="#mirnet-model-1"></a>
</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> recursive_residual_group(input_tensor, num_mrb, channels):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    conv1 <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>        input_tensor</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    )</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_mrb):</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>        conv1 <span class="op">=</span> multi_scale_residual_block(conv1, channels)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    conv2 <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(conv1)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    <span class="cf">return</span> layers.Add()([conv2, input_tensor])</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a><span class="kw">def</span> mirnet_model(num_rrg, num_mrb, channels):</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>    input_tensor <span class="op">=</span> keras.Input(shape<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>])</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    x1 <span class="op">=</span> layers.Conv2D(channels, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>        input_tensor</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    )</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_rrg):</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        x1 <span class="op">=</span> recursive_residual_group(x1, num_mrb, channels)</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    conv <span class="op">=</span> layers.Conv2D(<span class="dv">3</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), padding<span class="op">=</span><span class="st">"same"</span>)(x1)</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    output_tensor <span class="op">=</span> layers.Add()([input_tensor, conv])</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(input_tensor, output_tensor)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>model <span class="op">=</span> mirnet_model(num_rrg<span class="op">=</span><span class="dv">3</span>, num_mrb<span class="op">=</span><span class="dv">2</span>, channels<span class="op">=</span><span class="dv">64</span>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="training">Training<a class="anchor" aria-label="anchor" href="#training"></a>
</h2>
<ul>
<li>We train MIRNet using <strong>Charbonnier Loss</strong> as the loss
function and <strong>Adam Optimizer</strong> with a learning rate of
<code>1e-4</code>.</li>
<li>We use <strong>Peak Signal Noise Ratio</strong> or PSNR as a metric
which is an expression for the ratio between the maximum possible value
(power) of a signal and the power of distorting noise that affects the
quality of its representation.</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> charbonnier_loss(y_true, y_pred):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="cf">return</span> tf.reduce_mean(tf.sqrt(tf.square(y_true <span class="op">-</span> y_pred) <span class="op">+</span> tf.square(<span class="fl">1e-3</span>)))</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="kw">def</span> peak_signal_noise_ratio(y_true, y_pred):</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="cf">return</span> tf.image.psnr(y_pred, y_true, max_val<span class="op">=</span><span class="fl">255.0</span>)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>optimizer <span class="op">=</span> keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>    loss<span class="op">=</span>charbonnier_loss,</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>    metrics<span class="op">=</span>[peak_signal_noise_ratio],</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>    train_dataset,</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    validation_data<span class="op">=</span>val_dataset,</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>        keras.callbacks.ReduceLROnPlateau(</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>            monitor<span class="op">=</span><span class="st">"val_peak_signal_noise_ratio"</span>,</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>            factor<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a>            patience<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>            verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>            min_delta<span class="op">=</span><span class="fl">1e-7</span>,</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"max"</span>,</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>        )</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>    ],</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a>)</span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a><span class="kw">def</span> plot_history(value):</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>    plt.plot(history.history[value], label<span class="op">=</span><span class="ss">f"train_</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>    plt.plot(history.history[<span class="ss">f"val_</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>], label<span class="op">=</span><span class="ss">f"val_</span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>    plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>    plt.ylabel(value)</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>    plt.title(<span class="ss">f"Train and Validation </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss"> Over Epochs"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>    plt.legend()</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>    plt.grid()</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>plot_history(<span class="st">"loss"</span>)</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>plot_history(<span class="st">"peak_signal_noise_ratio"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="inference">Inference<a class="anchor" aria-label="anchor" href="#inference"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> plot_results(images, titles, figure_size<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>)):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>figure_size)</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(images)):</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>        fig.add_subplot(<span class="dv">1</span>, <span class="bu">len</span>(images), i <span class="op">+</span> <span class="dv">1</span>).set_title(titles[i])</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        _ <span class="op">=</span> plt.imshow(images[i])</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    plt.show()</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="kw">def</span> infer(original_image):</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    image <span class="op">=</span> keras.utils.img_to_array(original_image)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    image <span class="op">=</span> image.astype(<span class="st">"float32"</span>) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    image <span class="op">=</span> np.expand_dims(image, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    output <span class="op">=</span> model.predict(image, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>    output_image <span class="op">=</span> output[<span class="dv">0</span>] <span class="op">*</span> <span class="fl">255.0</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    output_image <span class="op">=</span> output_image.clip(<span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    output_image <span class="op">=</span> output_image.reshape(</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>        (np.shape(output_image)[<span class="dv">0</span>], np.shape(output_image)[<span class="dv">1</span>], <span class="dv">3</span>)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    )</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    output_image <span class="op">=</span> Image.fromarray(np.uint8(output_image))</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    original_image <span class="op">=</span> Image.fromarray(np.uint8(original_image))</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    <span class="cf">return</span> output_image</span></code></pre></div>
<div class="section level3">
<h3 id="inference-on-test-images">Inference on Test Images<a class="anchor" aria-label="anchor" href="#inference-on-test-images"></a>
</h3>
<p>We compare the test images from LOLDataset enhanced by MIRNet with
images enhanced via the <code>PIL.ImageOps.autocontrast()</code>
function.</p>
<p>You can use the trained model hosted on <a href="https://huggingface.co/keras-io/lowlight-enhance-mirnet" class="external-link">Hugging
Face Hub</a> and try the demo on <a href="https://huggingface.co/spaces/keras-io/Enhance_Low_Light_Image" class="external-link">Hugging
Face Spaces</a>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="cf">for</span> low_light_image <span class="kw">in</span> random.sample(test_low_light_images, <span class="dv">6</span>):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    original_image <span class="op">=</span> Image.<span class="bu">open</span>(low_light_image)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    enhanced_image <span class="op">=</span> infer(original_image)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    plot_results(</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>        [original_image, ImageOps.autocontrast(original_image), enhanced_image],</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>        [<span class="st">"Original"</span>, <span class="st">"PIL Autocontrast"</span>, <span class="st">"MIRNet Enhanced"</span>],</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>        (<span class="dv">20</span>, <span class="dv">12</span>),</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    )</span></code></pre></div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
