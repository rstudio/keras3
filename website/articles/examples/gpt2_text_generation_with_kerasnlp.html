<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Use KerasNLP GPT2 model and `samplers` to do text generation.">
<title>GPT2 Text Generation with KerasNLP ‚Ä¢ keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="GPT2 Text Generation with KerasNLP">
<meta property="og:description" content="Use KerasNLP GPT2 model and `samplers` to do text generation.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>GPT2 Text Generation with KerasNLP</h1>
                        <h4 data-toc-skip class="author">Chen Qian</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/gpt2_text_generation_with_kerasnlp.Rmd" class="external-link"><code>vignettes/examples/gpt2_text_generation_with_kerasnlp.Rmd</code></a></small>
      <div class="d-none name"><code>gpt2_text_generation_with_kerasnlp.Rmd</code></div>
    </div>

    
    
<p>In this tutorial, you will learn to use <a href="https://keras.io/keras_nlp/" class="external-link">KerasNLP</a> to load a pre-trained
Large Language Model (LLM) - <a href="https://openai.com/research/better-language-models" class="external-link">GPT-2
model</a> (originally invented by OpenAI), finetune it to a specific
text style, and generate text based on users‚Äô input (also known as
prompt). You will also learn how GPT2 adapts quickly to non-English
languages, such as Chinese.</p>
<div class="section level3">
<h3 id="before-we-begin">Before we begin<a class="anchor" aria-label="anchor" href="#before-we-begin"></a>
</h3>
<p>Colab offers different kinds of runtimes. Make sure to go to
<strong>Runtime -&gt; Change runtime type</strong> and choose the GPU
Hardware Accelerator runtime (which should have &gt;12G host RAM and
~15G GPU RAM) since you will finetune the GPT-2 model. Running this
tutorial on CPU runtime will take hours.</p>
</div>
<div class="section level3">
<h3 id="install-kerasnlp-choose-backend-and-import-dependencies">Install KerasNLP, Choose Backend and Import Dependencies<a class="anchor" aria-label="anchor" href="#install-kerasnlp-choose-backend-and-import-dependencies"></a>
</h3>
<p>This examples uses <a href="https://keras.io/keras_core/" class="external-link">Keras
3.0</a> to work in any of <code>"tensorflow"</code>, <code>"jax"</code>
or <code>"torch"</code>. Support for Keras Core is baked into KerasNLP,
simply change the <code>"KERAS_BACKEND"</code> environment variable to
select the backend of your choice. We select the JAX backend below.</p>
<p>pip install git+<a href="https://github.com/keras-team/keras-nlp.git" class="external-link uri">https://github.com/keras-team/keras-nlp.git</a> -q</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras_nlp</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">import</span> time</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="introduction-to-generative-large-language-models-llms">Introduction to Generative Large Language Models (LLMs)<a class="anchor" aria-label="anchor" href="#introduction-to-generative-large-language-models-llms"></a>
</h3>
<p>Large language models (LLMs) are a type of machine learning models
that are trained on a large corpus of text data to generate outputs for
various natural language processing (NLP) tasks, such as text
generation, question answering, and machine translation.</p>
<p>Generative LLMs are typically based on deep learning neural networks,
such as the <a href="https://arxiv.org/abs/1706.03762" class="external-link">Transformer
architecture</a> invented by Google researchers in 2017, and are trained
on massive amounts of text data, often involving billions of words.
These models, such as Google <a href="https://blog.google/technology/ai/lamda/" class="external-link">LaMDA</a> and <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" class="external-link">PaLM</a>,
are trained with a large dataset from various data sources which allows
them to generate output for many tasks. The core of Generative LLMs is
predicting the next word in a sentence, often referred as <strong>Causal
LM Pretraining</strong>. In this way LLMs can generate coherent text
based on user prompts. For a more pedagogical discussion on language
models, you can refer to the <a href="https://stanford-cs324.github.io/winter2022/lectures/introduction/" class="external-link">Stanford
CS324 LLM class</a>.</p>
</div>
<div class="section level3">
<h3 id="introduction-to-kerasnlp">Introduction to KerasNLP<a class="anchor" aria-label="anchor" href="#introduction-to-kerasnlp"></a>
</h3>
<p>Large Language Models are complex to build and expensive to train
from scratch. Luckily there are pretrained LLMs available for use right
away. <a href="https://keras.io/keras_nlp/" class="external-link">KerasNLP</a> provides a
large number of pre-trained checkpoints that allow you to experiment
with SOTA models without needing to train them yourself.</p>
<p>KerasNLP is a natural language processing library that supports users
through their entire development cycle. KerasNLP offers both pretrained
models and modularized building blocks, so developers could easily reuse
pretrained models or stack their own LLM.</p>
<p>In a nutshell, for generative LLM, KerasNLP offers:</p>
<ul>
<li>Pretrained models with <code>generate()</code> method, e.g.,
<code>keras_nlp.models.GPT2CausalLM</code> and
<code>keras_nlp.models.OPTCausalLM</code>.</li>
<li>Sampler class that implements generation algorithms such as Top-K,
Beam and contrastive search. These samplers can be used to generate text
with custom models.</li>
</ul>
</div>
<div class="section level3">
<h3 id="load-a-pre-trained-gpt-2-model-and-generate-some-text">Load a pre-trained GPT-2 model and generate some text<a class="anchor" aria-label="anchor" href="#load-a-pre-trained-gpt-2-model-and-generate-some-text"></a>
</h3>
<p>KerasNLP provides a number of pre-trained models, such as <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" class="external-link">Google
Bert</a> and <a href="https://openai.com/research/better-language-models" class="external-link">GPT-2</a>. You
can see the list of models available in the <a href="https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/models" class="external-link">KerasNLP
repository</a>.</p>
<p>It‚Äôs very easy to load the GPT-2 model as you can see below:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># To speed up training and generation, we use preprocessor of length 128</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># instead of full length 1024.</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>preprocessor <span class="op">=</span> keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    <span class="st">"gpt2_base_en"</span>,</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    sequence_length<span class="op">=</span><span class="dv">128</span>,</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>gpt2_lm <span class="op">=</span> keras_nlp.models.GPT2CausalLM.from_preset(</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    <span class="st">"gpt2_base_en"</span>, preprocessor<span class="op">=</span>preprocessor</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>)</span></code></pre></div>
<p>Once the model is loaded, you can use it to generate some text right
away. Run the cells below to give it a try. It‚Äôs as simple as calling a
single function <em>generate()</em>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"My trip to Yosemite was"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPT-2 output:"</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(output)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TOTAL TIME ELAPSED: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss">s"</span>)</span></code></pre></div>
<p>Try another one:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"That Italian restaurant is"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPT-2 output:"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="bu">print</span>(output)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TOTAL TIME ELAPSED: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss">s"</span>)</span></code></pre></div>
<p>Notice how much faster the second call is. This is because the
computational graph is <a href="https://www.tensorflow.org/xla" class="external-link">XLA
compiled</a> in the 1st run and re-used in the 2nd behind the
scenes.</p>
<p>The quality of the generated text looks OK, but we can improve it via
fine-tuning.</p>
</div>
<div class="section level3">
<h3 id="more-on-the-gpt-2-model-from-kerasnlp">More on the GPT-2 model from KerasNLP<a class="anchor" aria-label="anchor" href="#more-on-the-gpt-2-model-from-kerasnlp"></a>
</h3>
<p>Next up, we will actually fine-tune the model to update its
parameters, but before we do, let‚Äôs take a look at the full set of tools
we have to for working with for GPT2.</p>
<p>The code of GPT2 can be found <a href="https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/models/gpt2/" class="external-link">here</a>.
Conceptually the <code>GPT2CausalLM</code> can be hierarchically broken
down into several modules in KerasNLP, all of which have a
<em>from_preset()</em> function that loads a pretrained model:</p>
<ul>
<li>
<code>keras_nlp.models.GPT2Tokenizer</code>: The tokenizer used by
GPT2 model, which is a <a href="https://huggingface.co/course/chapter6/5?fw=pt" class="external-link">byte-pair
encoder</a>.</li>
<li>
<code>keras_nlp.models.GPT2CausalLMPreprocessor</code>: the
preprocessor used by GPT2 causal LM training. It does the tokenization
along with other preprocessing works such as creating the label and
appending the end token.</li>
<li>
<code>keras_nlp.models.GPT2Backbone</code>: the GPT2 model, which is
a stack of <code>keras_nlp.layers.TransformerDecoder</code>. This is
usually just referred as <code>GPT2</code>.</li>
<li>
<code>keras_nlp.models.GPT2CausalLM</code>: wraps
<code>GPT2Backbone</code>, it multiplies the output of
<code>GPT2Backbone</code> by embedding matrix to generate logits over
vocab tokens.</li>
</ul>
</div>
<div class="section level3">
<h3 id="finetune-on-reddit-dataset">Finetune on Reddit dataset<a class="anchor" aria-label="anchor" href="#finetune-on-reddit-dataset"></a>
</h3>
<p>Now you have the knowledge of the GPT-2 model from KerasNLP, you can
take one step further to finetune the model so that it generates text in
a specific style, short or long, strict or casual. In this tutorial, we
will use reddit dataset for example.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>reddit_ds <span class="op">=</span> tfds.load(<span class="st">"reddit_tifu"</span>, split<span class="op">=</span><span class="st">"train"</span>, as_supervised<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Let‚Äôs take a look inside sample data from the reddit TensorFlow
Dataset. There are two features:</p>
<ul>
<li>
<strong><strong>document</strong></strong>: text of the post.</li>
<li>
<strong><strong>title</strong></strong>: the title.</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="cf">for</span> document, title <span class="kw">in</span> reddit_ds:</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="bu">print</span>(document.numpy())</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    <span class="bu">print</span>(title.numpy())</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    <span class="cf">break</span></span></code></pre></div>
<p>In our case, we are performing next word prediction in a language
model, so we only need the ‚Äòdocument‚Äô feature.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>train_ds <span class="op">=</span> (</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    reddit_ds.<span class="bu">map</span>(<span class="kw">lambda</span> document, _: document)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    .batch(<span class="dv">32</span>)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    .cache()</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    .prefetch(tf.data.AUTOTUNE)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>)</span></code></pre></div>
<p>Now you can finetune the model using the familiar <em>fit()</em>
function. Note that <code>preprocessor</code> will be automatically
called inside <code>fit</code> method since <code>GPT2CausalLM</code> is
a <code>keras_nlp.models.Task</code> instance.</p>
<p>This step takes quite a bit of GPU memory and a long time if we were
to train it all the way to a fully trained state. Here we just use part
of the dataset for demo purposes.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.take(<span class="dv">500</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># Linearly decaying learning rate.</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>learning_rate <span class="op">=</span> keras.optimizers.schedules.PolynomialDecay(</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="fl">5e-5</span>,</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    decay_steps<span class="op">=</span>train_ds.cardinality() <span class="op">*</span> num_epochs,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    end_learning_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>gpt2_lm.<span class="bu">compile</span>(</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate),</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    loss<span class="op">=</span>loss,</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    weighted_metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>)</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>gpt2_lm.fit(train_ds, epochs<span class="op">=</span>num_epochs)</span></code></pre></div>
<p>After fine-tuning is finished, you can again generate text using the
same <em>generate()</em> function. This time, the text will be closer to
Reddit writing style, and the generated length will be close to our
preset length in the training set.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>start <span class="op">=</span> time.time()</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"I like basketball"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPT-2 output:"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="bu">print</span>(output)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>end <span class="op">=</span> time.time()</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TOTAL TIME ELAPSED: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss">s"</span>)</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="into-the-sampling-method">Into the Sampling Method<a class="anchor" aria-label="anchor" href="#into-the-sampling-method"></a>
</h3>
<p>In KerasNLP, we offer a few sampling methods, e.g., contrastive
search, Top-K and beam sampling. By default, our
<code>GPT2CausalLM</code> uses Top-k search, but you can choose your own
sampling method.</p>
<p>Much like optimizer and activations, there are two ways to specify
your custom sampler:</p>
<ul>
<li>Use a string identifier, such as ‚Äúgreedy‚Äù, you are using the default
configuration via this way.</li>
<li>Pass a <code>keras_nlp.samplers.Sampler</code> instance, you can use
custom configuration via this way.</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># Use a string identifier.</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>gpt2_lm.<span class="bu">compile</span>(sampler<span class="op">=</span><span class="st">"top_k"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"I like basketball"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPT-2 output:"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="bu">print</span>(output)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># Use a `Sampler` instance. `GreedySampler` tends to repeat itself,</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>greedy_sampler <span class="op">=</span> keras_nlp.samplers.GreedySampler()</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>gpt2_lm.<span class="bu">compile</span>(sampler<span class="op">=</span>greedy_sampler)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"I like basketball"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">GPT-2 output:"</span>)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="bu">print</span>(output)</span></code></pre></div>
<p>For more details on KerasNLP <code>Sampler</code> class, you can
check the code <a href="https://github.com/keras-team/keras-nlp/tree/master/keras_nlp/samplers" class="external-link">here</a>.</p>
</div>
<div class="section level3">
<h3 id="finetune-on-chinese-poem-dataset">Finetune on Chinese Poem Dataset<a class="anchor" aria-label="anchor" href="#finetune-on-chinese-poem-dataset"></a>
</h3>
<p>We can also finetune GPT2 on non-English datasets. For readers
knowing Chinese, this part illustrates how to fine-tune GPT2 on Chinese
poem dataset to teach our model to become a poet!</p>
<p>Because GPT2 uses byte-pair encoder, and the original pretraining
dataset contains some Chinese characters, we can use the original vocab
to finetune on Chinese dataset.</p>
</div>
<div class="section level2">
<h2 id="load-chinese-poetry-dataset-">Load chinese poetry dataset.<a class="anchor" aria-label="anchor" href="#load-chinese-poetry-dataset-"></a>
</h2>
<p>git clone <a href="https://github.com/chinese-poetry/chinese-poetry.git" class="external-link uri">https://github.com/chinese-poetry/chinese-poetry.git</a></p>
<p>Load text from the json file. We only use„ÄäÂÖ®ÂîêËØó„Äãfor demo
purposes.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>poem_collection <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> os.listdir(<span class="st">"chinese-poetry/ÂÖ®ÂîêËØó"</span>):</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">".json"</span> <span class="kw">not</span> <span class="kw">in</span> <span class="bu">file</span> <span class="kw">or</span> <span class="st">"poet"</span> <span class="kw">not</span> <span class="kw">in</span> <span class="bu">file</span>:</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    full_filename <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span><span class="st">'chinese-poetry/ÂÖ®ÂîêËØó'</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">file</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(full_filename, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        content <span class="op">=</span> json.load(f)</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>        poem_collection.extend(content)</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>paragraphs <span class="op">=</span> [<span class="st">""</span>.join(data[<span class="st">"paragraphs"</span>]) <span class="cf">for</span> data <span class="kw">in</span> poem_collection]</span></code></pre></div>
<p>Let‚Äôs take a look at sample data.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(paragraphs[<span class="dv">0</span>])</span></code></pre></div>
<p>Similar as Reddit example, we convert to TF dataset, and only use
partial data to train.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>train_ds <span class="op">=</span> (</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    tf.data.Dataset.from_tensor_slices(paragraphs)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    .batch(<span class="dv">16</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    .cache()</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    .prefetch(tf.data.AUTOTUNE)</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>)</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="co"># Running through the whole dataset takes long, only take `500` and run 1</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="co"># epochs for demo purposes.</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>train_ds <span class="op">=</span> train_ds.take(<span class="dv">500</span>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>learning_rate <span class="op">=</span> keras.optimizers.schedules.PolynomialDecay(</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>    <span class="fl">5e-4</span>,</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>    decay_steps<span class="op">=</span>train_ds.cardinality() <span class="op">*</span> num_epochs,</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>    end_learning_rate<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>)</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>loss <span class="op">=</span> keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>gpt2_lm.<span class="bu">compile</span>(</span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.Adam(learning_rate),</span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>    loss<span class="op">=</span>loss,</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a>    weighted_metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a>)</span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>gpt2_lm.fit(train_ds, epochs<span class="op">=</span>num_epochs)</span></code></pre></div>
<p>Let‚Äôs check the result!</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>output <span class="op">=</span> gpt2_lm.generate(<span class="st">"Êò®Â§úÈõ®ÁñèÈ£éÈ™§"</span>, max_length<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(output)</span></code></pre></div>
<p>Not bad üòÄ</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, Fran√ßois Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
