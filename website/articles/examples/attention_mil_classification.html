<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Classification using Attention-based Deep Multiple Instance Learning (MIL). • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../../bootstrap-toc.css">
<script src="../../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Classification using Attention-based Deep Multiple Instance Learning (MIL).">
<meta property="og:description" content="MIL approach to classify bags of instances and get their individual instance score.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">keras</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.13.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../../index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Tutorials
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/getting_started.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    </li>
    <li>
      <a href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    </li>
    <li>
      <a href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    </li>
    <li>
      <a href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li class="dropdown-header">Guides (New for TF 2.6)</li>
    <li>
      <a href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    </li>
    <li>
      <a href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    </li>
    <li>
      <a href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    </li>
    <li>
      <a href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    </li>
    <li>
      <a href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    </li>
    <li>
      <a href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Using Keras</li>
    <li>
      <a href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    </li>
    <li>
      <a href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    </li>
    <li>
      <a href="../../articles/functional_api.html">Functional API in Depth</a>
    </li>
    <li>
      <a href="../../articles/about_keras_models.html">About Keras Models</a>
    </li>
    <li>
      <a href="../../articles/about_keras_layers.html">About Keras Layers</a>
    </li>
    <li>
      <a href="../../articles/training_visualization.html">Training Visualization</a>
    </li>
    <li>
      <a href="../../articles/applications.html">Pre-Trained Models</a>
    </li>
    <li>
      <a href="../../articles/faq.html">Frequently Asked Questions</a>
    </li>
    <li>
      <a href="../../articles/why_use_keras.html">Why Use Keras?</a>
    </li>
    <li class="divider">
    </li>
<li class="dropdown-header">Advanced</li>
    <li>
      <a href="../../articles/eager_guide.html">Eager Execution</a>
    </li>
    <li>
      <a href="../../articles/training_callbacks.html">Training Callbacks</a>
    </li>
    <li>
      <a href="../../articles/backend.html">Keras Backend</a>
    </li>
    <li>
      <a href="../../articles/custom_layers.html">Custom Layers</a>
    </li>
    <li>
      <a href="../../articles/custom_models.html">Custom Models</a>
    </li>
    <li>
      <a href="../../articles/saving_serializing.html">Saving and serializing</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../articles/learn.html">Learn</a>
</li>
<li>
  <a href="../../articles/tools.html">Tools</a>
</li>
<li>
  <a href="../../articles/examples/index.html">Examples</a>
</li>
<li>
  <a href="../../reference/index.html">Reference</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/keras/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Classification using Attention-based Deep
Multiple Instance Learning (MIL).</h1>
                        <h4 data-toc-skip class="author"><a href="https://www.linkedin.com/in/mohamadjaber1/" class="external-link">Mohamad Jaber</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/attention_mil_classification.Rmd" class="external-link"><code>vignettes/examples/attention_mil_classification.Rmd</code></a></small>
      <div class="hidden name"><code>attention_mil_classification.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<div class="section level3">
<h3 id="what-is-multiple-instance-learning-mil">What is Multiple Instance Learning (MIL)?<a class="anchor" aria-label="anchor" href="#what-is-multiple-instance-learning-mil"></a>
</h3>
<p>Usually, with supervised learning algorithms, the learner receives
labels for a set of instances. In the case of MIL, the learner receives
labels for a set of bags, each of which contains a set of instances. The
bag is labeled positive if it contains at least one positive instance,
and negative if it does not contain any.</p>
</div>
<div class="section level3">
<h3 id="motivation">Motivation<a class="anchor" aria-label="anchor" href="#motivation"></a>
</h3>
<p>It is often assumed in image classification tasks that each image
clearly represents a class label. In medical imaging (e.g. computational
pathology, etc.) an <em>entire image</em> is represented by a single
class label (cancerous/non-cancerous) or a region of interest could be
given. However, one will be interested in knowing which patterns in the
image is actually causing it to belong to that class. In this context,
the image(s) will be divided and the subimages will form the bag of
instances.</p>
<p>Therefore, the goals are to:</p>
<ol style="list-style-type: decimal">
<li>Learn a model to predict a class label for a bag of instances.</li>
<li>Find out which instances within the bag caused a position class
label prediction.</li>
</ol>
</div>
<div class="section level3">
<h3 id="implementation">Implementation<a class="anchor" aria-label="anchor" href="#implementation"></a>
</h3>
<p>The following steps describe how the model works:</p>
<ol style="list-style-type: decimal">
<li>The feature extractor layers extract feature embeddings.</li>
<li>The embeddings are fed into the MIL attention layer to get the
attention scores. The layer is designed as permutation-invariant.</li>
<li>Input features and their corresponding attention scores are
multiplied together.</li>
<li>The resulting output is passed to a softmax function for
classification.</li>
</ol>
</div>
<div class="section level3">
<h3 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<ul>
<li>
<a href="https://arxiv.org/abs/1802.04712" class="external-link">Attention-based Deep
Multiple Instance Learning</a>.</li>
<li>Some of the attention operator code implementation was inspired from
<a href="https://github.com/utayao/Atten_Deep_MIL" class="external-link uri">https://github.com/utayao/Atten_Deep_MIL</a>.</li>
<li>Imbalanced data <a href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data" class="external-link">tutorial</a>
by TensorFlow.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>plt.style.use(<span class="st">"ggplot"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="create-dataset">Create dataset<a class="anchor" aria-label="anchor" href="#create-dataset"></a>
</h2>
<p>We will create a set of bags and assign their labels according to
their contents. If at least one positive instance is available in a bag,
the bag is considered as a positive bag. If it does not contain any
positive instance, the bag will be considered as negative.</p>
<div class="section level3">
<h3 id="configuration-parameters">Configuration parameters<a class="anchor" aria-label="anchor" href="#configuration-parameters"></a>
</h3>
<ul>
<li>
<code>POSITIVE_CLASS</code>: The desired class to be kept in the
positive bag.</li>
<li>
<code>BAG_COUNT</code>: The number of training bags.</li>
<li>
<code>VAL_BAG_COUNT</code>: The number of validation bags.</li>
<li>
<code>BAG_SIZE</code>: The number of instances in a bag.</li>
<li>
<code>PLOT_SIZE</code>: The number of bags to plot.</li>
<li>
<code>ENSEMBLE_AVG_COUNT</code>: The number of models to create and
average together. (Optional: often results in better performance - set
to 1 for single model)</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>POSITIVE_CLASS <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>BAG_COUNT <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>VAL_BAG_COUNT <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>BAG_SIZE <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>PLOT_SIZE <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>ENSEMBLE_AVG_COUNT <span class="op">=</span> <span class="dv">1</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="prepare-bags">Prepare bags<a class="anchor" aria-label="anchor" href="#prepare-bags"></a>
</h3>
<p>Since the attention operator is a permutation-invariant operator, an
instance with a positive class label is randomly placed among the
instances in the positive bag.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">def</span> create_bags(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    input_data, input_labels, positive_class, bag_count, instance_count</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>):</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    <span class="co"># Set up bags.</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    bags <span class="op">=</span> []</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    bag_labels <span class="op">=</span> []</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    <span class="co"># Normalize input data.</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    input_data <span class="op">=</span> np.divide(input_data, <span class="fl">255.0</span>)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    <span class="co"># Count positive samples.</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(bag_count):</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>        <span class="co"># Pick a fixed size random subset of samples.</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>        index <span class="op">=</span> np.random.choice(</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>            input_data.shape[<span class="dv">0</span>], instance_count, replace<span class="op">=</span><span class="va">False</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>        )</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>        instances_data <span class="op">=</span> input_data[index]</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>        instances_labels <span class="op">=</span> input_labels[index]</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>        <span class="co"># By default, all bags are labeled as 0.</span></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>        bag_label <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>        <span class="co"># Check if there is at least a positive class in the bag.</span></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>        <span class="cf">if</span> positive_class <span class="kw">in</span> instances_labels:</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a>            <span class="co"># Positive bag will be labeled as 1.</span></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>            bag_label <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>            count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>        bags.append(instances_data)</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>        bag_labels.append(np.array([bag_label]))</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Positive bags: </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Negative bags: </span><span class="sc">{</span>bag_count <span class="op">-</span> count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a>    <span class="cf">return</span> (<span class="bu">list</span>(np.swapaxes(bags, <span class="dv">0</span>, <span class="dv">1</span>)), np.array(bag_labels))</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a><span class="co"># Load the MNIST dataset.</span></span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>(x_train, y_train), (x_val, y_val) <span class="op">=</span> keras.datasets.mnist.load_data()</span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a><span class="co"># Create training data.</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>train_data, train_labels <span class="op">=</span> create_bags(</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a>    x_train, y_train, POSITIVE_CLASS, BAG_COUNT, BAG_SIZE</span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>)</span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a><span class="co"># Create validation data.</span></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a>val_data, val_labels <span class="op">=</span> create_bags(</span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a>    x_val, y_val, POSITIVE_CLASS, VAL_BAG_COUNT, BAG_SIZE</span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="create-the-model">Create the model<a class="anchor" aria-label="anchor" href="#create-the-model"></a>
</h2>
<p>We will now build the attention layer, prepare some utilities, then
build and train the entire model.</p>
<div class="section level3">
<h3 id="attention-operator-implementation">Attention operator implementation<a class="anchor" aria-label="anchor" href="#attention-operator-implementation"></a>
</h3>
<p>The output size of this layer is decided by the size of a single
bag.</p>
<p>The attention mechanism uses a weighted average of instances in a
bag, in which the sum of the weights must equal to 1 (invariant of the
bag size).</p>
<p>The weight matrices (parameters) are <strong>w</strong> and
<strong>v</strong>. To include positive and negative values, hyperbolic
tangent element-wise non-linearity is utilized.</p>
<p>A <strong>Gated attention mechanism</strong> can be used to deal with
complex relations. Another weight matrix, <strong>u</strong>, is added
to the computation. A sigmoid non-linearity is used to overcome
approximately linear behavior for <em>x</em> ∈ [−1, 1] by hyperbolic
tangent non-linearity.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">class</span> MILAttentionLayer(layers.Layer):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="co">"""Implementation of the attention-based Deep MIL layer.</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co">      weight_params_dim: Positive Integer. Dimension of the weight matrix.</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co">      kernel_initializer: Initializer for the `kernel` matrix.</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co">      kernel_regularizer: Regularizer function applied to the `kernel` matrix.</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co">      use_gated: Boolean, whether or not to use the gated mechanism.</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co">      List of 2D tensors with BAG_SIZE length.</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co">      The tensors are the attention scores after softmax with shape `(batch_size, 1)`.</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>        weight_params_dim,</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>        kernel_initializer<span class="op">=</span><span class="st">"glorot_uniform"</span>,</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>        use_gated<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>        <span class="op">**</span>kwargs,</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    ):</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>        <span class="va">self</span>.weight_params_dim <span class="op">=</span> weight_params_dim</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>        <span class="va">self</span>.use_gated <span class="op">=</span> use_gated</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>        <span class="va">self</span>.kernel_initializer <span class="op">=</span> keras.initializers.get(kernel_initializer)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>        <span class="va">self</span>.kernel_regularizer <span class="op">=</span> keras.regularizers.get(kernel_regularizer)</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>        <span class="va">self</span>.v_init <span class="op">=</span> <span class="va">self</span>.kernel_initializer</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>        <span class="va">self</span>.w_init <span class="op">=</span> <span class="va">self</span>.kernel_initializer</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>        <span class="va">self</span>.u_init <span class="op">=</span> <span class="va">self</span>.kernel_initializer</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>        <span class="va">self</span>.v_regularizer <span class="op">=</span> <span class="va">self</span>.kernel_regularizer</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>        <span class="va">self</span>.w_regularizer <span class="op">=</span> <span class="va">self</span>.kernel_regularizer</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>        <span class="va">self</span>.u_regularizer <span class="op">=</span> <span class="va">self</span>.kernel_regularizer</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>        <span class="co"># Input shape.</span></span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>        <span class="co"># List of 2D tensors with shape: (batch_size, input_dim).</span></span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>        input_dim <span class="op">=</span> input_shape[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>        <span class="va">self</span>.v_weight_params <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>            shape<span class="op">=</span>(input_dim, <span class="va">self</span>.weight_params_dim),</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>            initializer<span class="op">=</span><span class="va">self</span>.v_init,</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"v"</span>,</span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>            regularizer<span class="op">=</span><span class="va">self</span>.v_regularizer,</span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>        )</span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a></span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a>        <span class="va">self</span>.w_weight_params <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.weight_params_dim, <span class="dv">1</span>),</span>
<span id="cb4-54"><a href="#cb4-54" tabindex="-1"></a>            initializer<span class="op">=</span><span class="va">self</span>.w_init,</span>
<span id="cb4-55"><a href="#cb4-55" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"w"</span>,</span>
<span id="cb4-56"><a href="#cb4-56" tabindex="-1"></a>            regularizer<span class="op">=</span><span class="va">self</span>.w_regularizer,</span>
<span id="cb4-57"><a href="#cb4-57" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-58"><a href="#cb4-58" tabindex="-1"></a>        )</span>
<span id="cb4-59"><a href="#cb4-59" tabindex="-1"></a></span>
<span id="cb4-60"><a href="#cb4-60" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_gated:</span>
<span id="cb4-61"><a href="#cb4-61" tabindex="-1"></a>            <span class="va">self</span>.u_weight_params <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb4-62"><a href="#cb4-62" tabindex="-1"></a>                shape<span class="op">=</span>(input_dim, <span class="va">self</span>.weight_params_dim),</span>
<span id="cb4-63"><a href="#cb4-63" tabindex="-1"></a>                initializer<span class="op">=</span><span class="va">self</span>.u_init,</span>
<span id="cb4-64"><a href="#cb4-64" tabindex="-1"></a>                name<span class="op">=</span><span class="st">"u"</span>,</span>
<span id="cb4-65"><a href="#cb4-65" tabindex="-1"></a>                regularizer<span class="op">=</span><span class="va">self</span>.u_regularizer,</span>
<span id="cb4-66"><a href="#cb4-66" tabindex="-1"></a>                trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-67"><a href="#cb4-67" tabindex="-1"></a>            )</span>
<span id="cb4-68"><a href="#cb4-68" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb4-69"><a href="#cb4-69" tabindex="-1"></a>            <span class="va">self</span>.u_weight_params <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-70"><a href="#cb4-70" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" tabindex="-1"></a>        <span class="va">self</span>.input_built <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-72"><a href="#cb4-72" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb4-74"><a href="#cb4-74" tabindex="-1"></a>        <span class="co"># Assigning variables from the number of inputs.</span></span>
<span id="cb4-75"><a href="#cb4-75" tabindex="-1"></a>        instances <span class="op">=</span> [</span>
<span id="cb4-76"><a href="#cb4-76" tabindex="-1"></a>            <span class="va">self</span>.compute_attention_scores(instance) <span class="cf">for</span> instance <span class="kw">in</span> inputs</span>
<span id="cb4-77"><a href="#cb4-77" tabindex="-1"></a>        ]</span>
<span id="cb4-78"><a href="#cb4-78" tabindex="-1"></a></span>
<span id="cb4-79"><a href="#cb4-79" tabindex="-1"></a>        <span class="co"># Stack instances into a single tensor.</span></span>
<span id="cb4-80"><a href="#cb4-80" tabindex="-1"></a>        instances <span class="op">=</span> ops.stack(instances)</span>
<span id="cb4-81"><a href="#cb4-81" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" tabindex="-1"></a>        <span class="co"># Apply softmax over instances such that the output summation is equal to 1.</span></span>
<span id="cb4-83"><a href="#cb4-83" tabindex="-1"></a>        alpha <span class="op">=</span> ops.softmax(instances, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-84"><a href="#cb4-84" tabindex="-1"></a></span>
<span id="cb4-85"><a href="#cb4-85" tabindex="-1"></a>        <span class="co"># Split to recreate the same array of tensors we had as inputs.</span></span>
<span id="cb4-86"><a href="#cb4-86" tabindex="-1"></a>        <span class="cf">return</span> [alpha[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(alpha.shape[<span class="dv">0</span>])]</span>
<span id="cb4-87"><a href="#cb4-87" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" tabindex="-1"></a>    <span class="kw">def</span> compute_attention_scores(<span class="va">self</span>, instance):</span>
<span id="cb4-89"><a href="#cb4-89" tabindex="-1"></a>        <span class="co"># Reserve in-case "gated mechanism" used.</span></span>
<span id="cb4-90"><a href="#cb4-90" tabindex="-1"></a>        original_instance <span class="op">=</span> instance</span>
<span id="cb4-91"><a href="#cb4-91" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" tabindex="-1"></a>        <span class="co"># tanh(v*h_k^T)</span></span>
<span id="cb4-93"><a href="#cb4-93" tabindex="-1"></a>        instance <span class="op">=</span> ops.tanh(</span>
<span id="cb4-94"><a href="#cb4-94" tabindex="-1"></a>            ops.tensordot(instance, <span class="va">self</span>.v_weight_params, axes<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-95"><a href="#cb4-95" tabindex="-1"></a>        )</span>
<span id="cb4-96"><a href="#cb4-96" tabindex="-1"></a></span>
<span id="cb4-97"><a href="#cb4-97" tabindex="-1"></a>        <span class="co"># for learning non-linear relations efficiently.</span></span>
<span id="cb4-98"><a href="#cb4-98" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_gated:</span>
<span id="cb4-99"><a href="#cb4-99" tabindex="-1"></a>            instance <span class="op">=</span> instance <span class="op">*</span> ops.sigmoid(</span>
<span id="cb4-100"><a href="#cb4-100" tabindex="-1"></a>                ops.tensordot(original_instance, <span class="va">self</span>.u_weight_params, axes<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-101"><a href="#cb4-101" tabindex="-1"></a>            )</span>
<span id="cb4-102"><a href="#cb4-102" tabindex="-1"></a></span>
<span id="cb4-103"><a href="#cb4-103" tabindex="-1"></a>        <span class="co"># w^T*(tanh(v*h_k^T)) / w^T*(tanh(v*h_k^T)*sigmoid(u*h_k^T))</span></span>
<span id="cb4-104"><a href="#cb4-104" tabindex="-1"></a>        <span class="cf">return</span> ops.tensordot(instance, <span class="va">self</span>.w_weight_params, axes<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="visualizer-tool">Visualizer tool<a class="anchor" aria-label="anchor" href="#visualizer-tool"></a>
</h2>
<p>Plot the number of bags (given by <code>PLOT_SIZE</code>) with
respect to the class.</p>
<p>Moreover, if activated, the class label prediction with its
associated instance score for each bag (after the model has been
trained) can be seen.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">def</span> plot(data, labels, bag_class, predictions<span class="op">=</span><span class="va">None</span>, attention_weights<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="co">""" "Utility for plotting bags and attention weights.</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co">      data: Input data that contains the bags of instances.</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co">      labels: The associated bag labels of the input data.</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co">      bag_class: String name of the desired bag class.</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co">        The options are: "positive" or "negative".</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co">      predictions: Class labels model predictions.</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="co">      If you don't specify anything, ground truth labels will be used.</span></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co">      attention_weights: Attention weights for each instance within the input data.</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="co">      If you don't specify anything, the values won't be displayed.</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    <span class="cf">return</span>  <span class="co">## </span><span class="al">TODO</span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    labels <span class="op">=</span> np.array(labels).reshape(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>    <span class="cf">if</span> bag_class <span class="op">==</span> <span class="st">"positive"</span>:</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        <span class="cf">if</span> predictions <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>            labels <span class="op">=</span> np.where(predictions.argmax(<span class="dv">1</span>) <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>            bags <span class="op">=</span> np.array(data)[:, labels[<span class="dv">0</span>:PLOT_SIZE]]</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>            labels <span class="op">=</span> np.where(labels <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>            bags <span class="op">=</span> np.array(data)[:, labels[<span class="dv">0</span>:PLOT_SIZE]]</span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a>    <span class="cf">elif</span> bag_class <span class="op">==</span> <span class="st">"negative"</span>:</span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a>        <span class="cf">if</span> predictions <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>            labels <span class="op">=</span> np.where(predictions.argmax(<span class="dv">1</span>) <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>            bags <span class="op">=</span> np.array(data)[:, labels[<span class="dv">0</span>:PLOT_SIZE]]</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a>            labels <span class="op">=</span> np.where(labels <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a>            bags <span class="op">=</span> np.array(data)[:, labels[<span class="dv">0</span>:PLOT_SIZE]]</span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"There is no class </span><span class="sc">{</span>bag_class<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"The bag class label is </span><span class="sc">{</span>bag_class<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(PLOT_SIZE):</span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>        figure <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb5-41"><a href="#cb5-41" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Bag number: </span><span class="sc">{</span>labels[i]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-42"><a href="#cb5-42" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(BAG_SIZE):</span>
<span id="cb5-43"><a href="#cb5-43" tabindex="-1"></a>            image <span class="op">=</span> bags[j][i]</span>
<span id="cb5-44"><a href="#cb5-44" tabindex="-1"></a>            figure.add_subplot(<span class="dv">1</span>, BAG_SIZE, j <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-45"><a href="#cb5-45" tabindex="-1"></a>            plt.grid(<span class="va">False</span>)</span>
<span id="cb5-46"><a href="#cb5-46" tabindex="-1"></a>            <span class="cf">if</span> attention_weights <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb5-47"><a href="#cb5-47" tabindex="-1"></a>                plt.title(np.around(attention_weights[labels[i]][j], <span class="dv">2</span>))</span>
<span id="cb5-48"><a href="#cb5-48" tabindex="-1"></a>            plt.imshow(image)</span>
<span id="cb5-49"><a href="#cb5-49" tabindex="-1"></a>        plt.show()</span>
<span id="cb5-50"><a href="#cb5-50" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" tabindex="-1"></a><span class="co"># Plot some of validation data bags per class.</span></span>
<span id="cb5-53"><a href="#cb5-53" tabindex="-1"></a>plot(val_data, val_labels, <span class="st">"positive"</span>)</span>
<span id="cb5-54"><a href="#cb5-54" tabindex="-1"></a>plot(val_data, val_labels, <span class="st">"negative"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="create-model">Create model<a class="anchor" aria-label="anchor" href="#create-model"></a>
</h2>
<p>First we will create some embeddings per instance, invoke the
attention operator and then use the softmax function to output the class
probabilities.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> create_model(instance_shape):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="co"># Extract features from inputs.</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    inputs, embeddings <span class="op">=</span> [], []</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    shared_dense_layer_1 <span class="op">=</span> layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    shared_dense_layer_2 <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">"relu"</span>)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(BAG_SIZE):</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>        inp <span class="op">=</span> layers.Input(instance_shape)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        flatten <span class="op">=</span> layers.Flatten()(inp)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>        dense_1 <span class="op">=</span> shared_dense_layer_1(flatten)</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>        dense_2 <span class="op">=</span> shared_dense_layer_2(dense_1)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>        inputs.append(inp)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>        embeddings.append(dense_2)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    <span class="co"># Invoke the attention layer.</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    alpha <span class="op">=</span> MILAttentionLayer(</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        weight_params_dim<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>        kernel_regularizer<span class="op">=</span>keras.regularizers.L2(<span class="fl">0.01</span>),</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>        use_gated<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>        name<span class="op">=</span><span class="st">"alpha"</span>,</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    )(embeddings)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    <span class="co"># Multiply attention weights with the input layers.</span></span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>    multiply_layers <span class="op">=</span> [</span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>        layers.multiply([alpha[i], embeddings[i]]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(alpha))</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>    ]</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>    <span class="co"># Concatenate layers.</span></span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>    concat <span class="op">=</span> layers.concatenate(multiply_layers, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>    <span class="co"># Classification output node.</span></span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>    output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, activation<span class="op">=</span><span class="st">"softmax"</span>)(concat)</span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>    <span class="cf">return</span> keras.Model(inputs, output)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="class-weights">Class weights<a class="anchor" aria-label="anchor" href="#class-weights"></a>
</h2>
<p>Since this kind of problem could simply turn into imbalanced data
classification problem, class weighting should be considered.</p>
<p>Let’s say there are 1000 bags. There often could be cases were ~90 %
of the bags do not contain any positive label and ~10 % do. Such data
can be referred to as <strong>Imbalanced data</strong>.</p>
<p>Using class weights, the model will tend to give a higher weight to
the rare class.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> compute_class_weights(labels):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="co"># Count number of postive and negative bags.</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    negative_count <span class="op">=</span> <span class="bu">len</span>(np.where(labels <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>])</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    positive_count <span class="op">=</span> <span class="bu">len</span>(np.where(labels <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>])</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    total_count <span class="op">=</span> negative_count <span class="op">+</span> positive_count</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>    <span class="co"># Build class weight dictionary.</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="dv">0</span>: (<span class="dv">1</span> <span class="op">/</span> negative_count) <span class="op">*</span> (total_count <span class="op">/</span> <span class="dv">2</span>),</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>        <span class="dv">1</span>: (<span class="dv">1</span> <span class="op">/</span> positive_count) <span class="op">*</span> (total_count <span class="op">/</span> <span class="dv">2</span>),</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    }</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="build-and-train-model">Build and train model<a class="anchor" aria-label="anchor" href="#build-and-train-model"></a>
</h2>
<p>The model is built and trained in this section.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> train(train_data, train_labels, val_data, val_labels, model):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="co"># Train model.</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="co"># Prepare callbacks.</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="co"># Path where to save best weights.</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    <span class="co"># Take the file name from the wrapper.</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    file_path <span class="op">=</span> <span class="st">"/tmp/best_model.weights.h5"</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    <span class="co"># Initialize model checkpoint callback.</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    model_checkpoint <span class="op">=</span> keras.callbacks.ModelCheckpoint(</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>        file_path,</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>,</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"min"</span>,</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>        save_best_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>        save_weights_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    )</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    <span class="co"># Initialize early stopping callback.</span></span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    <span class="co"># The model performance is monitored across the validation data and stops training</span></span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>    <span class="co"># when the generalization error cease to decrease.</span></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    early_stopping <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>        monitor<span class="op">=</span><span class="st">"val_loss"</span>, patience<span class="op">=</span><span class="dv">10</span>, mode<span class="op">=</span><span class="st">"min"</span></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>    )</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>    <span class="co"># Compile model.</span></span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"sparse_categorical_crossentropy"</span>,</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">"accuracy"</span>],</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>    )</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>    <span class="co"># Fit model.</span></span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a>    model.fit(</span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>        train_data,</span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a>        train_labels,</span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a>        validation_data<span class="op">=</span>(val_data, val_labels),</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>        class_weight<span class="op">=</span>compute_class_weights(train_labels),</span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>        batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a>        callbacks<span class="op">=</span>[early_stopping, model_checkpoint],</span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a>    )</span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>    <span class="co"># Load best weights.</span></span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>    model.load_weights(file_path)</span>
<span id="cb8-47"><a href="#cb8-47" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb8-49"><a href="#cb8-49" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" tabindex="-1"></a><span class="co"># Building model(s).</span></span>
<span id="cb8-52"><a href="#cb8-52" tabindex="-1"></a>instance_shape <span class="op">=</span> train_data[<span class="dv">0</span>][<span class="dv">0</span>].shape</span>
<span id="cb8-53"><a href="#cb8-53" tabindex="-1"></a>models <span class="op">=</span> [create_model(instance_shape) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(ENSEMBLE_AVG_COUNT)]</span>
<span id="cb8-54"><a href="#cb8-54" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" tabindex="-1"></a><span class="co"># Show single model architecture.</span></span>
<span id="cb8-56"><a href="#cb8-56" tabindex="-1"></a><span class="bu">print</span>(models[<span class="dv">0</span>].summary())</span>
<span id="cb8-57"><a href="#cb8-57" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" tabindex="-1"></a><span class="co"># Training model(s).</span></span>
<span id="cb8-59"><a href="#cb8-59" tabindex="-1"></a>trained_models <span class="op">=</span> [</span>
<span id="cb8-60"><a href="#cb8-60" tabindex="-1"></a>    train(train_data, train_labels, val_data, val_labels, model)</span>
<span id="cb8-61"><a href="#cb8-61" tabindex="-1"></a>    <span class="cf">for</span> model <span class="kw">in</span> tqdm(models)</span>
<span id="cb8-62"><a href="#cb8-62" tabindex="-1"></a>]</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<p>The models are now ready for evaluation. With each model we also
create an associated intermediate model to get the weights from the
attention layer.</p>
<p>We will compute a prediction for each of our
<code>ENSEMBLE_AVG_COUNT</code> models, and average them together for
our final prediction.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> predict(data, labels, trained_models):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    <span class="co"># Collect info per model.</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    models_predictions <span class="op">=</span> []</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    models_attention_weights <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    models_losses <span class="op">=</span> []</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    models_accuracies <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    <span class="cf">for</span> model <span class="kw">in</span> trained_models:</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>        <span class="co"># Predict output classes on data.</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>        predictions <span class="op">=</span> model.predict(data)</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        models_predictions.append(predictions)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>        <span class="co"># Create intermediate model to get MIL attention layer weights.</span></span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        intermediate_model <span class="op">=</span> keras.Model(</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>            model.<span class="bu">input</span>, model.get_layer(<span class="st">"alpha"</span>).output</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>        )</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>        <span class="co"># Predict MIL attention layer weights.</span></span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>        intermediate_predictions <span class="op">=</span> intermediate_model.predict(data)</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a>        attention_weights <span class="op">=</span> np.squeeze(</span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>            np.swapaxes(intermediate_predictions, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>        )</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>        models_attention_weights.append(attention_weights)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" tabindex="-1"></a>        loss, accuracy <span class="op">=</span> model.evaluate(data, labels, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-27"><a href="#cb9-27" tabindex="-1"></a>        models_losses.append(loss)</span>
<span id="cb9-28"><a href="#cb9-28" tabindex="-1"></a>        models_accuracies.append(accuracy)</span>
<span id="cb9-29"><a href="#cb9-29" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb9-31"><a href="#cb9-31" tabindex="-1"></a>        <span class="ss">f"The average loss and accuracy are </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(models_losses, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> ENSEMBLE_AVG_COUNT<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb9-32"><a href="#cb9-32" tabindex="-1"></a>        <span class="ss">f" and </span><span class="sc">{</span><span class="dv">100</span> <span class="op">*</span> np<span class="sc">.</span><span class="bu">sum</span>(models_accuracies, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> ENSEMBLE_AVG_COUNT<span class="sc">:.2f}</span><span class="ss"> % resp."</span></span>
<span id="cb9-33"><a href="#cb9-33" tabindex="-1"></a>    )</span>
<span id="cb9-34"><a href="#cb9-34" tabindex="-1"></a></span>
<span id="cb9-35"><a href="#cb9-35" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb9-36"><a href="#cb9-36" tabindex="-1"></a>        np.<span class="bu">sum</span>(models_predictions, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> ENSEMBLE_AVG_COUNT,</span>
<span id="cb9-37"><a href="#cb9-37" tabindex="-1"></a>        np.<span class="bu">sum</span>(models_attention_weights, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">/</span> ENSEMBLE_AVG_COUNT,</span>
<span id="cb9-38"><a href="#cb9-38" tabindex="-1"></a>    )</span>
<span id="cb9-39"><a href="#cb9-39" tabindex="-1"></a></span>
<span id="cb9-40"><a href="#cb9-40" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" tabindex="-1"></a><span class="co"># Evaluate and predict classes and attention scores on validation data.</span></span>
<span id="cb9-42"><a href="#cb9-42" tabindex="-1"></a>class_predictions, attention_params <span class="op">=</span> predict(</span>
<span id="cb9-43"><a href="#cb9-43" tabindex="-1"></a>    val_data, val_labels, trained_models</span>
<span id="cb9-44"><a href="#cb9-44" tabindex="-1"></a>)</span>
<span id="cb9-45"><a href="#cb9-45" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" tabindex="-1"></a><span class="co"># Plot some results from our validation data.</span></span>
<span id="cb9-47"><a href="#cb9-47" tabindex="-1"></a>plot(</span>
<span id="cb9-48"><a href="#cb9-48" tabindex="-1"></a>    val_data,</span>
<span id="cb9-49"><a href="#cb9-49" tabindex="-1"></a>    val_labels,</span>
<span id="cb9-50"><a href="#cb9-50" tabindex="-1"></a>    <span class="st">"positive"</span>,</span>
<span id="cb9-51"><a href="#cb9-51" tabindex="-1"></a>    predictions<span class="op">=</span>class_predictions,</span>
<span id="cb9-52"><a href="#cb9-52" tabindex="-1"></a>    attention_weights<span class="op">=</span>attention_params,</span>
<span id="cb9-53"><a href="#cb9-53" tabindex="-1"></a>)</span>
<span id="cb9-54"><a href="#cb9-54" tabindex="-1"></a>plot(</span>
<span id="cb9-55"><a href="#cb9-55" tabindex="-1"></a>    val_data,</span>
<span id="cb9-56"><a href="#cb9-56" tabindex="-1"></a>    val_labels,</span>
<span id="cb9-57"><a href="#cb9-57" tabindex="-1"></a>    <span class="st">"negative"</span>,</span>
<span id="cb9-58"><a href="#cb9-58" tabindex="-1"></a>    predictions<span class="op">=</span>class_predictions,</span>
<span id="cb9-59"><a href="#cb9-59" tabindex="-1"></a>    attention_weights<span class="op">=</span>attention_params,</span>
<span id="cb9-60"><a href="#cb9-60" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>From the above plot, you can notice that the weights always sum to 1.
In a positively predict bag, the instance which resulted in the positive
labeling will have a substantially higher attention score than the rest
of the bag. However, in a negatively predicted bag, there are two
cases:</p>
<ul>
<li>All instances will have approximately similar scores.</li>
<li>An instance will have relatively higher score (but not as high as of
a positive instance). This is because the feature space of this instance
is close to that of the positive instance.</li>
</ul>
</div>
<div class="section level2">
<h2 id="remarks">Remarks<a class="anchor" aria-label="anchor" href="#remarks"></a>
</h2>
<ul>
<li>If the model is overfit, the weights will be equally distributed for
all bags. Hence, the regularization techniques are necessary.</li>
<li>In the paper, the bag sizes can differ from one bag to another. For
simplicity, the bag sizes are fixed here.</li>
<li>In order not to rely on the random initial weights of a single
model, averaging ensemble methods should be considered.</li>
</ul>
<p>Example available on HuggingFace.</p>
<table class="table">
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead><tr class="header">
<th align="center">Trained Model</th>
<th align="center">Demo</th>
</tr></thead>
<tbody><tr class="odd">
<td align="center"><a href="https://huggingface.co/keras-io/attention_mil" class="external-link"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Model-Attention%20MIL-black.svg" alt="Generic badge"></a></td>
<td align="center"><a href="https://huggingface.co/spaces/keras-io/Attention_based_Deep_Multiple_Instance_Learning" class="external-link"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Attention%20MIL-black.svg" alt="Generic badge"></a></td>
</tr></tbody>
</table>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
