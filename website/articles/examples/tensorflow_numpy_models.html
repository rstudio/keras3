<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Overview of how to use the TensorFlow NumPy API to write Keras models.">
<title>Writing Keras Models With TensorFlow NumPy • keras</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><link href="../../extra.css" rel="stylesheet">
<script src="../../extra.js"></script><meta property="og:title" content="Writing Keras Models With TensorFlow NumPy">
<meta property="og:description" content="Overview of how to use the TensorFlow NumPy API to write Keras models.">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-tutorials">Tutorials</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-tutorials">
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <a class="dropdown-item" href="../../writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <h6 class="dropdown-header" data-toc-skip>Guides (New for TF 2.6)</h6>
    <a class="dropdown-item" href="../../articles/new-guides/python_subclasses.html">Python Subclasses</a>
    <a class="dropdown-item" href="../../articles/new-guides/making_new_layers_and_models_via_subclassing.html">Making New Layers and Models via Subclassing</a>
    <a class="dropdown-item" href="../../articles/new-guides/customizing_what_happens_in_fit.html">Customizing What Happens in Fit</a>
    <a class="dropdown-item" href="../../articles/new-guides/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <a class="dropdown-item" href="../../articles/new-guides/preprocessing_layers.html">Working with Preprocessing Layers</a>
    <a class="dropdown-item" href="../../argicles/new-guides/working_with_rnns.html">Working with RNNs</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Using Keras</h6>
    <a class="dropdown-item" href="../../articles/guide_keras.html">Guide to Keras Basics</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model in Depth</a>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API in Depth</a>
    <a class="dropdown-item" href="../../articles/about_keras_models.html">About Keras Models</a>
    <a class="dropdown-item" href="../../articles/about_keras_layers.html">About Keras Layers</a>
    <a class="dropdown-item" href="../../articles/training_visualization.html">Training Visualization</a>
    <a class="dropdown-item" href="../../articles/applications.html">Pre-Trained Models</a>
    <a class="dropdown-item" href="../../articles/faq.html">Frequently Asked Questions</a>
    <a class="dropdown-item" href="../../articles/why_use_keras.html">Why Use Keras?</a>
    <div class="dropdown-divider"></div>
    <h6 class="dropdown-header" data-toc-skip>Advanced</h6>
    <a class="dropdown-item" href="../../articles/eager_guide.html">Eager Execution</a>
    <a class="dropdown-item" href="../../articles/training_callbacks.html">Training Callbacks</a>
    <a class="dropdown-item" href="../../articles/backend.html">Keras Backend</a>
    <a class="dropdown-item" href="../../articles/custom_layers.html">Custom Layers</a>
    <a class="dropdown-item" href="../../articles/custom_models.html">Custom Models</a>
    <a class="dropdown-item" href="../../articles/saving_serializing.html">Saving and serializing</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/learn.html">Learn</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../articles/tools.html">Tools</a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../../logo.png" class="logo" alt=""><h1>Writing Keras Models With TensorFlow NumPy</h1>
                        <h4 data-toc-skip class="author"><a href="https://lukewood.xyz" class="external-link">lukewood</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/tensorflow_numpy_models.Rmd" class="external-link"><code>vignettes/examples/tensorflow_numpy_models.Rmd</code></a></small>
      <div class="d-none name"><code>tensorflow_numpy_models.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p><a href="https://numpy.org/" class="external-link">NumPy</a> is a hugely successful Python
linear algebra library.</p>
<p>TensorFlow recently launched <a href="https://www.tensorflow.org/guide/tf_numpy" class="external-link">tf_numpy</a>, a
TensorFlow implementation of a large subset of the NumPy API. Thanks to
<code>tf_numpy</code>, you can write Keras layers or models in the NumPy
style!</p>
<p>The TensorFlow NumPy API has full integration with the TensorFlow
ecosystem. Features such as automatic differentiation, TensorBoard,
Keras model callbacks, TPU distribution and model exporting are all
supported.</p>
<p>Let’s run through a few examples.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<p>TensorFlow NumPy requires TensorFlow 2.5 or later.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> tensorflow.experimental.numpy <span class="im">as</span> tnp</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span></code></pre></div>
<p>Optionally, you can call
<code>tnp.experimental_enable_numpy_behavior()</code> to enable type
promotion in TensorFlow. This allows TNP to more closely follow the
NumPy standard.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>tnp.experimental_enable_numpy_behavior()</span></code></pre></div>
<p>To test our models we will use the Boston housing prices regression
dataset.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.boston_housing.load_data(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    path<span class="op">=</span><span class="st">"boston_housing.npz"</span>, test_split<span class="op">=</span><span class="fl">0.2</span>, seed<span class="op">=</span><span class="dv">113</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>input_dim <span class="op">=</span> x_train.shape[<span class="dv">1</span>]</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="kw">def</span> evaluate_model(model: keras.Model):</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    [loss, percent_error] <span class="op">=</span> model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Mean absolute percent error before training: "</span>, percent_error)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    model.fit(x_train, y_train, epochs<span class="op">=</span><span class="dv">200</span>, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>    [loss, percent_error] <span class="op">=</span> model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Mean absolute percent error after training:"</span>, percent_error)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="subclassing-keras-model-with-tnp">Subclassing keras.Model with TNP<a class="anchor" aria-label="anchor" href="#subclassing-keras-model-with-tnp"></a>
</h2>
<p>The most flexible way to make use of the Keras API is to subclass the
<a href="https://keras.io/api/models/model/" class="external-link"><code>keras.Model</code></a>
class. Subclassing the Model class gives you the ability to fully
customize what occurs in the training loop. This makes subclassing Model
a popular option for researchers.</p>
<p>In this example, we will implement a <code>Model</code> subclass that
performs regression over the boston housing dataset using the TNP API.
Note that differentiation and gradient descent is handled automatically
when using the TNP API alongside keras.</p>
<p>First let’s define a simple
<code>TNPForwardFeedRegressionNetwork</code> class.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">class</span> TNPForwardFeedRegressionNetwork(keras.Model):</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, blocks<span class="op">=</span><span class="va">None</span>, <span class="op">**</span>kwargs):</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(blocks, <span class="bu">list</span>):</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"blocks must be a list, got blocks=</span><span class="sc">{</span>blocks<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        <span class="va">self</span>.blocks <span class="op">=</span> blocks</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        <span class="va">self</span>.block_weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>        current_shape <span class="op">=</span> input_shape[<span class="dv">1</span>]</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        <span class="va">self</span>.block_weights <span class="op">=</span> []</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> []</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        <span class="cf">for</span> i, block <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.blocks):</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>            <span class="va">self</span>.block_weights.append(</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>                <span class="va">self</span>.add_weight(</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>                    shape<span class="op">=</span>(current_shape, block),</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>                    trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>                    name<span class="op">=</span><span class="ss">f"block-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>                    initializer<span class="op">=</span><span class="st">"glorot_normal"</span>,</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>                )</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>            )</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>            <span class="va">self</span>.biases.append(</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>                <span class="va">self</span>.add_weight(</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>                    shape<span class="op">=</span>(block,),</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>                    trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>                    name<span class="op">=</span><span class="ss">f"bias-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>                    initializer<span class="op">=</span><span class="st">"zeros"</span>,</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>                )</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>            )</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>            current_shape <span class="op">=</span> block</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>        <span class="va">self</span>.linear_layer <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>            shape<span class="op">=</span>(current_shape, <span class="dv">1</span>),</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"linear_projector"</span>,</span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"glorot_normal"</span>,</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>        )</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>        activations <span class="op">=</span> inputs</span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>        <span class="cf">for</span> w, b <span class="kw">in</span> <span class="bu">zip</span>(<span class="va">self</span>.block_weights, <span class="va">self</span>.biases):</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>            activations <span class="op">=</span> tnp.matmul(activations, w) <span class="op">+</span> b</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>            <span class="co"># ReLu activation function</span></span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>            activations <span class="op">=</span> tnp.maximum(activations, <span class="fl">0.0</span>)</span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a></span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a>        <span class="cf">return</span> tnp.matmul(activations, <span class="va">self</span>.linear_layer)</span></code></pre></div>
<p>Just like with any other Keras model we can utilize any supported
optimizer, loss, metrics or callbacks that we want.</p>
<p>Let’s see how the model performs!</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>model <span class="op">=</span> TNPForwardFeedRegressionNetwork(blocks<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"mean_squared_error"</span>,</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>evaluate_model(model)</span></code></pre></div>
<p>Great! Our model seems to be effectively learning to solve the
problem at hand.</p>
<p>We can also write our own custom loss function using TNP.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> tnp_mse(y_true, y_pred):</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="cf">return</span> tnp.mean(tnp.square(y_true <span class="op">-</span> y_pred), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>keras.backend.clear_session()</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>model <span class="op">=</span> TNPForwardFeedRegressionNetwork(blocks<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    loss<span class="op">=</span>tnp_mse,</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>evaluate_model(model)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implementing-a-keras-layer-based-model-with-tnp">Implementing a Keras Layer Based Model with TNP<a class="anchor" aria-label="anchor" href="#implementing-a-keras-layer-based-model-with-tnp"></a>
</h2>
<p>If desired, TNP can also be used in layer oriented Keras code
structure. Let’s implement the same model, but using a layered
approach!</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> tnp_relu(x):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    <span class="cf">return</span> tnp.maximum(x, <span class="dv">0</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="kw">class</span> TNPDense(keras.layers.Layer):</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, units, activation<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>        <span class="va">self</span>.units <span class="op">=</span> units</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>        <span class="va">self</span>.activation <span class="op">=</span> activation</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>    <span class="kw">def</span> build(<span class="va">self</span>, input_shape):</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"weights"</span>,</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>            shape<span class="op">=</span>(input_shape[<span class="dv">1</span>], <span class="va">self</span>.units),</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"random_normal"</span>,</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>        )</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="va">self</span>.add_weight(</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"bias"</span>,</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>            shape<span class="op">=</span>(<span class="va">self</span>.units,),</span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a>            initializer<span class="op">=</span><span class="st">"zeros"</span>,</span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>            trainable<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a>        )</span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, inputs):</span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a>        outputs <span class="op">=</span> tnp.matmul(inputs, <span class="va">self</span>.w) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.activation:</span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.activation(outputs)</span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a>        <span class="cf">return</span> outputs</span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="kw">def</span> create_layered_tnp_model():</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a>    <span class="cf">return</span> keras.Sequential(</span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a>        [</span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>            TNPDense(<span class="dv">3</span>, activation<span class="op">=</span>tnp_relu),</span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>            TNPDense(<span class="dv">3</span>, activation<span class="op">=</span>tnp_relu),</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>            TNPDense(<span class="dv">1</span>),</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a>        ]</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>    )</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>model <span class="op">=</span> create_layered_tnp_model()</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"mean_squared_error"</span>,</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>)</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>model.build((<span class="va">None</span>, input_dim))</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a>model.summary()</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a>evaluate_model(model)</span></code></pre></div>
<p>You can also seamlessly switch between TNP layers and native Keras
layers!</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> create_mixed_model():</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="cf">return</span> keras.Sequential(</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        [</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>            TNPDense(<span class="dv">3</span>, activation<span class="op">=</span>tnp_relu),</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>            <span class="co"># The model will have no issue using a normal Dense layer</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>            layers.Dense(<span class="dv">3</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>            <span class="co"># ... or switching back to tnp layers!</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>            TNPDense(<span class="dv">1</span>),</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>        ]</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    )</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>model <span class="op">=</span> create_mixed_model()</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">"mean_squared_error"</span>,</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>    metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>)</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>model.build((<span class="va">None</span>, input_dim))</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>model.summary()</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>evaluate_model(model)</span></code></pre></div>
<p>The Keras API offers a wide variety of layers. The ability to use
them alongside NumPy code can be a huge time saver in projects.</p>
</div>
<div class="section level2">
<h2 id="distribution-strategy">Distribution Strategy<a class="anchor" aria-label="anchor" href="#distribution-strategy"></a>
</h2>
<p>TensorFlow NumPy and Keras integrate with <a href="https://www.tensorflow.org/guide/distributed_training" class="external-link">TensorFlow
Distribution Strategies</a>. This makes it simple to perform distributed
training across multiple GPUs, or even an entire TPU Pod.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>gpus <span class="op">=</span> tf.config.list_logical_devices(<span class="st">"GPU"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="cf">if</span> gpus:</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    strategy <span class="op">=</span> tf.distribute.MirroredStrategy(gpus)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    <span class="co"># We can fallback to a no-op CPU strategy.</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    strategy <span class="op">=</span> tf.distribute.get_strategy()</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Running with strategy:"</span>, <span class="bu">str</span>(strategy.__class__.<span class="va">__name__</span>))</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    model <span class="op">=</span> create_layered_tnp_model()</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"mean_squared_error"</span>,</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>        metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>    )</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>    model.build((<span class="va">None</span>, input_dim))</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>    model.summary()</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>    evaluate_model(model)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="tensorboard-integration">TensorBoard Integration<a class="anchor" aria-label="anchor" href="#tensorboard-integration"></a>
</h2>
<p>One of the many benefits of using the Keras API is the ability to
monitor training through TensorBoard. Using the TensorFlow NumPy API
alongside Keras allows you to easily leverage TensorBoard.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>keras.backend.clear_session()</span></code></pre></div>
<p>To load the TensorBoard from a Jupyter notebook, you can run the
following magic:</p>
<pre><code>%load_ext tensorboard</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>models <span class="op">=</span> [</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>    (</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>        TNPForwardFeedRegressionNetwork(blocks<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">3</span>]),</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>        <span class="st">"TNPForwardFeedRegressionNetwork"</span>,</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    ),</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    (create_layered_tnp_model(), <span class="st">"layered_tnp_model"</span>),</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>    (create_mixed_model(), <span class="st">"mixed_model"</span>),</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>]</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="cf">for</span> model, model_name <span class="kw">in</span> models:</span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">"adam"</span>,</span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">"mean_squared_error"</span>,</span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a>        metrics<span class="op">=</span>[keras.metrics.MeanAbsolutePercentageError()],</span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a>    )</span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a>    model.fit(</span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a>        x_train,</span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a>        y_train,</span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a>        epochs<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a>        verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a>        callbacks<span class="op">=</span>[keras.callbacks.TensorBoard(log_dir<span class="op">=</span><span class="ss">f"logs/</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">"</span>)],</span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a>    )</span></code></pre></div>
<p>To load the TensorBoard from a Jupyter notebook you can use the
<code>%tensorboard</code> magic:</p>
<pre><code>%tensorboard --logdir logs</code></pre>
<p>The TensorBoard monitor metrics and examine the training curve.</p>
<div class="float">
<img src="https://i.imgur.com/wsOuFnz.png" alt="Tensorboard training graph"><div class="figcaption">Tensorboard training graph</div>
</div>
<p>The TensorBoard also allows you to explore the computation graph used
in your models.</p>
<div class="float">
<img src="https://i.imgur.com/tOrezDL.png" alt="Tensorboard graph exploration"><div class="figcaption">Tensorboard graph exploration</div>
</div>
<p>The ability to introspect into your models can be valuable during
debugging.</p>
</div>
<div class="section level2">
<h2 id="conclusion">Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>Porting existing NumPy code to Keras models using the
<code>tensorflow_numpy</code> API is easy! By integrating with Keras you
gain the ability to use existing Keras callbacks, metrics and
optimizers, easily distribute your training and use Tensorboard.</p>
<p>Migrating a more complex model, such as a ResNet, to the TensorFlow
NumPy API would be a great follow up learning exercise.</p>
<p>Several open source NumPy ResNet implementations are available
online.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, RStudio, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
