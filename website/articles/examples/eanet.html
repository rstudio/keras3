<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Image classification with a Transformer that leverages external attention.">
<title>Image classification with EANet (External Attention Transformer) • keras3</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../../apple-touch-icon-60x60.png">
<script src="../../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../../deps/Fira_Mono-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../../pkgdown.js"></script><meta property="og:title" content="Image classification with EANet (External Attention Transformer)">
<meta property="og:description" content="Image classification with a Transformer that leverages external attention.">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-inverse navbar-expand-lg bg-primary"><div class="container">
    
    <a class="navbar-brand me-2" href="../../index.html">keras3</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2.13.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../../index.html">Home</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-getting-started">Getting Started</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-getting-started">
    <a class="dropdown-item" href="../../articles/intro_to_keras_for_engineers.html">Introduction to Keras for engineers</a>
    <h6 class="dropdown-header" data-toc-skip>Tutorials</h6>
    <a class="dropdown-item" href="../../articles/getting_started.html">Getting Started</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_classification.html">Basic Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_text_classification.html">Text Classification</a>
    <a class="dropdown-item" href="../../articles/tutorial_basic_regression.html">Basic Regression</a>
    <a class="dropdown-item" href="../../articles/tutorial_overfit_underfit.html">Overfitting and Underfitting</a>
    <a class="dropdown-item" href="../../articles/tutorial_save_and_restore.html">Save and Restore Models</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-guides">Guides</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-guides">
    <h6 class="dropdown-header" data-toc-skip>Model definition</h6>
    <a class="dropdown-item" href="../../articles/functional_api.html">Functional API</a>
    <a class="dropdown-item" href="../../articles/sequential_model.html">Sequential Model</a>
    <h6 class="dropdown-header" data-toc-skip>Extending and customizing</h6>
    <a class="dropdown-item" href="../../articles/making_new_layers_and_models_via_subclassing.html">Making new layers and models via subclassing</a>
    <a class="dropdown-item" href="../../articles/training_with_built_in_methods.html">Training &amp; evaluation with the built-in methods</a>
    <a class="dropdown-item" href="../../articles/writing_a_custom_training_loop_in_tensorflow.html">Writing a training loop from scratch in TensorFlow</a>
    <a class="dropdown-item" href="../../articles/writing_your_own_callbacks.html">Writing Your Own Callbacks</a>
    <h6 class="dropdown-header" data-toc-skip>Other topics</h6>
    <a class="dropdown-item" href="../../articles/transfer_learning.html">Transfer learning and fine tuning</a>
    <a class="dropdown-item" href="../../articles/distributed_training_with_tensorflow.html">Distributed training with TensorFlow</a>
  </div>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../../articles/examples/index.html">Examples</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../../news/index.html">News</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/rstudio/keras/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Image classification with EANet (External Attention Transformer)</h1>
                        <h4 data-toc-skip class="author"><a href="https://github.com/czy00000" class="external-link">ZhiYong Chang</a></h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/rstudio/keras/blob/HEAD/vignettes/examples/eanet.Rmd" class="external-link"><code>vignettes/examples/eanet.Rmd</code></a></small>
      <div class="d-none name"><code>eanet.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>This example implements the <a href="https://arxiv.org/abs/2105.02358" class="external-link">EANet</a> model for image
classification, and demonstrates it on the CIFAR-100 dataset. EANet
introduces a novel attention mechanism named <strong><em>external
attention</em></strong>, based on two external, small, learnable, and
shared memories, which can be implemented easily by simply using two
cascaded linear layers and two normalization layers. It conveniently
replaces self-attention as used in existing architectures. External
attention has linear complexity, as it only implicitly considers the
correlations between all samples.</p>
</div>
<div class="section level2">
<h2 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> keras <span class="im">as</span> keras</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> ops</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="prepare-the-data">Prepare the data<a class="anchor" aria-label="anchor" href="#prepare-the-data"></a>
</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>input_shape <span class="op">=</span> (<span class="dv">32</span>, <span class="dv">32</span>, <span class="dv">3</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> keras.datasets.cifar100.load_data()</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>y_train <span class="op">=</span> keras.utils.to_categorical(y_train, num_classes)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>y_test <span class="op">=</span> keras.utils.to_categorical(y_test, num_classes)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_train shape: </span><span class="sc">{</span>x_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_train shape: </span><span class="sc">{</span>y_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x_test shape: </span><span class="sc">{</span>x_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> - y_test shape: </span><span class="sc">{</span>y_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configure-the-hyperparameters">Configure the hyperparameters<a class="anchor" aria-label="anchor" href="#configure-the-hyperparameters"></a>
</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>label_smoothing <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>validation_split <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>patch_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Size of the patches to be extracted from the input images.</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>num_patches <span class="op">=</span> (input_shape[<span class="dv">0</span>] <span class="op">//</span> patch_size) <span class="op">**</span> <span class="dv">2</span>  <span class="co"># Number of patch</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">64</span>  <span class="co"># Number of hidden units.</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>mlp_dim <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>dim_coefficient <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>attention_dropout <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>projection_dropout <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>num_transformer_blocks <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Number of repetitions of the transformer layer</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patch size: </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> X </span><span class="sc">{</span>patch_size<span class="sc">}</span><span class="ss"> = </span><span class="sc">{</span>patch_size <span class="op">**</span> <span class="dv">2</span><span class="sc">}</span><span class="ss"> "</span>)</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Patches per image: </span><span class="sc">{</span>num_patches<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="use-data-augmentation">Use data augmentation<a class="anchor" aria-label="anchor" href="#use-data-augmentation"></a>
</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>data_augmentation <span class="op">=</span> keras.Sequential(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    [</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>        layers.Normalization(),</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>        layers.RandomFlip(<span class="st">"horizontal"</span>),</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>        layers.RandomRotation(factor<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        layers.RandomContrast(factor<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        layers.RandomZoom(height_factor<span class="op">=</span><span class="fl">0.2</span>, width_factor<span class="op">=</span><span class="fl">0.2</span>),</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    ],</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    name<span class="op">=</span><span class="st">"data_augmentation"</span>,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co"># Compute the mean and the variance of the training data for normalization.</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>data_augmentation.layers[<span class="dv">0</span>].adapt(x_train)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-patch-extraction-and-encoding-layer">Implement the patch extraction and encoding layer<a class="anchor" aria-label="anchor" href="#implement-the-patch-extraction-and-encoding-layer"></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">class</span> PatchExtract(layers.Layer):</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, patch_size, <span class="op">**</span>kwargs):</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>        <span class="va">self</span>.patch_size <span class="op">=</span> patch_size</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, x):</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        B, C <span class="op">=</span> ops.shape(x)[<span class="dv">0</span>], ops.shape(x)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        x <span class="op">=</span> ops.image.extract_patches(x, <span class="va">self</span>.patch_size)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>        x <span class="op">=</span> ops.reshape(x, (B, <span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.patch_size <span class="op">*</span> <span class="va">self</span>.patch_size <span class="op">*</span> C))</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="kw">class</span> PatchEmbedding(layers.Layer):</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_patch, embed_dim, <span class="op">**</span>kwargs):</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(<span class="op">**</span>kwargs)</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>        <span class="va">self</span>.num_patch <span class="op">=</span> num_patch</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>        <span class="va">self</span>.proj <span class="op">=</span> layers.Dense(embed_dim)</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>        <span class="va">self</span>.pos_embed <span class="op">=</span> layers.Embedding(</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>            input_dim<span class="op">=</span>num_patch, output_dim<span class="op">=</span>embed_dim</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>        )</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>    <span class="kw">def</span> call(<span class="va">self</span>, patch):</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>        pos <span class="op">=</span> ops.arange(start<span class="op">=</span><span class="dv">0</span>, stop<span class="op">=</span><span class="va">self</span>.num_patch, step<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.proj(patch) <span class="op">+</span> <span class="va">self</span>.pos_embed(pos)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-external-attention-block">Implement the external attention block<a class="anchor" aria-label="anchor" href="#implement-the-external-attention-block"></a>
</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">def</span> external_attention(</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    x,</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    dim,</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    num_heads,</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    dim_coefficient<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    attention_dropout<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    projection_dropout<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>):</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>    _, num_patch, channel <span class="op">=</span> x.shape</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    <span class="cf">assert</span> dim <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    num_heads <span class="op">=</span> num_heads <span class="op">*</span> dim_coefficient</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(dim <span class="op">*</span> dim_coefficient)(x)</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>    <span class="co"># create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    x <span class="op">=</span> ops.reshape(</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>        x, (<span class="op">-</span><span class="dv">1</span>, num_patch, num_heads, dim <span class="op">*</span> dim_coefficient <span class="op">//</span> num_heads)</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>    )</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a>    x <span class="op">=</span> ops.transpose(x, axes<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    <span class="co"># a linear layer M_k</span></span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    attn <span class="op">=</span> layers.Dense(dim <span class="op">//</span> dim_coefficient)(x)</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>    <span class="co"># normalize attention map</span></span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>    attn <span class="op">=</span> layers.Softmax(axis<span class="op">=</span><span class="dv">2</span>)(attn)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a>    <span class="co"># dobule-normalization</span></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a>    attn <span class="op">=</span> layers.Lambda(</span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a>        <span class="kw">lambda</span> attn: ops.divide(</span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a>            attn,</span>
<span id="cb6-27"><a href="#cb6-27" tabindex="-1"></a>            ops.convert_to_tensor(<span class="fl">1e-9</span>) <span class="op">+</span> ops.<span class="bu">sum</span>(attn, axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb6-28"><a href="#cb6-28" tabindex="-1"></a>        )</span>
<span id="cb6-29"><a href="#cb6-29" tabindex="-1"></a>    )(attn)</span>
<span id="cb6-30"><a href="#cb6-30" tabindex="-1"></a>    attn <span class="op">=</span> layers.Dropout(attention_dropout)(attn)</span>
<span id="cb6-31"><a href="#cb6-31" tabindex="-1"></a>    <span class="co"># a linear layer M_v</span></span>
<span id="cb6-32"><a href="#cb6-32" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(dim <span class="op">*</span> dim_coefficient <span class="op">//</span> num_heads)(attn)</span>
<span id="cb6-33"><a href="#cb6-33" tabindex="-1"></a>    x <span class="op">=</span> ops.transpose(x, axes<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb6-34"><a href="#cb6-34" tabindex="-1"></a>    x <span class="op">=</span> ops.reshape(x, [<span class="op">-</span><span class="dv">1</span>, num_patch, dim <span class="op">*</span> dim_coefficient])</span>
<span id="cb6-35"><a href="#cb6-35" tabindex="-1"></a>    <span class="co"># a linear layer to project original dim</span></span>
<span id="cb6-36"><a href="#cb6-36" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(dim)(x)</span>
<span id="cb6-37"><a href="#cb6-37" tabindex="-1"></a>    x <span class="op">=</span> layers.Dropout(projection_dropout)(x)</span>
<span id="cb6-38"><a href="#cb6-38" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-mlp-block">Implement the MLP block<a class="anchor" aria-label="anchor" href="#implement-the-mlp-block"></a>
</h2>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="kw">def</span> mlp(x, embedding_dim, mlp_dim, drop_rate<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(mlp_dim, activation<span class="op">=</span>ops.gelu)(x)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>    x <span class="op">=</span> layers.Dropout(drop_rate)(x)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(embedding_dim)(x)</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>    x <span class="op">=</span> layers.Dropout(drop_rate)(x)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-transformer-block">Implement the Transformer block<a class="anchor" aria-label="anchor" href="#implement-the-transformer-block"></a>
</h2>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> transformer_encoder(</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    x,</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    embedding_dim,</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    mlp_dim,</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>    num_heads,</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>    dim_coefficient,</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    attention_dropout,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    projection_dropout,</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    attention_type<span class="op">=</span><span class="st">"external_attention"</span>,</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>):</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    residual_1 <span class="op">=</span> x</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>    x <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)(x)</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    <span class="cf">if</span> attention_type <span class="op">==</span> <span class="st">"external_attention"</span>:</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>        x <span class="op">=</span> external_attention(</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>            x,</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>            embedding_dim,</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>            num_heads,</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>            dim_coefficient,</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>            attention_dropout,</span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>            projection_dropout,</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>        )</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>    <span class="cf">elif</span> attention_type <span class="op">==</span> <span class="st">"self_attention"</span>:</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a>        x <span class="op">=</span> layers.MultiHeadAttention(</span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a>            num_heads<span class="op">=</span>num_heads,</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>            key_dim<span class="op">=</span>embedding_dim,</span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>            dropout<span class="op">=</span>attention_dropout,</span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>        )(x, x)</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>    x <span class="op">=</span> layers.add([x, residual_1])</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>    residual_2 <span class="op">=</span> x</span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>    x <span class="op">=</span> layers.LayerNormalization(epsilon<span class="op">=</span><span class="fl">1e-5</span>)(x)</span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>    x <span class="op">=</span> mlp(x, embedding_dim, mlp_dim)</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>    x <span class="op">=</span> layers.add([x, residual_2])</span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="implement-the-eanet-model">Implement the EANet model<a class="anchor" aria-label="anchor" href="#implement-the-eanet-model"></a>
</h2>
<p>The EANet model leverages external attention. The computational
complexity of traditional self attention is <code>O(d * N ** 2)</code>,
where <code>d</code> is the embedding size, and <code>N</code> is the
number of patch. the authors find that most pixels are closely related
to just a few other pixels, and an <code>N</code>-to-<code>N</code>
attention matrix may be redundant. So, they propose as an alternative an
external attention module where the computational complexity of external
attention is <code>O(d * S * N)</code>. As <code>d</code> and
<code>S</code> are hyper-parameters, the proposed algorithm is linear in
the number of pixels. In fact, this is equivalent to a drop patch
operation, because a lot of information contained in a patch in an image
is redundant and unimportant.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="kw">def</span> get_model(attention_type<span class="op">=</span><span class="st">"external_attention"</span>):</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>input_shape)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>    <span class="co"># Image augment</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>    x <span class="op">=</span> data_augmentation(inputs)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>    <span class="co"># Extract patches.</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>    x <span class="op">=</span> PatchExtract(patch_size)(x)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>    <span class="co"># Create patch embedding.</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>    x <span class="op">=</span> PatchEmbedding(num_patches, embedding_dim)(x)</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>    <span class="co"># Create Transformer block.</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_transformer_blocks):</span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a>        x <span class="op">=</span> transformer_encoder(</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a>            x,</span>
<span id="cb9-13"><a href="#cb9-13" tabindex="-1"></a>            embedding_dim,</span>
<span id="cb9-14"><a href="#cb9-14" tabindex="-1"></a>            mlp_dim,</span>
<span id="cb9-15"><a href="#cb9-15" tabindex="-1"></a>            num_heads,</span>
<span id="cb9-16"><a href="#cb9-16" tabindex="-1"></a>            dim_coefficient,</span>
<span id="cb9-17"><a href="#cb9-17" tabindex="-1"></a>            attention_dropout,</span>
<span id="cb9-18"><a href="#cb9-18" tabindex="-1"></a>            projection_dropout,</span>
<span id="cb9-19"><a href="#cb9-19" tabindex="-1"></a>            attention_type,</span>
<span id="cb9-20"><a href="#cb9-20" tabindex="-1"></a>        )</span>
<span id="cb9-21"><a href="#cb9-21" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" tabindex="-1"></a>    x <span class="op">=</span> layers.GlobalAveragePooling1D()(x)</span>
<span id="cb9-23"><a href="#cb9-23" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Dense(num_classes, activation<span class="op">=</span><span class="st">"softmax"</span>)(x)</span>
<span id="cb9-24"><a href="#cb9-24" tabindex="-1"></a>    model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb9-25"><a href="#cb9-25" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="train-on-cifar-100">Train on CIFAR-100<a class="anchor" aria-label="anchor" href="#train-on-cifar-100"></a>
</h2>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>model <span class="op">=</span> get_model(attention_type<span class="op">=</span><span class="st">"external_attention"</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    loss<span class="op">=</span>keras.losses.CategoricalCrossentropy(label_smoothing<span class="op">=</span>label_smoothing),</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>    optimizer<span class="op">=</span>keras.optimizers.AdamW(</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>        learning_rate<span class="op">=</span>learning_rate, weight_decay<span class="op">=</span>weight_decay</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    ),</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    metrics<span class="op">=</span>[</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>        keras.metrics.CategoricalAccuracy(name<span class="op">=</span><span class="st">"accuracy"</span>),</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>        keras.metrics.TopKCategoricalAccuracy(<span class="dv">5</span>, name<span class="op">=</span><span class="st">"top-5-accuracy"</span>),</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    ],</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>)</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>    x_train,</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>    y_train,</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>    batch_size<span class="op">=</span>batch_size,</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>    epochs<span class="op">=</span>num_epochs,</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>    validation_split<span class="op">=</span>validation_split,</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>)</span></code></pre></div>
<div class="section level3">
<h3 id="lets-visualize-the-training-progress-of-the-model-">Let’s visualize the training progress of the model.<a class="anchor" aria-label="anchor" href="#lets-visualize-the-training-progress-of-the-model-"></a>
</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>plt.plot(history.history[<span class="st">"loss"</span>], label<span class="op">=</span><span class="st">"train_loss"</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_loss"</span>], label<span class="op">=</span><span class="st">"val_loss"</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>plt.xlabel(<span class="st">"Epochs"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>plt.title(<span class="st">"Train and Validation Losses Over Epochs"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>plt.grid()</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>plt.show()</span></code></pre></div>
</div>
<div class="section level3">
<h3 id="lets-display-the-final-results-of-the-test-on-cifar-100-">Let’s display the final results of the test on CIFAR-100.<a class="anchor" aria-label="anchor" href="#lets-display-the-final-results-of-the-test-on-cifar-100-"></a>
</h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>loss, accuracy, top_5_accuracy <span class="op">=</span> model.evaluate(x_test, y_test)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test loss: </span><span class="sc">{</span><span class="bu">round</span>(loss, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span><span class="bu">round</span>(accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test top 5 accuracy: </span><span class="sc">{</span><span class="bu">round</span>(top_5_accuracy <span class="op">*</span> <span class="dv">100</span>, <span class="dv">2</span>)<span class="sc">}</span><span class="ss">%"</span>)</span></code></pre></div>
<p>EANet just replaces self attention in Vit with external attention.
The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1
accuracy after training 50 epochs, but with 0.6M parameters. Under the
same experimental environment and the same hyperparameters, The EANet
model we just trained has just 0.3M parameters, and it gets us to ~73%
test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the
effectiveness of external attention.</p>
<p>We only show the training process of EANet, you can train Vit under
the same experimental conditions and observe the test results.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Tomasz Kalinowski, JJ Allaire, François Chollet, Posit Software, PBC, Google.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
