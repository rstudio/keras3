Help on class Embedding in module keras.src.layers.core.embedding:

class Embedding(keras.src.layers.layer.Layer)
 |  Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_constraint=None, mask_zero=False, weights=None, lora_rank=None, lora_alpha=None, **kwargs)
 |
 |  Turns nonnegative integers (indexes) into dense vectors of fixed size.
 |
 |  e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`
 |
 |  This layer can only be used on nonnegative integer inputs of a fixed range.
 |
 |  Example:
 |
 |  >>> model = keras.Sequential()
 |  >>> model.add(keras.layers.Embedding(1000, 64))
 |  >>> # The model will take as input an integer matrix of size (batch,
 |  >>> # input_length), and the largest integer (i.e. word index) in the input
 |  >>> # should be no larger than 999 (vocabulary size).
 |  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch
 |  >>> # dimension.
 |  >>> input_array = np.random.randint(1000, size=(32, 10))
 |  >>> model.compile('rmsprop', 'mse')
 |  >>> output_array = model.predict(input_array)
 |  >>> print(output_array.shape)
 |  (32, 10, 64)
 |
 |  Args:
 |      input_dim: Integer. Size of the vocabulary,
 |          i.e. maximum integer index + 1.
 |      output_dim: Integer. Dimension of the dense embedding.
 |      embeddings_initializer: Initializer for the `embeddings`
 |          matrix (see `keras.initializers`).
 |      embeddings_regularizer: Regularizer function applied to
 |          the `embeddings` matrix (see `keras.regularizers`).
 |      embeddings_constraint: Constraint function applied to
 |          the `embeddings` matrix (see `keras.constraints`).
 |      mask_zero: Boolean, whether or not the input value 0 is a special
 |          "padding" value that should be masked out.
 |          This is useful when using recurrent layers which
 |          may take variable length input. If this is `True`,
 |          then all subsequent layers in the model need
 |          to support masking or an exception will be raised.
 |          If `mask_zero` is set to `True`, as a consequence,
 |          index 0 cannot be used in the vocabulary (`input_dim` should
 |          equal size of vocabulary + 1).
 |      weights: Optional floating-point matrix of size
 |          `(input_dim, output_dim)`. The initial embeddings values
 |          to use.
 |      lora_rank: Optional integer. If set, the layer's forward pass
 |          will implement LoRA (Low-Rank Adaptation)
 |          with the provided rank. LoRA sets the layer's embeddings
 |          matrix to non-trainable and replaces it with a delta over the
 |          original matrix, obtained via multiplying two lower-rank
 |          trainable matrices. This can be useful to reduce the
 |          computation cost of fine-tuning large embedding layers.
 |          You can also enable LoRA on an existing
 |          `Embedding` layer by calling `layer.enable_lora(rank)`.
 |      lora_alpha: Optional integer. If set, this parameter scales the
 |          low-rank adaptation delta (computed as the product of two lower-rank
 |          trainable matrices) during the forward pass. The delta is scaled by
 |          `lora_alpha / lora_rank`, allowing you to fine-tune the strength of
 |          the LoRA adjustment independently of `lora_rank`.
 |
 |  Input shape:
 |      2D tensor with shape: `(batch_size, input_length)`.
 |
 |  Output shape:
 |      3D tensor with shape: `(batch_size, input_length, output_dim)`.
 |
 |  Method resolution order:
 |      Embedding
 |      keras.src.layers.layer.Layer
 |      keras.src.backend.tensorflow.layer.TFLayer
 |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable
 |      tensorflow.python.trackable.autotrackable.AutoTrackable
 |      tensorflow.python.trackable.base.Trackable
 |      keras.src.ops.operation.Operation
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    input_dim,
 |    output_dim,
 |    embeddings_initializer='uniform',
 |    embeddings_regularizer=None,
 |    embeddings_constraint=None,
 |    mask_zero=False,
 |    weights=None,
 |    lora_rank=None,
 |    lora_alpha=None,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  build(self, input_shape=None)
 |
 |  call(self, inputs)
 |
 |  compute_mask(
 |    self,
 |    inputs,
 |    mask=None
 |  )
 |
 |  compute_output_shape(self, input_shape)
 |
 |  compute_output_spec(self, inputs)
 |
 |  enable_lora(
 |    self,
 |    rank,
 |    lora_alpha=None,
 |    a_initializer='he_uniform',
 |    b_initializer='zeros'
 |  )
 |
 |  get_config(self)
 |      Returns the config of the object.
 |
 |      An object config is a Python dictionary (serializable)
 |      containing the information needed to re-instantiate it.
 |
 |  load_own_variables(self, store)
 |      Loads the state of the layer.
 |
 |      You can override this method to take full control of how the state of
 |      the layer is loaded upon calling `keras.models.load_model()`.
 |
 |      Args:
 |          store: Dict from which the state of the model will be loaded.
 |
 |  quantize(
 |    self,
 |    mode,
 |    type_check=True
 |  )
 |
 |  quantized_build(
 |    self,
 |    embeddings_shape,
 |    mode
 |  )
 |
 |  quantized_call(
 |    self,
 |    *args,
 |    **kwargs
 |  )
 |
 |  save_own_variables(self, store)
 |      Saves the state of the layer.
 |
 |      You can override this method to take full control of how the state of
 |      the layer is saved upon calling `model.save()`.
 |
 |      Args:
 |          store: Dict where the state of the model will be saved.
 |
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |
 |  embeddings
 |

