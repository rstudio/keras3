Help on class Dense in module keras.src.layers.core.dense:

class Dense(keras.src.layers.layer.Layer)
 |  Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, lora_rank=None, lora_alpha=None, **kwargs)
 |
 |  Just your regular densely-connected NN layer.
 |
 |  `Dense` implements the operation:
 |  `output = activation(dot(input, kernel) + bias)`
 |  where `activation` is the element-wise activation function
 |  passed as the `activation` argument, `kernel` is a weights matrix
 |  created by the layer, and `bias` is a bias vector created by the layer
 |  (only applicable if `use_bias` is `True`).
 |
 |  Note: If the input to the layer has a rank greater than 2, `Dense`
 |  computes the dot product between the `inputs` and the `kernel` along the
 |  last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).
 |  For example, if input has dimensions `(batch_size, d0, d1)`, then we create
 |  a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2
 |  of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are
 |  `batch_size * d0` such sub-tensors). The output in this case will have
 |  shape `(batch_size, d0, units)`.
 |
 |  Args:
 |      units: Positive integer, dimensionality of the output space.
 |      activation: Activation function to use.
 |          If you don't specify anything, no activation is applied
 |          (ie. "linear" activation: `a(x) = x`).
 |      use_bias: Boolean, whether the layer uses a bias vector.
 |      kernel_initializer: Initializer for the `kernel` weights matrix.
 |      bias_initializer: Initializer for the bias vector.
 |      kernel_regularizer: Regularizer function applied to
 |          the `kernel` weights matrix.
 |      bias_regularizer: Regularizer function applied to the bias vector.
 |      activity_regularizer: Regularizer function applied to
 |          the output of the layer (its "activation").
 |      kernel_constraint: Constraint function applied to
 |          the `kernel` weights matrix.
 |      bias_constraint: Constraint function applied to the bias vector.
 |      lora_rank: Optional integer. If set, the layer's forward pass
 |          will implement LoRA (Low-Rank Adaptation)
 |          with the provided rank. LoRA sets the layer's kernel
 |          to non-trainable and replaces it with a delta over the
 |          original kernel, obtained via multiplying two lower-rank
 |          trainable matrices. This can be useful to reduce the
 |          computation cost of fine-tuning large dense layers.
 |          You can also enable LoRA on an existing
 |          `Dense` layer by calling `layer.enable_lora(rank)`.
 |      lora_alpha: Optional integer. If set, this parameter scales the
 |          low-rank adaptation delta (computed as the product of two lower-rank
 |          trainable matrices) during the forward pass. The delta is scaled by
 |          `lora_alpha / lora_rank`, allowing you to fine-tune the strength of
 |          the LoRA adjustment independently of `lora_rank`.
 |
 |  Input shape:
 |      N-D tensor with shape: `(batch_size, ..., input_dim)`.
 |      The most common situation would be
 |      a 2D input with shape `(batch_size, input_dim)`.
 |
 |  Output shape:
 |      N-D tensor with shape: `(batch_size, ..., units)`.
 |      For instance, for a 2D input with shape `(batch_size, input_dim)`,
 |      the output would have shape `(batch_size, units)`.
 |
 |  Method resolution order:
 |      Dense
 |      keras.src.layers.layer.Layer
 |      keras.src.backend.tensorflow.layer.TFLayer
 |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable
 |      tensorflow.python.trackable.autotrackable.AutoTrackable
 |      tensorflow.python.trackable.base.Trackable
 |      keras.src.ops.operation.Operation
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    units,
 |    activation=None,
 |    use_bias=True,
 |    kernel_initializer='glorot_uniform',
 |    bias_initializer='zeros',
 |    kernel_regularizer=None,
 |    bias_regularizer=None,
 |    activity_regularizer=None,
 |    kernel_constraint=None,
 |    bias_constraint=None,
 |    lora_rank=None,
 |    lora_alpha=None,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  build(self, input_shape)
 |
 |  call(
 |    self,
 |    inputs,
 |    training=None
 |  )
 |
 |  compute_output_shape(self, input_shape)
 |
 |  enable_lora(
 |    self,
 |    rank,
 |    lora_alpha=None,
 |    a_initializer='he_uniform',
 |    b_initializer='zeros'
 |  )
 |
 |  get_config(self)
 |      Returns the config of the object.
 |
 |      An object config is a Python dictionary (serializable)
 |      containing the information needed to re-instantiate it.
 |
 |  load_own_variables(self, store)
 |      Loads the state of the layer.
 |
 |      You can override this method to take full control of how the state of
 |      the layer is loaded upon calling `keras.models.load_model()`.
 |
 |      Args:
 |          store: Dict from which the state of the model will be loaded.
 |
 |  quantize(
 |    self,
 |    mode,
 |    type_check=True
 |  )
 |
 |  quantized_build(
 |    self,
 |    kernel_shape,
 |    mode
 |  )
 |
 |  save_own_variables(self, store)
 |      Saves the state of the layer.
 |
 |      You can override this method to take full control of how the state of
 |      the layer is saved upon calling `model.save()`.
 |
 |      Args:
 |          store: Dict where the state of the model will be saved.
 |
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |
 |  kernel
 |

