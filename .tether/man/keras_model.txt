Help on class Model in module keras.src.models.model:

class Model(keras.src.backend.tensorflow.trainer.TensorFlowTrainer, keras.src.trainers.trainer.Trainer, keras.src.layers.layer.Layer)
 |  Model(*args, **kwargs)
 |
 |  A model grouping layers into an object with training/inference features.
 |
 |  There are three ways to instantiate a `Model`:
 |
 |  ## With the "Functional API"
 |
 |  You start from `Input`,
 |  you chain layer calls to specify the model's forward pass,
 |  and finally, you create your model from inputs and outputs:
 |
 |  ```python
 |  inputs = keras.Input(shape=(37,))
 |  x = keras.layers.Dense(32, activation="relu")(inputs)
 |  outputs = keras.layers.Dense(5, activation="softmax")(x)
 |  model = keras.Model(inputs=inputs, outputs=outputs)
 |  ```
 |
 |  Note: Only dicts, lists, and tuples of input tensors are supported. Nested
 |  inputs are not supported (e.g. lists of list or dicts of dict).
 |
 |  A new Functional API model can also be created by using the
 |  intermediate tensors. This enables you to quickly extract sub-components
 |  of the model.
 |
 |  Example:
 |
 |  ```python
 |  inputs = keras.Input(shape=(None, None, 3))
 |  processed = keras.layers.RandomCrop(width=128, height=128)(inputs)
 |  conv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)
 |  pooling = keras.layers.GlobalAveragePooling2D()(conv)
 |  feature = keras.layers.Dense(10)(pooling)
 |
 |  full_model = keras.Model(inputs, feature)
 |  backbone = keras.Model(processed, conv)
 |  activations = keras.Model(conv, feature)
 |  ```
 |
 |  Note that the `backbone` and `activations` models are not
 |  created with `keras.Input` objects, but with the tensors that originate
 |  from `keras.Input` objects. Under the hood, the layers and weights will
 |  be shared across these models, so that user can train the `full_model`, and
 |  use `backbone` or `activations` to do feature extraction.
 |  The inputs and outputs of the model can be nested structures of tensors as
 |  well, and the created models are standard Functional API models that support
 |  all the existing APIs.
 |
 |  ## By subclassing the `Model` class
 |
 |  In that case, you should define your
 |  layers in `__init__()` and you should implement the model's forward pass
 |  in `call()`.
 |
 |  ```python
 |  class MyModel(keras.Model):
 |      def __init__(self):
 |          super().__init__()
 |          self.dense1 = keras.layers.Dense(32, activation="relu")
 |          self.dense2 = keras.layers.Dense(5, activation="softmax")
 |
 |      def call(self, inputs):
 |          x = self.dense1(inputs)
 |          return self.dense2(x)
 |
 |  model = MyModel()
 |  ```
 |
 |  If you subclass `Model`, you can optionally have
 |  a `training` argument (boolean) in `call()`, which you can use to specify
 |  a different behavior in training and inference:
 |
 |  ```python
 |  class MyModel(keras.Model):
 |      def __init__(self):
 |          super().__init__()
 |          self.dense1 = keras.layers.Dense(32, activation="relu")
 |          self.dense2 = keras.layers.Dense(5, activation="softmax")
 |          self.dropout = keras.layers.Dropout(0.5)
 |
 |      def call(self, inputs, training=False):
 |          x = self.dense1(inputs)
 |          x = self.dropout(x, training=training)
 |          return self.dense2(x)
 |
 |  model = MyModel()
 |  ```
 |
 |  Once the model is created, you can config the model with losses and metrics
 |  with `model.compile()`, train the model with `model.fit()`, or use the model
 |  to do prediction with `model.predict()`.
 |
 |  ## With the `Sequential` class
 |
 |  In addition, `keras.Sequential` is a special case of model where
 |  the model is purely a stack of single-input, single-output layers.
 |
 |  ```python
 |  model = keras.Sequential([
 |      keras.Input(shape=(None, None, 3)),
 |      keras.layers.Conv2D(filters=32, kernel_size=3),
 |  ])
 |  ```
 |
 |  Method resolution order:
 |      Model
 |      keras.src.backend.tensorflow.trainer.TensorFlowTrainer
 |      keras.src.trainers.trainer.Trainer
 |      keras.src.layers.layer.Layer
 |      keras.src.backend.tensorflow.layer.TFLayer
 |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable
 |      tensorflow.python.trackable.autotrackable.AutoTrackable
 |      tensorflow.python.trackable.base.Trackable
 |      keras.src.ops.operation.Operation
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    *args,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  build_from_config(self, config)
 |      Builds the layer's states with the supplied config dict.
 |
 |      By default, this method calls the `build(config["input_shape"])` method,
 |      which creates weights based on the layer's input shape in the supplied
 |      config. If your config contains other information needed to load the
 |      layer's state, you should override this method.
 |
 |      Args:
 |          config: Dict containing the input shape associated with this layer.
 |
 |  call(
 |    self,
 |    *args,
 |    **kwargs
 |  )
 |
 |  export(
 |    self,
 |    filepath,
 |    format='tf_saved_model',
 |    verbose=None,
 |    input_signature=None,
 |    **kwargs
 |  )
 |      Export the model as an artifact for inference.
 |
 |      Args:
 |          filepath: `str` or `pathlib.Path` object. The path to save the
 |              artifact.
 |          format: `str`. The export format. Supported values:
 |              `"tf_saved_model"` and `"onnx"`.  Defaults to
 |              `"tf_saved_model"`.
 |          verbose: `bool`. Whether to print a message during export. Defaults
 |              to `None`, which uses the default value set by different
 |              backends and formats.
 |          input_signature: Optional. Specifies the shape and dtype of the
 |              model inputs. Can be a structure of `keras.InputSpec`,
 |              `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If
 |              not provided, it will be automatically computed. Defaults to
 |              `None`.
 |          **kwargs: Additional keyword arguments:
 |              - Specific to the JAX backend and `format="tf_saved_model"`:
 |                  - `is_static`: Optional `bool`. Indicates whether `fn` is
 |                      static. Set to `False` if `fn` involves state updates
 |                      (e.g., RNG seeds and counters).
 |                  - `jax2tf_kwargs`: Optional `dict`. Arguments for
 |                      `jax2tf.convert`. See the documentation for
 |                      [`jax2tf.convert`](
 |                          https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).
 |                      If `native_serialization` and `polymorphic_shapes` are
 |                      not provided, they will be automatically computed.
 |
 |      **Note:** This feature is currently supported only with TensorFlow, JAX
 |      and Torch backends.
 |
 |      **Note:** Be aware that the exported artifact may contain information
 |      from the local file system when using `format="onnx"`, `verbose=True`
 |      and Torch backend.
 |
 |      Examples:
 |
 |      Here's how to export a TensorFlow SavedModel for inference.
 |
 |      ```python
 |      # Export the model as a TensorFlow SavedModel artifact
 |      model.export("path/to/location", format="tf_saved_model")
 |
 |      # Load the artifact in a different process/environment
 |      reloaded_artifact = tf.saved_model.load("path/to/location")
 |      predictions = reloaded_artifact.serve(input_data)
 |      ```
 |
 |      Here's how to export an ONNX for inference.
 |
 |      ```python
 |      # Export the model as a ONNX artifact
 |      model.export("path/to/location", format="onnx")
 |
 |      # Load the artifact in a different process/environment
 |      ort_session = onnxruntime.InferenceSession("path/to/location")
 |      ort_inputs = {
 |          k.name: v for k, v in zip(ort_session.get_inputs(), input_data)
 |      }
 |      predictions = ort_session.run(None, ort_inputs)
 |      ```
 |
 |  get_layer(
 |    self,
 |    name=None,
 |    index=None
 |  )
 |      Retrieves a layer based on either its name (unique) or index.
 |
 |      If `name` and `index` are both provided, `index` will take precedence.
 |      Indices are based on order of horizontal graph traversal (bottom-up).
 |
 |      Args:
 |          name: String, name of layer.
 |          index: Integer, index of layer.
 |
 |      Returns:
 |          A layer instance.
 |
 |  get_state_tree(self, value_format='backend_tensor')
 |      Retrieves tree-like structure of model variables.
 |
 |      This method allows retrieval of different model variables (trainable,
 |      non-trainable, optimizer, and metrics). The variables are returned in a
 |      nested dictionary format, where the keys correspond to the variable
 |      names and the values are the nested representations of the variables.
 |
 |      Returns:
 |          dict: A dictionary containing the nested representations of the
 |              requested variables. The keys are the variable names, and the
 |              values are the corresponding nested dictionaries.
 |          value_format: One of `"backend_tensor"`, `"numpy_array"`.
 |              The kind of array to return as the leaves of the nested
 |                  state tree.
 |
 |      Example:
 |
 |      ```python
 |      model = keras.Sequential([
 |          keras.Input(shape=(1,), name="my_input"),
 |          keras.layers.Dense(1, activation="sigmoid", name="my_dense"),
 |      ], name="my_sequential")
 |      model.compile(optimizer="adam", loss="mse", metrics=["mae"])
 |      model.fit(np.array([[1.0]]), np.array([[1.0]]))
 |      state_tree = model.get_state_tree()
 |      ```
 |
 |      The `state_tree` dictionary returned looks like:
 |
 |      ```
 |      {
 |          'metrics_variables': {
 |              'loss': {
 |                  'count': ...,
 |                  'total': ...,
 |              },
 |              'mean_absolute_error': {
 |                  'count': ...,
 |                  'total': ...,
 |              }
 |          },
 |          'trainable_variables': {
 |              'my_sequential': {
 |                  'my_dense': {
 |                      'bias': ...,
 |                      'kernel': ...,
 |                  }
 |              }
 |          },
 |          'non_trainable_variables': {},
 |          'optimizer_variables': {
 |              'adam': {
 |                      'iteration': ...,
 |                      'learning_rate': ...,
 |                      'my_sequential_my_dense_bias_momentum': ...,
 |                      'my_sequential_my_dense_bias_velocity': ...,
 |                      'my_sequential_my_dense_kernel_momentum': ...,
 |                      'my_sequential_my_dense_kernel_velocity': ...,
 |                  }
 |              }
 |          }
 |      }
 |      ```
 |
 |  load_weights(
 |    self,
 |    filepath,
 |    skip_mismatch=False,
 |    **kwargs
 |  )
 |      Load the weights from a single file or sharded files.
 |
 |      Weights are loaded based on the network's topology. This means the
 |      architecture should be the same as when the weights were saved. Note
 |      that layers that don't have weights are not taken into account in the
 |      topological ordering, so adding or removing layers is fine as long as
 |      they don't have weights.
 |
 |      **Partial weight loading**
 |
 |      If you have modified your model, for instance by adding a new layer
 |      (with weights) or by changing the shape of the weights of a layer, you
 |      can choose to ignore errors and continue loading by setting
 |      `skip_mismatch=True`. In this case any layer with mismatching weights
 |      will be skipped. A warning will be displayed for each skipped layer.
 |
 |      **Sharding**
 |
 |      When loading sharded weights, it is important to specify `filepath` that
 |      ends with `*.weights.json` which is used as the configuration file.
 |      Additionally, the sharded files `*_xxxxx.weights.h5` must be in the same
 |      directory as the configuration file.
 |
 |      Args:
 |          filepath: `str` or `pathlib.Path` object. Path where the weights
 |              will be saved.  When sharding, the filepath must end in
 |              `.weights.json`.
 |          skip_mismatch: Boolean, whether to skip loading of layers where
 |              there is a mismatch in the number of weights, or a mismatch in
 |              the shape of the weights.
 |
 |      Example:
 |
 |      ```python
 |      # Load the weights in a single file.
 |      model.load_weights("model.weights.h5")
 |
 |      # Load the weights in sharded files.
 |      model.load_weights("model.weights.json")
 |      ```
 |
 |  quantize(
 |    self,
 |    mode,
 |    **kwargs
 |  )
 |      Quantize the weights of the model.
 |
 |      Note that the model must be built first before calling this method.
 |      `quantize` will recursively call `quantize(mode)` in all layers and
 |      will be skipped if the layer doesn't implement the function.
 |
 |      Args:
 |          mode: The mode of the quantization. Only 'int8' is supported at this
 |              time.
 |
 |  save(
 |    self,
 |    filepath,
 |    overwrite=True,
 |    zipped=None,
 |    **kwargs
 |  )
 |      Saves a model as a `.keras` file.
 |
 |      Note that `model.save()` is an alias for `keras.saving.save_model()`.
 |
 |      The saved `.keras` file contains:
 |
 |      - The model's configuration (architecture)
 |      - The model's weights
 |      - The model's optimizer's state (if any)
 |
 |      Thus models can be reinstantiated in the exact same state.
 |
 |      Args:
 |          filepath: `str` or `pathlib.Path` object.
 |              The path where to save the model. Must end in `.keras`
 |              (unless saving the model as an unzipped directory
 |              via `zipped=False`).
 |          overwrite: Whether we should overwrite any existing model at
 |              the target location, or instead ask the user via
 |              an interactive prompt.
 |          zipped: Whether to save the model as a zipped `.keras`
 |              archive (default when saving locally), or as an
 |              unzipped directory (default when saving on the
 |              Hugging Face Hub).
 |
 |      Example:
 |
 |      ```python
 |      model = keras.Sequential(
 |          [
 |              keras.layers.Dense(5, input_shape=(3,)),
 |              keras.layers.Softmax(),
 |          ],
 |      )
 |      model.save("model.keras")
 |      loaded_model = keras.saving.load_model("model.keras")
 |      x = keras.random.uniform((10, 3))
 |      assert np.allclose(model.predict(x), loaded_model.predict(x))
 |      ```
 |
 |  save_weights(
 |    self,
 |    filepath,
 |    overwrite=True,
 |    max_shard_size=None
 |  )
 |      Saves all weights to a single file or sharded files.
 |
 |      By default, the weights will be saved in a single `.weights.h5` file.
 |      If sharding is enabled (`max_shard_size` is not `None`), the weights
 |      will be saved in multiple files, each with a size at most
 |      `max_shard_size` (in GB). Additionally, a configuration file
 |      `.weights.json` will contain the metadata for the sharded files.
 |
 |      The saved sharded files contain:
 |
 |      - `*.weights.json`: The configuration file containing 'metadata' and
 |          'weight_map'.
 |      - `*_xxxxxx.weights.h5`: The sharded files containing only the
 |          weights.
 |
 |      Args:
 |          filepath: `str` or `pathlib.Path` object. Path where the weights
 |              will be saved.  When sharding, the filepath must end in
 |              `.weights.json`. If `.weights.h5` is provided, it will be
 |              overridden.
 |          overwrite: Whether to overwrite any existing weights at the target
 |              location or instead ask the user via an interactive prompt.
 |          max_shard_size: `int` or `float`. Maximum size in GB for each
 |              sharded file. If `None`, no sharding will be done. Defaults to
 |              `None`.
 |
 |      Example:
 |
 |      ```python
 |      # Instantiate a EfficientNetV2L model with about 454MB of weights.
 |      model = keras.applications.EfficientNetV2L(weights=None)
 |
 |      # Save the weights in a single file.
 |      model.save_weights("model.weights.h5")
 |
 |      # Save the weights in sharded files. Use `max_shard_size=0.25` means
 |      # each sharded file will be at most ~250MB.
 |      model.save_weights("model.weights.json", max_shard_size=0.25)
 |
 |      # Load the weights in a new model with the same architecture.
 |      loaded_model = keras.applications.EfficientNetV2L(weights=None)
 |      loaded_model.load_weights("model.weights.h5")
 |      x = keras.random.uniform((1, 480, 480, 3))
 |      assert np.allclose(model.predict(x), loaded_model.predict(x))
 |
 |      # Load the sharded weights in a new model with the same architecture.
 |      loaded_model = keras.applications.EfficientNetV2L(weights=None)
 |      loaded_model.load_weights("model.weights.json")
 |      x = keras.random.uniform((1, 480, 480, 3))
 |      assert np.allclose(model.predict(x), loaded_model.predict(x))
 |      ```
 |
 |  set_state_tree(self, state_tree)
 |      Assigns values to variables of the model.
 |
 |      This method takes a dictionary of nested variable values, which
 |      represents the state tree of the model, and assigns them to the
 |      corresponding variables of the model. The dictionary keys represent the
 |      variable names (e.g., `'trainable_variables'`, `'optimizer_variables'`),
 |      and the values are nested dictionaries containing the variable
 |      paths and their corresponding values.
 |
 |      Args:
 |          state_tree: A dictionary representing the state tree of the model.
 |              The keys are the variable names, and the values are nested
 |              dictionaries representing the variable paths and their values.
 |
 |  summary(
 |    self,
 |    line_length=None,
 |    positions=None,
 |    print_fn=None,
 |    expand_nested=False,
 |    show_trainable=False,
 |    layer_range=None
 |  )
 |      Prints a string summary of the network.
 |
 |      Args:
 |          line_length: Total length of printed lines
 |              (e.g. set this to adapt the display to different
 |              terminal window sizes).
 |          positions: Relative or absolute positions of log elements
 |              in each line. If not provided, becomes
 |              `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.
 |          print_fn: Print function to use. By default, prints to `stdout`.
 |              If `stdout` doesn't work in your environment, change to `print`.
 |              It will be called on each line of the summary.
 |              You can set it to a custom function
 |              in order to capture the string summary.
 |          expand_nested: Whether to expand the nested models.
 |              Defaults to `False`.
 |          show_trainable: Whether to show if a layer is trainable.
 |              Defaults to `False`.
 |          layer_range: a list or tuple of 2 strings,
 |              which is the starting layer name and ending layer name
 |              (both inclusive) indicating the range of layers to be printed
 |              in summary. It also accepts regex patterns instead of exact
 |              names. In this case, the start predicate will be
 |              the first element that matches `layer_range[0]`
 |              and the end predicate will be the last element
 |              that matches `layer_range[1]`.
 |              By default `None` considers all layers of the model.
 |
 |      Raises:
 |          ValueError: if `summary()` is called before the model is built.
 |
 |  to_json(self, **kwargs)
 |      Returns a JSON string containing the network configuration.
 |
 |      To load a network from a JSON save file, use
 |      `keras.models.model_from_json(json_string, custom_objects={...})`.
 |
 |      Args:
 |          **kwargs: Additional keyword arguments to be passed to
 |              `json.dumps()`.
 |
 |      Returns:
 |          A JSON string.
 |
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |
 |  from_config(config, custom_objects=None)
 |      Creates an operation from its config.
 |
 |      This method is the reverse of `get_config`, capable of instantiating the
 |      same operation from the config dictionary.
 |
 |      Note: If you override this method, you might receive a serialized dtype
 |      config, which is a `dict`. You can deserialize it as follows:
 |
 |      ```python
 |      if "dtype" in config and isinstance(config["dtype"], dict):
 |          policy = dtype_policies.deserialize(config["dtype"])
 |      ```
 |
 |      Args:
 |          config: A Python dictionary, typically the output of `get_config`.
 |
 |      Returns:
 |          An operation instance.
 |
 |  ----------------------------------------------------------------------
 |  Static methods defined here:
 |
 |  __new__(
 |    cls,
 |    *args,
 |    **kwargs
 |  )
 |      Create and return a new object.  See help(type) for accurate signature.
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |
 |  layers
 |

