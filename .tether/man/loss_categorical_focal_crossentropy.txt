#' Computes the alpha balanced focal crossentropy loss.
#'
#' @description
#' Use this crossentropy loss function when there are two or more label
#' classes and if you want to handle class imbalance without using
#' `class_weights`. We expect labels to be provided in a `one_hot`
#' representation.
#'
#' According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it
#' helps to apply a focal factor to down-weight easy examples and focus more on
#' hard examples. The general formula for the focal loss (FL)
#' is as follows:
#'
#' `FL(p_t) = (1 - p_t) ** gamma * log(p_t)`
#'
#' where `p_t` is defined as follows:
#' `p_t = output if y_true == 1, else 1 - output`
#'
#' `(1 - p_t) ** gamma` is the `modulating_factor`, where `gamma` is a focusing
#' parameter. When `gamma` = 0, there is no focal effect on the cross entropy.
#' `gamma` reduces the importance given to simple examples in a smooth manner.
#'
#' The authors use alpha-balanced variant of focal loss (FL) in the paper:
#' `FL(p_t) = -alpha * (1 - p_t) ** gamma * log(p_t)`
#'
#' where `alpha` is the weight factor for the classes. If `alpha` = 1, the
#' loss won't be able to handle class imbalance properly as all
#' classes will have the same weight. This can be a constant or a list of
#' constants. If alpha is a list, it must have the same length as the number
#' of classes.
#'
#' The formula above can be generalized to:
#' `FL(p_t) = alpha * (1 - p_t) ** gamma * CrossEntropy(y_true, y_pred)`
#'
#' where minus comes from `CrossEntropy(y_true, y_pred)` (CE).
#'
#' Extending this to multi-class case is straightforward:
#' `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`
#'
#' In the snippet below, there is `num_classes` floating pointing values per
#' example. The shape of both `y_pred` and `y_true` are
#' `(batch_size, num_classes)`.
#'
#' # Examples
#' ```python
#' y_true = [[0, 1, 0], [0, 0, 1]]
#' y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]
#' loss = keras.losses.categorical_focal_crossentropy(y_true, y_pred)
#' assert loss.shape == (2,)
#' loss
#' # array([2.63401289e-04, 6.75912094e-01], dtype=float32)
#' ```
#' Standalone usage:
#'
#' ```python
#' y_true = [[0., 1., 0.], [0., 0., 1.]]
#' y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
#' # Using 'auto'/'sum_over_batch_size' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy()
#' cce(y_true, y_pred)
#' # 0.23315276
#' ```
#'
#' ```python
#' # Calling with 'sample_weight'.
#' cce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
#' # 0.1632
#' ```
#'
#' ```python
#' # Using 'sum' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy(
#'     reduction="sum")
#' cce(y_true, y_pred)
#' # 0.46631
#' ```
#'
#' ```python
#' # Using 'none' reduction type.
#' cce = keras.losses.CategoricalFocalCrossentropy(
#'     reduction=None)
#' cce(y_true, y_pred)
#' # array([3.2058331e-05, 4.6627346e-01], dtype=float32)
#' ```
#'
#' Usage with the `compile()` API:
#'
#' ```python
#' model.compile(optimizer='adam',
#'               loss=keras.losses.CategoricalFocalCrossentropy())
#' ```
#'
#' @returns
#' Categorical focal crossentropy loss value.
#'
#' @param alpha
#' A weight balancing factor for all classes, default is `0.25` as
#' mentioned in the reference. It can be a list of floats or a scalar.
#' In the multi-class case, alpha may be set by inverse class
#' frequency by using `compute_class_weight` from `sklearn.utils`.
#'
#' @param gamma
#' A focusing parameter, default is `2.0` as mentioned in the
#' reference. It helps to gradually reduce the importance given to
#' simple examples in a smooth manner. When `gamma` = 0, there is
#' no focal effect on the categorical crossentropy.
#'
#' @param from_logits
#' Whether `output` is expected to be a logits tensor. By
#' default, we consider that `output` encodes a probability
#' distribution.
#'
#' @param label_smoothing
#' Float in `[0, 1].` When > 0, label values are smoothed,
#' meaning the confidence on label values are relaxed. For example, if
#' `0.1`, use `0.1 / num_classes` for non-target labels and
#' `0.9 + 0.1 / num_classes` for target labels.
#'
#' @param axis
#' The axis along which to compute crossentropy (the features
#' axis). Defaults to `-1`.
#'
#' @param reduction
#' Type of reduction to apply to the loss. In almost all cases
#' this should be `"sum_over_batch_size"`.
#' Supported options are `"sum"`, `"sum_over_batch_size"` or `None`.
#'
#' @param name
#' Optional name for the loss instance.
#'
#' @param y_true
#' Tensor of one-hot true targets.
#'
#' @param y_pred
#' Tensor of predicted targets.
#'
#' @param ...
#' Passed on to the Python callable
#'
#' @export
#' @family losses
#' @seealso
#' + <https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalFocalCrossentropy>
loss_categorical_focal_crossentropy <-
__signature__
keras.losses.CategoricalFocalCrossentropy(
  alpha=0.25,
  gamma=2.0,
  from_logits=False,
  label_smoothing=0.0,
  axis=-1,
  reduction='sum_over_batch_size',
  name='categorical_focal_crossentropy'
)
