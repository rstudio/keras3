Help on class LossScaleOptimizer in module keras.src.optimizers.loss_scale_optimizer:

class LossScaleOptimizer(keras.src.optimizers.optimizer.Optimizer)
 |  LossScaleOptimizer(inner_optimizer, initial_scale=32768.0, dynamic_growth_steps=2000, **kwargs)
 |
 |  An optimizer that dynamically scales the loss to prevent underflow.
 |
 |  Loss scaling is a technique to prevent numeric underflow in intermediate
 |  gradients when float16 is used. To prevent underflow, the loss is multiplied
 |  (or "scaled") by a certain factor called the "loss scale", which causes
 |  intermediate gradients to be scaled by the loss scale as well. The final
 |  gradients are divided (or "unscaled") by the loss scale to bring them back
 |  to their original value.
 |
 |  `LossScaleOptimizer` wraps another optimizer and applies dynamic loss
 |  scaling to it. This loss scale is dynamically updated over time as follows:
 |  - On any train step, if a nonfinite gradient is encountered, the loss scale
 |    is halved, and the train step is skipped.
 |  - If `dynamic_growth_steps` have ocurred since the last time the loss scale
 |    was updated, and no nonfinite gradients have occurred, the loss scale
 |    is doubled.
 |
 |  Args:
 |      inner_optimizer: The `keras.optimizers.Optimizer` instance to wrap.
 |      initial_scale: Float. The initial loss scale. This scale will be updated
 |          during training. It is recommended for this to be a very high
 |          number, because a loss scale that is too high gets lowered far more
 |          quickly than a loss scale that is too low gets raised.
 |      dynamic_growth_steps: Int. How often to update the scale upwards. After
 |          every `dynamic_growth_steps` steps with finite gradients, the
 |          loss scale is doubled.
 |      name: String. The name to use
 |          for momentum accumulator weights created by
 |          the optimizer.
 |      weight_decay: Float. If set, weight decay is applied.
 |      clipnorm: Float. If set, the gradient of each weight is individually
 |          clipped so that its norm is no higher than this value.
 |      clipvalue: Float. If set, the gradient of each weight is clipped to be
 |          no higher than this value.
 |      global_clipnorm: Float. If set, the gradient of all weights is clipped
 |          so that their global norm is no higher than this value.
 |      use_ema: Boolean, defaults to False. If True, exponential moving average
 |          (EMA) is applied. EMA consists of computing an exponential moving
 |          average of the weights of the model (as the weight values change
 |          after each training batch), and periodically overwriting the
 |          weights with their moving average.
 |      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.
 |          This is the momentum to use when computing
 |          the EMA of the model's weights:
 |          `new_average = ema_momentum * old_average + (1 - ema_momentum) *
 |          current_variable_value`.
 |      ema_overwrite_frequency: Int or None, defaults to None. Only used if
 |          `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,
 |          we overwrite the model variable by its moving average.
 |          If None, the optimizer
 |          does not overwrite model variables in the middle of training,
 |          and you need to explicitly overwrite the variables
 |          at the end of training by calling
 |          `optimizer.finalize_variable_values()` (which updates the model
 |          variables in-place). When using the built-in `fit()` training loop,
 |          this happens automatically after the last epoch,
 |          and you don't need to do anything.
 |      loss_scale_factor: Float or `None`. If a float, the scale factor will
 |          be multiplied the loss before computing gradients, and the inverse
 |          of the scale factor will be multiplied by the gradients before
 |          updating variables. Useful for preventing underflow during
 |          mixed precision training. Alternately,
 |          `keras.optimizers.LossScaleOptimizer` will
 |          automatically set a loss scale factor.
 |      gradient_accumulation_steps: Int or `None`. If an int, model & optimizer
 |          variables will not be updated at every step; instead they will be
 |          updated every `gradient_accumulation_steps` steps, using the average
 |          value of the gradients since the last update. This is known as
 |          "gradient accumulation". This can be useful
 |          when your batch size is very small, in order to reduce gradient
 |          noise at each update step.
 |
 |  Method resolution order:
 |      LossScaleOptimizer
 |      keras.src.optimizers.optimizer.Optimizer
 |      keras.src.backend.tensorflow.optimizer.TFOptimizer
 |      keras.src.optimizers.base_optimizer.BaseOptimizer
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    inner_optimizer,
 |    initial_scale=32768.0,
 |    dynamic_growth_steps=2000,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  apply(
 |    self,
 |    grads,
 |    trainable_variables=None
 |  )
 |      Update traininable variables according to provided gradient values.
 |
 |      `grads` should be a list of gradient tensors
 |      with 1:1 mapping to the list of variables the optimizer was built with.
 |
 |      `trainable_variables` can be provided
 |      on the first call to build the optimizer.
 |
 |  build(self, var_list)
 |
 |  check_finite(self, grads)
 |
 |  finalize_variable_values(self, var_list)
 |      Set the final value of model's trainable variables.
 |
 |      Sometimes there are some extra steps before ending the variable updates,
 |      such as overriding the model variables with its average value.
 |
 |      Args:
 |        var_list: list of model variables.
 |
 |  get_config(self)
 |      Returns the config of the optimizer.
 |
 |      An optimizer config is a Python dictionary (serializable)
 |      containing the configuration of an optimizer.
 |      The same optimizer can be reinstantiated later
 |      (without any saved state) from this configuration.
 |
 |      Subclass optimizer should override this method to include other
 |      hyperparameters.
 |
 |      Returns:
 |          Python dictionary.
 |
 |  scale_loss(self, loss)
 |      Scale the loss before computing gradients.
 |
 |      Scales the loss before gradients are computed in a `train_step`. This
 |      is primarily useful during mixed precision training to prevent numeric
 |      underflow.
 |
 |  stateless_apply(
 |    self,
 |    optimizer_variables,
 |    grads,
 |    trainable_variables
 |  )
 |
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |
 |  from_config(config, custom_objects=None) from builtins.type
 |      Creates an optimizer from its config.
 |
 |      This method is the reverse of `get_config`, capable of instantiating the
 |      same optimizer from the config dictionary.
 |
 |      Args:
 |          config: A Python dictionary, typically the output of get_config.
 |          custom_objects: A Python dictionary mapping names to additional
 |            user-defined Python objects needed to recreate this optimizer.
 |
 |      Returns:
 |          An optimizer instance.
 |
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |
 |  variables
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |
 |  learning_rate
 |

