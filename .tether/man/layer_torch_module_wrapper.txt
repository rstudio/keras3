Help on class TorchModuleWrapper in module keras.src.utils.torch_utils:

class TorchModuleWrapper(keras.src.layers.layer.Layer)
 |  TorchModuleWrapper(module, name=None, **kwargs)
 |
 |  Torch module wrapper layer.
 |
 |  `TorchModuleWrapper` is a wrapper class that can turn any
 |  `torch.nn.Module` into a Keras layer, in particular by making its
 |  parameters trackable by Keras.
 |
 |  Args:
 |      module: `torch.nn.Module` instance. If it's a `LazyModule`
 |          instance, then its parameters must be initialized before
 |          passing the instance to `TorchModuleWrapper` (e.g. by calling
 |          it once).
 |      name: The name of the layer (string).
 |
 |  Examples:
 |
 |  Here's an example of how the `TorchModuleWrapper` can be used with vanilla
 |  PyTorch modules.
 |
 |  ```python
 |  import torch.nn as nn
 |  import torch.nn.functional as F
 |
 |  import keras
 |  from keras.layers import TorchModuleWrapper
 |
 |  class Classifier(keras.Model):
 |      def __init__(self, **kwargs):
 |          super().__init__(**kwargs)
 |          # Wrap `torch.nn.Module`s with `TorchModuleWrapper`
 |          # if they contain parameters
 |          self.conv1 = TorchModuleWrapper(
 |              nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))
 |          )
 |          self.conv2 = TorchModuleWrapper(
 |              nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))
 |          )
 |          self.pool = nn.MaxPool2d(kernel_size=(2, 2))
 |          self.flatten = nn.Flatten()
 |          self.dropout = nn.Dropout(p=0.5)
 |          self.fc = TorchModuleWrapper(nn.Linear(1600, 10))
 |
 |      def call(self, inputs):
 |          x = F.relu(self.conv1(inputs))
 |          x = self.pool(x)
 |          x = F.relu(self.conv2(x))
 |          x = self.pool(x)
 |          x = self.flatten(x)
 |          x = self.dropout(x)
 |          x = self.fc(x)
 |          return F.softmax(x, dim=1)
 |
 |
 |  model = Classifier()
 |  model.build((1, 28, 28))
 |  print("Output shape:", model(torch.ones(1, 1, 28, 28).to("cuda")).shape)
 |
 |  model.compile(
 |      loss="sparse_categorical_crossentropy",
 |      optimizer="adam",
 |      metrics=["accuracy"]
 |  )
 |  model.fit(train_loader, epochs=5)
 |  ```
 |
 |  Method resolution order:
 |      TorchModuleWrapper
 |      keras.src.layers.layer.Layer
 |      keras.src.backend.tensorflow.layer.TFLayer
 |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable
 |      tensorflow.python.trackable.autotrackable.AutoTrackable
 |      tensorflow.python.trackable.base.Trackable
 |      keras.src.ops.operation.Operation
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    module,
 |    name=None,
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  call(
 |    self,
 |    *args,
 |    **kwargs
 |  )
 |
 |  get_config(self)
 |      Returns the config of the object.
 |
 |      An object config is a Python dictionary (serializable)
 |      containing the information needed to re-instantiate it.
 |
 |  load_own_variables(self, store)
 |      Loads model's state via `state_dict`.
 |
 |  parameters(self, recurse=True)
 |
 |  save_own_variables(self, store)
 |      Saves model's state from `state_dict`.
 |      `model.parameters` excludes some of model's state like
 |      `BatchNorm` mean and variance. So, use `state_dict` to obtain
 |      all of model's state.
 |
 |  ----------------------------------------------------------------------
 |  Class methods defined here:
 |
 |  from_config(config) from builtins.type
 |      Creates a layer from its config.
 |
 |      This method is the reverse of `get_config`,
 |      capable of instantiating the same layer from the config
 |      dictionary. It does not handle layer connectivity
 |      (handled by Network), nor weights (handled by `set_weights`).
 |
 |      Args:
 |          config: A Python dictionary, typically the
 |              output of get_config.
 |
 |      Returns:
 |          A layer instance.
 |

