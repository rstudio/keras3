Help on class SparseCategoricalCrossentropy in module keras.src.losses.losses:

class SparseCategoricalCrossentropy(LossFunctionWrapper)
 |  SparseCategoricalCrossentropy(from_logits=False, ignore_class=None, reduction='sum_over_batch_size', axis=-1, name='sparse_categorical_crossentropy', dtype=None)
 |
 |  Computes the crossentropy loss between the labels and predictions.
 |
 |  Use this crossentropy loss function when there are two or more label
 |  classes.  We expect labels to be provided as integers. If you want to
 |  provide labels using `one-hot` representation, please use
 |  `CategoricalCrossentropy` loss.  There should be `# classes` floating point
 |  values per feature for `y_pred` and a single floating point value per
 |  feature for `y_true`.
 |
 |  In the snippet below, there is a single floating point value per example for
 |  `y_true` and `num_classes` floating pointing values per example for
 |  `y_pred`. The shape of `y_true` is `[batch_size]` and the shape of `y_pred`
 |  is `[batch_size, num_classes]`.
 |
 |  Args:
 |      from_logits: Whether `y_pred` is expected to be a logits tensor. By
 |          default, we assume that `y_pred` encodes a probability distribution.
 |      reduction: Type of reduction to apply to the loss. In almost all cases
 |          this should be `"sum_over_batch_size"`. Supported options are
 |          `"sum"`, `"sum_over_batch_size"`, `"mean"`,
 |          `"mean_with_sample_weight"` or `None`. `"sum"` sums the loss,
 |          `"sum_over_batch_size"` and `"mean"` sum the loss and divide by the
 |          sample size, and `"mean_with_sample_weight"` sums the loss and
 |          divides by the sum of the sample weights. `"none"` and `None`
 |          perform no aggregation. Defaults to `"sum_over_batch_size"`.
 |      axis: The axis along which to compute crossentropy (the features
 |          axis). Defaults to `-1`.
 |      name: Optional name for the loss instance.
 |      dtype: The dtype of the loss's computations. Defaults to `None`, which
 |          means using `keras.backend.floatx()`. `keras.backend.floatx()` is a
 |          `"float32"` unless set to different value
 |          (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is
 |          provided, then the `compute_dtype` will be utilized.
 |
 |  Examples:
 |
 |  >>> y_true = np.array([1, 2])
 |  >>> y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
 |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.
 |  >>> scce = keras.losses.SparseCategoricalCrossentropy()
 |  >>> scce(y_true, y_pred)
 |  1.177
 |
 |  >>> # Calling with 'sample_weight'.
 |  >>> scce(y_true, y_pred, sample_weight=np.array([0.3, 0.7]))
 |  0.814
 |
 |  >>> # Using 'sum' reduction type.
 |  >>> scce = keras.losses.SparseCategoricalCrossentropy(
 |  ...     reduction="sum")
 |  >>> scce(y_true, y_pred)
 |  2.354
 |
 |  >>> # Using 'none' reduction type.
 |  >>> scce = keras.losses.SparseCategoricalCrossentropy(
 |  ...     reduction=None)
 |  >>> scce(y_true, y_pred)
 |  array([0.0513, 2.303], dtype=float32)
 |
 |  Usage with the `compile()` API:
 |
 |  ```python
 |  model.compile(optimizer='sgd',
 |                loss=keras.losses.SparseCategoricalCrossentropy())
 |  ```
 |
 |  Method resolution order:
 |      SparseCategoricalCrossentropy
 |      LossFunctionWrapper
 |      keras.src.losses.loss.Loss
 |      keras.src.saving.keras_saveable.KerasSaveable
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    from_logits=False,
 |    ignore_class=None,
 |    reduction='sum_over_batch_size',
 |    axis=-1,
 |    name='sparse_categorical_crossentropy',
 |    dtype=None
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  get_config(self)
 |

