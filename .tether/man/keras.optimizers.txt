Adadelta(
  learning_rate=0.001,
  rho=0.95,
  epsilon=1e-07,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adadelta',
  **kwargs
)
Adafactor(
  learning_rate=0.001,
  beta_2_decay=-0.8,
  epsilon_1=1e-30,
  epsilon_2=0.001,
  clip_threshold=1.0,
  relative_step=True,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adafactor',
  **kwargs
)
Adagrad(
  learning_rate=0.001,
  initial_accumulator_value=0.1,
  epsilon=1e-07,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adagrad',
  **kwargs
)
Adam(
  learning_rate=0.001,
  beta_1=0.9,
  beta_2=0.999,
  epsilon=1e-07,
  amsgrad=False,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adam',
  **kwargs
)
Adamax(
  learning_rate=0.001,
  beta_1=0.9,
  beta_2=0.999,
  epsilon=1e-07,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adamax',
  **kwargs
)
AdamW(
  learning_rate=0.001,
  weight_decay=0.004,
  beta_1=0.9,
  beta_2=0.999,
  epsilon=1e-07,
  amsgrad=False,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='adamw',
  **kwargs
)
deserialize(config, custom_objects=None)
Ftrl(
  learning_rate=0.001,
  learning_rate_power=-0.5,
  initial_accumulator_value=0.1,
  l1_regularization_strength=0.0,
  l2_regularization_strength=0.0,
  l2_shrinkage_regularization_strength=0.0,
  beta=0.0,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='ftrl',
  **kwargs
)
get(identifier)
legacy: Module(keras.api.optimizers.legacy)
Lion(
  learning_rate=0.001,
  beta_1=0.9,
  beta_2=0.99,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='lion',
  **kwargs
)
LossScaleOptimizer(
  inner_optimizer,
  initial_scale=32768.0,
  dynamic_growth_steps=2000,
  **kwargs
)
Nadam(
  learning_rate=0.001,
  beta_1=0.9,
  beta_2=0.999,
  epsilon=1e-07,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='nadam',
  **kwargs
)
Optimizer(*args, **kwargs)
RMSprop(
  learning_rate=0.001,
  rho=0.9,
  momentum=0.0,
  epsilon=1e-07,
  centered=False,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='rmsprop',
  **kwargs
)
schedules: Module(keras.api.optimizers.schedules)
serialize(optimizer)
SGD(
  learning_rate=0.01,
  momentum=0.0,
  nesterov=False,
  weight_decay=None,
  clipnorm=None,
  clipvalue=None,
  global_clipnorm=None,
  use_ema=False,
  ema_momentum=0.99,
  ema_overwrite_frequency=None,
  loss_scale_factor=None,
  gradient_accumulation_steps=None,
  name='SGD',
  **kwargs
)

