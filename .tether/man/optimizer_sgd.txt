Help on class SGD in module keras.src.optimizers.sgd:

class SGD(keras.src.optimizers.optimizer.Optimizer)
 |  SGD(learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, name='SGD', **kwargs)
 |
 |  Gradient descent (with momentum) optimizer.
 |
 |  Update rule for parameter `w` with gradient `g` when `momentum` is 0:
 |
 |  ```python
 |  w = w - learning_rate * g
 |  ```
 |
 |  Update rule when `momentum` is larger than 0:
 |
 |  ```python
 |  velocity = momentum * velocity - learning_rate * g
 |  w = w + velocity
 |  ```
 |
 |  When `nesterov=True`, this rule becomes:
 |
 |  ```python
 |  velocity = momentum * velocity - learning_rate * g
 |  w = w + momentum * velocity - learning_rate * g
 |  ```
 |
 |  Args:
 |      learning_rate: A float, a
 |          `keras.optimizers.schedules.LearningRateSchedule` instance, or
 |          a callable that takes no arguments and returns the actual value to
 |          use. The learning rate. Defaults to `0.01`.
 |      momentum: float hyperparameter >= 0 that accelerates gradient descent in
 |          the relevant direction and dampens oscillations. 0 is vanilla
 |          gradient descent. Defaults to `0.0`.
 |      nesterov: boolean. Whether to apply Nesterov momentum.
 |          Defaults to `False`.
 |      name: String. The name to use
 |        for momentum accumulator weights created by
 |        the optimizer.
 |      weight_decay: Float. If set, weight decay is applied.
 |      clipnorm: Float. If set, the gradient of each weight is individually
 |        clipped so that its norm is no higher than this value.
 |      clipvalue: Float. If set, the gradient of each weight is clipped to be
 |        no higher than this value.
 |      global_clipnorm: Float. If set, the gradient of all weights is clipped
 |        so that their global norm is no higher than this value.
 |      use_ema: Boolean, defaults to False. If True, exponential moving average
 |        (EMA) is applied. EMA consists of computing an exponential moving
 |        average of the weights of the model (as the weight values change after
 |        each training batch), and periodically overwriting the weights with
 |        their moving average.
 |      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.
 |        This is the momentum to use when computing
 |        the EMA of the model's weights:
 |        `new_average = ema_momentum * old_average + (1 - ema_momentum) *
 |        current_variable_value`.
 |      ema_overwrite_frequency: Int or None, defaults to None. Only used if
 |        `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,
 |        we overwrite the model variable by its moving average.
 |        If None, the optimizer
 |        does not overwrite model variables in the middle of training, and you
 |        need to explicitly overwrite the variables at the end of training
 |        by calling `optimizer.finalize_variable_values()`
 |        (which updates the model
 |        variables in-place). When using the built-in `fit()` training loop,
 |        this happens automatically after the last epoch,
 |        and you don't need to do anything.
 |      loss_scale_factor: Float or `None`. If a float, the scale factor will
 |        be multiplied the loss before computing gradients, and the inverse of
 |        the scale factor will be multiplied by the gradients before updating
 |        variables. Useful for preventing underflow during mixed precision
 |        training. Alternately, `keras.optimizers.LossScaleOptimizer` will
 |        automatically set a loss scale factor.
 |
 |  Method resolution order:
 |      SGD
 |      keras.src.optimizers.optimizer.Optimizer
 |      keras.src.backend.tensorflow.optimizer.TFOptimizer
 |      keras.src.optimizers.base_optimizer.BaseOptimizer
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __init__(
 |    self,
 |    learning_rate=0.01,
 |    momentum=0.0,
 |    nesterov=False,
 |    weight_decay=None,
 |    clipnorm=None,
 |    clipvalue=None,
 |    global_clipnorm=None,
 |    use_ema=False,
 |    ema_momentum=0.99,
 |    ema_overwrite_frequency=None,
 |    name='SGD',
 |    **kwargs
 |  )
 |      Initialize self.  See help(type(self)) for accurate signature.
 |
 |  build(self, variables)
 |      Initialize optimizer variables.
 |
 |      SGD optimizer has one variable `momentums`, only set if `self.momentum`
 |      is not 0.
 |
 |      Args:
 |        var_list: list of model variables to build SGD variables on.
 |
 |  get_config(self)
 |      Returns the config of the optimizer.
 |
 |      An optimizer config is a Python dictionary (serializable)
 |      containing the configuration of an optimizer.
 |      The same optimizer can be reinstantiated later
 |      (without any saved state) from this configuration.
 |
 |      Subclass optimizer should override this method to include other
 |      hyperparameters.
 |
 |      Returns:
 |          Python dictionary.
 |
 |  update_step(
 |    self,
 |    gradient,
 |    variable,
 |    learning_rate
 |  )
 |      Update step given gradient and the associated model variable.
 |
